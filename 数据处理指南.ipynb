{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📦 数据处理指南\n",
    "\n",
    "> **写给小白的话**：这篇文档会手把手教你如何准备训练数据,不需要任何专业背景,跟着做就行!\n",
    "\n",
    "## 🎯 核心概念:3 分钟快速理解\n",
    "\n",
    "### 训练 AI 需要什么数据?\n",
    "\n",
    "想象一下教小孩学说话的过程:\n",
    "\n",
    "```\n",
    "👶 第一阶段:听大量日常对话 → 学会基本语言能力\n",
    "👧 第二阶段:学习问答方式 → 懂得对话结构\n",
    "👨 第三阶段:学习回答问题 → 能按要求回答\n",
    "```\n",
    "\n",
    "训练 AI 模型也是一样的**三个阶段**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建训练阶段对比表\n",
    "training_stages = pd.DataFrame({\n",
    "    '阶段': ['1️⃣', '2️⃣', '3️⃣'],\n",
    "    '名称': ['预训练 (Pretraining)', '中期训练 (Midtraining)', '微调 (Fine-tuning)'],\n",
    "    '数据类型': ['海量网页文本', '对话记录', '指令对话对'],\n",
    "    '学什么': ['语言的基本规律、语法、词汇、常识', '对话的格式、一问一答的结构', '理解和执行指令、做个好助手'],\n",
    "    '数据量': ['超级大 (几十 GB)', '中等 (几百 MB)', '较小 (几十 MB)']\n",
    "})\n",
    "\n",
    "print(\"\\n🎯 AI 训练的三个阶段\\n\")\n",
    "display(training_stages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么要分三个阶段?\n",
    "\n",
    "**类比**:就像学英语\n",
    "\n",
    "- 预训练 = 大量阅读英文书籍(学语法和词汇)\n",
    "- 中期训练 = 学习英语对话(学怎么交流)\n",
    "- 微调 = 学习回答面试问题(学特定任务)\n",
    "\n",
    "如果直接让 AI 学习回答问题而不先学语言,就像让完全不懂英语的人直接参加英语面试,肯定学不好!\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 第一阶段:预训练数据\n",
    "\n",
    "### 用什么数据?\n",
    "\n",
    "项目默认使用 **FineWeb-Edu** 数据集:\n",
    "\n",
    "- 📖 来源:HuggingFace 整理的高质量网页内容\n",
    "- 📊 规模:约 1000 亿个单词(是的,1000 亿!)\n",
    "- ✨ 质量:经过筛选,去掉了垃圾内容\n",
    "- 🎁 免费:完全开源,直接下载\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何下载?\n",
    "\n",
    "**一条命令搞定:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📦 NanoChat 数据处理指南\n",
    "\n",
    "> **写给小白的话**：这个 Notebook 会手把手教你如何准备训练数据，不需要任何专业背景，跟着运行每个单元格就行！\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 目录\n",
    "\n",
    "1. [核心概念：3 分钟快速理解](#核心概念)\n",
    "2. [第一阶段：预训练数据](#预训练数据)\n",
    "3. [第二阶段：中期训练数据](#中期训练数据)\n",
    "4. [第三阶段：微调数据](#微调数据)\n",
    "5. [实战：准备中文数据](#准备中文数据)\n",
    "6. [数据质量检查工具](#数据质量检查)\n",
    "7. [数据量计算器](#数据量计算器)\n",
    "8. [完整流程检查清单](#检查清单)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"核心概念\"></a>1. 核心概念：3 分钟快速理解\n",
    "\n",
    "### 训练 AI 需要什么数据？\n",
    "\n",
    "想象一下教小孩学说话的过程：\n",
    "\n",
    "```\n",
    "👶 第一阶段：听大量日常对话 → 学会基本语言能力\n",
    "👧 第二阶段：学习问答方式 → 懂得对话结构  \n",
    "👨 第三阶段：学习回答问题 → 能按要求回答\n",
    "```\n",
    "\n",
    "训练 AI 模型也是一样的 **三个阶段**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建训练阶段对比表\n",
    "training_stages = pd.DataFrame({\n",
    "    '阶段': ['1️⃣', '2️⃣', '3️⃣'],\n",
    "    '名称': ['预训练 (Pretraining)', '中期训练 (Midtraining)', '微调 (Fine-tuning)'],\n",
    "    '数据类型': ['海量网页文本', '对话记录', '指令对话对'],\n",
    "    '学什么': ['语言的基本规律、语法、词汇、常识', '对话的格式、一问一答的结构', '理解和执行指令、做个好助手'],\n",
    "    '数据量': ['超级大 (几十 GB)', '中等 (几百 MB)', '较小 (几十 MB)']\n",
    "})\n",
    "\n",
    "print(\"\\n🎯 AI 训练的三个阶段\\n\")\n",
    "display(training_stages)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💡 为什么要分三个阶段？\n",
    "\n",
    "**类比：就像学英语**\n",
    "\n",
    "- **预训练** = 大量阅读英文书籍（学语法和词汇）\n",
    "- **中期训练** = 学习英语对话（学怎么交流）\n",
    "- **微调** = 学习回答面试问题（学特定任务）\n",
    "\n",
    "如果直接让 AI 学习回答问题而不先学语言，就像让完全不懂英语的人直接参加英语面试，肯定学不好！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"预训练数据\"></a>2. 第一阶段：预训练数据\n",
    "\n",
    "### 用什么数据？\n",
    "\n",
    "项目默认使用 **FineWeb-Edu** 数据集：\n",
    "\n",
    "- 📖 来源：HuggingFace 整理的高质量网页内容\n",
    "- 📊 规模：约 1000 亿个单词（是的，1000 亿！）\n",
    "- ✨ 质量：经过筛选，去掉了垃圾内容\n",
    "- 🎁 免费：完全开源，直接下载\n",
    "\n",
    "### 📊 我需要下载多少数据？\n",
    "\n",
    "取决于你要训练多大的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同模型规模的数据需求对比表\n",
    "data_requirements = pd.DataFrame({\n",
    "    '模型规模': ['d10 (迷你)', 'd12 (小)', 'd20 (默认)', 'd26 (大)', 'd32 (超大)'],\n",
    "    '参数量': ['42M', '123M', '561M', '1.2B', '2.1B'],\n",
    "    '需要下载': ['16 个分片', '48 个分片', '215 个分片', '460 个分片', '806 个分片'],\n",
    "    '磁盘空间': ['~2GB', '~5GB', '~21GB', '~45GB', '~79GB'],\n",
    "    '训练时间': ['30 分钟', '1-2 小时', '4 小时', '12 小时', '24 小时']\n",
    "})\n",
    "\n",
    "print(\"\\n📊 模型规模与数据需求对照表\\n\")\n",
    "display(data_requirements)\n",
    "print(\"\\n💡 新手建议：先用 d10 或 d12 练手，熟悉流程后再训练大模型！\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 如何下载？\n",
    "\n",
    "**一条命令搞定！** 运行下面的代码单元格："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载 8 个分片用于训练分词器（约 800MB）\n",
    "# 这是最小下载量，适合快速测试\n",
    "\n",
    "!python -m nanochat.dataset -n 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果要训练 d20 模型，需要下载更多数据\n",
    "# ⚠️ 警告：这会下载约 21GB 数据，需要较长时间！\n",
    "# 如果不需要，请不要运行这个单元格\n",
    "\n",
    "# !python -m nanochat.dataset -n 215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📁 数据下载到哪了？\n",
    "\n",
    "所有数据自动保存到 `~/.cache/nanochat/base_data/`\n",
    "\n",
    "让我们检查一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 获取数据目录\n",
    "data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "\n",
    "print(f\"📁 数据目录: {data_dir}\\n\")\n",
    "\n",
    "if data_dir.exists():\n",
    "    # 统计已下载的文件\n",
    "    parquet_files = list(data_dir.glob(\"*.parquet\"))\n",
    "    \n",
    "    if parquet_files:\n",
    "        print(f\"✅ 找到 {len(parquet_files)} 个数据文件\")\n",
    "        \n",
    "        # 计算总大小\n",
    "        total_size = sum(f.stat().st_size for f in parquet_files)\n",
    "        print(f\"💽 总大小: {total_size / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # 显示前 5 个文件\n",
    "        print(\"\\n前 5 个文件:\")\n",
    "        for f in sorted(parquet_files)[:5]:\n",
    "            size_mb = f.stat().st_size / (1024**2)\n",
    "            print(f\"  📄 {f.name:25s} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"⚠️ 数据目录存在，但没有找到 .parquet 文件\")\n",
    "        print(\"   请先运行上面的下载命令！\")\n",
    "else:\n",
    "    print(\"⚠️ 数据目录不存在，请先下载数据！\")\n",
    "    print(f\"   运行: python -m nanochat.dataset -n 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 查看数据内容\n",
    "\n",
    "让我们打开一个文件看看里面是什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# 读取第一个分片\n",
    "data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "parquet_files = list(data_dir.glob(\"*.parquet\")) if data_dir.exists() else []\n",
    "\n",
    "if parquet_files:\n",
    "    first_file = sorted(parquet_files)[0]\n",
    "    print(f\"📖 正在读取: {first_file.name}\\n\")\n",
    "    \n",
    "    # 读取 Parquet 文件\n",
    "    table = pq.read_table(first_file)\n",
    "    \n",
    "    print(f\"📊 文件信息:\")\n",
    "    print(f\"   行数: {len(table):,}\")\n",
    "    print(f\"   列名: {table.column_names}\")\n",
    "    \n",
    "    # 显示前 3 条数据\n",
    "    print(\"\\n📝 前 3 条数据示例:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i in range(min(3, len(table))):\n",
    "        text = table['text'][i].as_py()\n",
    "        # 只显示前 200 个字符\n",
    "        preview = text[:200] + \"...\" if len(text) > 200 else text\n",
    "        print(f\"\\n第 {i+1} 条 (长度: {len(text)} 字符)\")\n",
    "        print(\"-\" * 80)\n",
    "        print(preview)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"⚠️ 找不到数据文件，请先下载数据！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"中期训练数据\"></a>3. 第二阶段：中期训练数据\n",
    "\n",
    "### 用什么数据？\n",
    "\n",
    "项目默认使用 **SmolTalk** 对话数据集：\n",
    "\n",
    "- 🗣️ 内容：真实的人类对话记录\n",
    "- 📝 格式：一问一答的对话形式\n",
    "- 🎯 目的：让模型学会对话的格式\n",
    "\n",
    "### 数据格式示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 对话数据格式示例\n",
    "dialogue_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"你好！请介绍一下自己\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"你好！我是一个 AI 助手，可以回答问题、提供建议...\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"你会说中文吗？\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"是的，我可以使用中文交流。\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"📝 对话数据格式示例：\\n\")\n",
    "print(json.dumps(dialogue_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\n💡 重要字段说明：\")\n",
    "print(\"   • role: 说话的角色，'user'(用户) 或 'assistant'(助手)\")\n",
    "print(\"   • content: 说话的内容\")\n",
    "\n",
    "print(\"\\n✅ 好消息：训练脚本会自动下载 SmolTalk 数据集，无需手动操作！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"微调数据\"></a>4. 第三阶段：微调数据\n",
    "\n",
    "### 用什么数据？\n",
    "\n",
    "微调阶段混合使用多个任务数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微调数据集概览\n",
    "sft_datasets = pd.DataFrame({\n",
    "    '数据集': ['ARC-Easy', 'ARC-Challenge', 'GSM8K', 'SmolTalk'],\n",
    "    '内容': ['简单选择题', '困难选择题', '小学数学题', '日常对话'],\n",
    "    '数量': ['2,300 条', '1,100 条', '8,000 条', '10,000 条'],\n",
    "    '学什么能力': ['常识推理', '深度推理', '数学计算', '闲聊能力']\n",
    "})\n",
    "\n",
    "print(\"\\n🎯 微调阶段的数据集\\n\")\n",
    "display(sft_datasets)\n",
    "print(\"\\n📊 总计：约 21,400 条训练样本\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据格式示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数学题示例 (GSM8K)\n",
    "math_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"小明有8个苹果，吃掉了3个，还剩几个？\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"让我来算一下：\\n8 - 3 = 5\\n所以小明还剩5个苹果。\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 选择题示例 (ARC)\n",
    "arc_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"哪个物体会浮在水面上？\\nA. 石头\\nB. 铁钉\\nC. 木头\\nD. 玻璃球\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"答案是C. 木头。因为木头的密度比水小，所以会浮在水面上。\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"📝 数学题示例 (GSM8K)：\\n\")\n",
    "print(json.dumps(math_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"📝 选择题示例 (ARC)：\\n\")\n",
    "print(json.dumps(arc_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\n✅ 这些数据集会在运行微调脚本时自动下载！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"准备中文数据\"></a>5. 实战：准备中文数据\n",
    "\n",
    "> 如果你想训练中文模型，需要准备中文数据。下面是一个完整的示例！\n",
    "\n",
    "### 方法一：使用 HuggingFace 中文数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 HuggingFace 镜像（可选，如果下载太慢）\n",
    "import os\n",
    "\n",
    "# 使用国内镜像加速下载\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "print(\"✅ 已设置 HuggingFace 镜像：https://hf-mirror.com\")\n",
    "print(\"   这会加速数据集下载速度\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载中文维基百科数据（示例）\n",
    "# ⚠️ 警告：这会下载较大的数据集，需要时间！\n",
    "# 如果不需要，请不要运行这个单元格\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"📥 正在下载中文维基百科（前 1000 条用于演示）...\\n\")\n",
    "\n",
    "try:\n",
    "    # 只下载前 1000 条用于演示\n",
    "    wiki = load_dataset(\n",
    "        \"wikipedia\",\n",
    "        \"20220301.zh\",  # 中文版本\n",
    "        split=\"train[:1000]\",  # 只取前 1000 条\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 成功下载：{len(wiki):,} 条数据\\n\")\n",
    "    \n",
    "    # 显示第一条\n",
    "    print(\"📝 第一条数据示例：\")\n",
    "    print(\"=\"*80)\n",
    "    first_text = wiki[0]['text'][:300] + \"...\"\n",
    "    print(first_text)\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 下载失败：{e}\")\n",
    "    print(\"   可能需要检查网络连接或尝试使用镜像\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二：转换自己的文本数据\n",
    "\n",
    "如果你有自己收集的中文文本，可以使用项目提供的转换工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用内置工具转换自定义数据\n",
    "# 详细说明请查看 data_check/convert_custom_data.py\n",
    "\n",
    "print(\"🛠️ 转换自定义文本数据的步骤：\\n\")\n",
    "print(\"1. 准备你的文本数据（.txt 文件）\")\n",
    "print(\"2. 运行转换命令：\")\n",
    "print(\"   python -m data_check.convert_custom_data\")\n",
    "print(\"\\n支持的输入格式：\")\n",
    "print(\"   • 单个文本文件（每行一条数据）\")\n",
    "print(\"   • 目录（包含多个 .txt 文件）\")\n",
    "print(\"\\n详细代码请查看：data_check/convert_custom_data.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"数据质量检查\"></a>6. 数据质量检查工具\n",
    "\n",
    "项目提供了完整的数据检查工具集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据检查工具概览\n",
    "tools = pd.DataFrame({\n",
    "    '工具': [\n",
    "        'check_data.py',\n",
    "        'check_length_distribution.py',\n",
    "        'check_content_quality.py',\n",
    "        'check_char_distribution.py',\n",
    "        'convert_custom_data.py'\n",
    "    ],\n",
    "    '用途': [\n",
    "        '验证数据文件完整性',\n",
    "        '检查文本长度分布',\n",
    "        '抽样检查内容质量',\n",
    "        '检查字符分布统计',\n",
    "        '转换自定义文本数据'\n",
    "    ],\n",
    "    '命令': [\n",
    "        'python -m data_check.check_data',\n",
    "        'python -m data_check.check_length_distribution',\n",
    "        'python -m data_check.check_content_quality',\n",
    "        'python -m data_check.check_char_distribution',\n",
    "        'python -m data_check.convert_custom_data'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n🛠️ 数据检查工具总览\\n\")\n",
    "display(tools)\n",
    "print(\"\\n💡 所有工具的详细代码都在 data_check/ 目录下\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 快速检查数据完整性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行数据完整性检查\n",
    "!python -m data_check.check_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检查文本长度分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析数据的长度分布\n",
    "# 这有助于了解数据质量\n",
    "\n",
    "!python -m data_check.check_length_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"数据量计算器\"></a>7. 数据量计算器\n",
    "\n",
    "### Chinchilla 定律\n",
    "\n",
    "**数据 token 数 = 模型参数量 × 20**\n",
    "\n",
    "让我们计算不同模型需要多少数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_requirement(model_params_million):\n",
    "    \"\"\"\n",
    "    计算训练所需的数据量\n",
    "    \n",
    "    参数:\n",
    "        model_params_million: 模型参数量(百万)，如123表示123M参数\n",
    "    \n",
    "    返回:\n",
    "        字典，包含各种数据量信息\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 需要的 token 数（参数量 × 20）\n",
    "    tokens_billion = model_params_million / 1000 * 20\n",
    "    \n",
    "    # 2. 需要的字符数（1 token ≈ 4.8 字符）\n",
    "    chars_billion = tokens_billion * 4.8\n",
    "    \n",
    "    # 3. 需要的分片数（每个分片 250M 字符）\n",
    "    num_shards = int(chars_billion * 1000 / 250)\n",
    "    \n",
    "    # 4. 磁盘空间（每个分片约 100MB）\n",
    "    disk_gb = num_shards * 100 / 1024\n",
    "    \n",
    "    return {\n",
    "        'model_params': f\"{model_params_million}M\",\n",
    "        'tokens': f\"{tokens_billion:.1f}B\",\n",
    "        'chars': f\"{chars_billion:.0f}B\",\n",
    "        'shards': num_shards,\n",
    "        'disk': f\"{disk_gb:.1f}GB\"\n",
    "    }\n",
    "\n",
    "# 不同规模模型\n",
    "models = {\n",
    "    'd10': 42,\n",
    "    'd12': 123,\n",
    "    'd20': 561,\n",
    "    'd26': 1200,\n",
    "    'd32': 2100\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, params in models.items():\n",
    "    req = calculate_data_requirement(params)\n",
    "    results.append({\n",
    "        '模型': name,\n",
    "        '参数量': req['model_params'],\n",
    "        'Token数': req['tokens'],\n",
    "        '字符数': req['chars'],\n",
    "        '分片数': req['shards'],\n",
    "        '磁盘': req['disk']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n📊 模型数据需求计算表\\n\")\n",
    "display(df_results)\n",
    "print(\"\\n💡 提示：数据量基于 Chinchilla 定律计算（参数量 × 20）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义计算\n",
    "\n",
    "输入你的模型参数量，计算需要多少数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义模型参数量（单位：百万）\n",
    "my_model_params = 100  # 修改这里！\n",
    "\n",
    "result = calculate_data_requirement(my_model_params)\n",
    "\n",
    "print(f\"\\n🎯 您的模型（{my_model_params}M 参数）需要：\\n\")\n",
    "print(f\"   Token 数量：{result['tokens']}\")\n",
    "print(f\"   字符数量：{result['chars']}\")\n",
    "print(f\"   数据分片：{result['shards']} 个\")\n",
    "print(f\"   磁盘空间：{result['disk']}\")\n",
    "print(\"\\n下载命令：\")\n",
    "print(f\"   python -m nanochat.dataset -n {result['shards']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"检查清单\"></a>8. 完整流程检查清单\n",
    "\n",
    "准备好数据了吗？对照这个清单检查："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def check_data_readiness():\n",
    "    \"\"\"检查数据准备情况\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 数据准备状态检查\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # 1. 检查预训练数据\n",
    "    base_data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "    if base_data_dir.exists():\n",
    "        parquet_files = list(base_data_dir.glob(\"*.parquet\"))\n",
    "        if len(parquet_files) >= 8:\n",
    "            checks.append((\"✅\", f\"预训练数据：找到 {len(parquet_files)} 个分片\"))\n",
    "        else:\n",
    "            checks.append((\"⚠️\", f\"预训练数据：只有 {len(parquet_files)} 个分片（建议至少 8 个）\"))\n",
    "    else:\n",
    "        checks.append((\"❌\", \"预训练数据：未下载\"))\n",
    "    \n",
    "    # 2. 检查分词器\n",
    "    tokenizer_dir = Path.home() / \".cache\" / \"nanochat\" / \"tokenizer\"\n",
    "    if tokenizer_dir.exists() and list(tokenizer_dir.glob(\"*.model\")):\n",
    "        checks.append((\"✅\", \"分词器：已训练\"))\n",
    "    else:\n",
    "        checks.append((\"⚠️\", \"分词器：未训练（需要运行 tok_train）\"))\n",
    "    \n",
    "    # 3. 检查磁盘空间\n",
    "    cache_dir = Path.home() / \".cache\"\n",
    "    if cache_dir.exists():\n",
    "        try:\n",
    "            stat = shutil.disk_usage(cache_dir)\n",
    "            free_gb = stat.free / (1024**3)\n",
    "            if free_gb > 30:\n",
    "                checks.append((\"✅\", f\"磁盘空间：剩余 {free_gb:.1f} GB\"))\n",
    "            else:\n",
    "                checks.append((\"⚠️\", f\"磁盘空间：剩余 {free_gb:.1f} GB（建议至少 30GB）\"))\n",
    "        except:\n",
    "            checks.append((\"ℹ️\", \"磁盘空间：无法检测\"))\n",
    "    \n",
    "    # 4. 检查环境变量\n",
    "    if 'HF_ENDPOINT' in os.environ:\n",
    "        checks.append((\"✅\", f\"HuggingFace 镜像：{os.environ['HF_ENDPOINT']}\"))\n",
    "    else:\n",
    "        checks.append((\"ℹ️\", \"HuggingFace 镜像：未设置（国内用户建议设置）\"))\n",
    "    \n",
    "    # 显示结果\n",
    "    for status, msg in checks:\n",
    "        print(f\"{status} {msg}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 总结\n",
    "    ready_count = sum(1 for s, _ in checks if s == \"✅\")\n",
    "    total_count = len(checks)\n",
    "    \n",
    "    print(f\"\\n📊 就绪状态：{ready_count}/{total_count}\")\n",
    "    \n",
    "    if ready_count >= 2:  # 至少有数据和空间就算基本就绪\n",
    "        print(\"\\n🎉 数据基本准备完成，可以开始训练了！\")\n",
    "    else:\n",
    "        print(\"\\n💡 还有一些准备工作需要完成，请查看上面的提示\")\n",
    "\n",
    "# 运行检查\n",
    "check_data_readiness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 下一步\n",
    "\n",
    "数据准备好了！接下来：\n",
    "\n",
    "### 1. 训练分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练分词器\n",
    "# ⚠️ 警告：这可能需要较长时间！\n",
    "\n",
    "# !python -m scripts.tok_train --max_chars=2000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 开始预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始预训练（需要 GPU）\n",
    "# ⚠️ 警告：这需要大量时间和计算资源！\n",
    "\n",
    "# !torchrun --standalone --nproc_per_node=8 -m scripts.base_train --depth=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 扩展阅读\n",
    "\n",
    "想深入了解？推荐阅读：\n",
    "\n",
    "- 📄 [README.md](README.md) - 项目整体介绍\n",
    "- 📄 [数据.md](数据.md) - 更详细的数据文档\n",
    "- 🌐 [FineWeb-Edu 数据集](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n",
    "- 📚 [Chinchilla 论文](https://arxiv.org/abs/2203.15556) - 理解数据量和模型规模的关系\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 需要帮助？\n",
    "\n",
    "遇到问题了？\n",
    "\n",
    "1. 先查看本文档的各个章节\n",
    "2. 运行数据检查工具诊断问题\n",
    "3. 查看 `数据.md` 文档获取更详细的信息\n",
    "4. 查看项目的 GitHub Issues\n",
    "\n",
    "---\n",
    "\n",
    "**祝你训练顺利！** 🎉\n",
    "\n",
    "> 💡 记住：数据质量比数量更重要！不要盲目追求大数据，先用小模型验证流程，再逐步扩大规模。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
