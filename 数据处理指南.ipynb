{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“¦ æ•°æ®å¤„ç†æŒ‡å—\n",
    "\n",
    "> **å†™ç»™å°ç™½çš„è¯**ï¼šè¿™ç¯‡æ–‡æ¡£ä¼šæ‰‹æŠŠæ‰‹æ•™ä½ å¦‚ä½•å‡†å¤‡è®­ç»ƒæ•°æ®,ä¸éœ€è¦ä»»ä½•ä¸“ä¸šèƒŒæ™¯,è·Ÿç€åšå°±è¡Œ!\n",
    "\n",
    "## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ:3 åˆ†é’Ÿå¿«é€Ÿç†è§£\n",
    "\n",
    "### è®­ç»ƒ AI éœ€è¦ä»€ä¹ˆæ•°æ®?\n",
    "\n",
    "æƒ³è±¡ä¸€ä¸‹æ•™å°å­©å­¦è¯´è¯çš„è¿‡ç¨‹:\n",
    "\n",
    "```\n",
    "ğŸ‘¶ ç¬¬ä¸€é˜¶æ®µ:å¬å¤§é‡æ—¥å¸¸å¯¹è¯ â†’ å­¦ä¼šåŸºæœ¬è¯­è¨€èƒ½åŠ›\n",
    "ğŸ‘§ ç¬¬äºŒé˜¶æ®µ:å­¦ä¹ é—®ç­”æ–¹å¼ â†’ æ‡‚å¾—å¯¹è¯ç»“æ„\n",
    "ğŸ‘¨ ç¬¬ä¸‰é˜¶æ®µ:å­¦ä¹ å›ç­”é—®é¢˜ â†’ èƒ½æŒ‰è¦æ±‚å›ç­”\n",
    "```\n",
    "\n",
    "è®­ç»ƒ AI æ¨¡å‹ä¹Ÿæ˜¯ä¸€æ ·çš„**ä¸‰ä¸ªé˜¶æ®µ**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒé˜¶æ®µå¯¹æ¯”è¡¨\n",
    "training_stages = pd.DataFrame({\n",
    "    'é˜¶æ®µ': ['1ï¸âƒ£', '2ï¸âƒ£', '3ï¸âƒ£'],\n",
    "    'åç§°': ['é¢„è®­ç»ƒ (Pretraining)', 'ä¸­æœŸè®­ç»ƒ (Midtraining)', 'å¾®è°ƒ (Fine-tuning)'],\n",
    "    'æ•°æ®ç±»å‹': ['æµ·é‡ç½‘é¡µæ–‡æœ¬', 'å¯¹è¯è®°å½•', 'æŒ‡ä»¤å¯¹è¯å¯¹'],\n",
    "    'å­¦ä»€ä¹ˆ': ['è¯­è¨€çš„åŸºæœ¬è§„å¾‹ã€è¯­æ³•ã€è¯æ±‡ã€å¸¸è¯†', 'å¯¹è¯çš„æ ¼å¼ã€ä¸€é—®ä¸€ç­”çš„ç»“æ„', 'ç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤ã€åšä¸ªå¥½åŠ©æ‰‹'],\n",
    "    'æ•°æ®é‡': ['è¶…çº§å¤§ (å‡ å GB)', 'ä¸­ç­‰ (å‡ ç™¾ MB)', 'è¾ƒå° (å‡ å MB)']\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ¯ AI è®­ç»ƒçš„ä¸‰ä¸ªé˜¶æ®µ\\n\")\n",
    "display(training_stages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸ºä»€ä¹ˆè¦åˆ†ä¸‰ä¸ªé˜¶æ®µ?\n",
    "\n",
    "**ç±»æ¯”**:å°±åƒå­¦è‹±è¯­\n",
    "\n",
    "- é¢„è®­ç»ƒ = å¤§é‡é˜…è¯»è‹±æ–‡ä¹¦ç±(å­¦è¯­æ³•å’Œè¯æ±‡)\n",
    "- ä¸­æœŸè®­ç»ƒ = å­¦ä¹ è‹±è¯­å¯¹è¯(å­¦æ€ä¹ˆäº¤æµ)\n",
    "- å¾®è°ƒ = å­¦ä¹ å›ç­”é¢è¯•é—®é¢˜(å­¦ç‰¹å®šä»»åŠ¡)\n",
    "\n",
    "å¦‚æœç›´æ¥è®© AI å­¦ä¹ å›ç­”é—®é¢˜è€Œä¸å…ˆå­¦è¯­è¨€,å°±åƒè®©å®Œå…¨ä¸æ‡‚è‹±è¯­çš„äººç›´æ¥å‚åŠ è‹±è¯­é¢è¯•,è‚¯å®šå­¦ä¸å¥½!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ç¬¬ä¸€é˜¶æ®µ:é¢„è®­ç»ƒæ•°æ®\n",
    "\n",
    "### ç”¨ä»€ä¹ˆæ•°æ®?\n",
    "\n",
    "é¡¹ç›®é»˜è®¤ä½¿ç”¨ **FineWeb-Edu** æ•°æ®é›†:\n",
    "\n",
    "- ğŸ“– æ¥æº:HuggingFace æ•´ç†çš„é«˜è´¨é‡ç½‘é¡µå†…å®¹\n",
    "- ğŸ“Š è§„æ¨¡:çº¦ 1000 äº¿ä¸ªå•è¯(æ˜¯çš„,1000 äº¿!)\n",
    "- âœ¨ è´¨é‡:ç»è¿‡ç­›é€‰,å»æ‰äº†åƒåœ¾å†…å®¹\n",
    "- ğŸ å…è´¹:å®Œå…¨å¼€æº,ç›´æ¥ä¸‹è½½\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¦‚ä½•ä¸‹è½½?\n",
    "\n",
    "**ä¸€æ¡å‘½ä»¤æå®š:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“¦ NanoChat æ•°æ®å¤„ç†æŒ‡å—\n",
    "\n",
    "> **å†™ç»™å°ç™½çš„è¯**ï¼šè¿™ä¸ª Notebook ä¼šæ‰‹æŠŠæ‰‹æ•™ä½ å¦‚ä½•å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä¸éœ€è¦ä»»ä½•ä¸“ä¸šèƒŒæ™¯ï¼Œè·Ÿç€è¿è¡Œæ¯ä¸ªå•å…ƒæ ¼å°±è¡Œï¼\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ç›®å½•\n",
    "\n",
    "1. [æ ¸å¿ƒæ¦‚å¿µï¼š3 åˆ†é’Ÿå¿«é€Ÿç†è§£](#æ ¸å¿ƒæ¦‚å¿µ)\n",
    "2. [ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒæ•°æ®](#é¢„è®­ç»ƒæ•°æ®)\n",
    "3. [ç¬¬äºŒé˜¶æ®µï¼šä¸­æœŸè®­ç»ƒæ•°æ®](#ä¸­æœŸè®­ç»ƒæ•°æ®)\n",
    "4. [ç¬¬ä¸‰é˜¶æ®µï¼šå¾®è°ƒæ•°æ®](#å¾®è°ƒæ•°æ®)\n",
    "5. [å®æˆ˜ï¼šå‡†å¤‡ä¸­æ–‡æ•°æ®](#å‡†å¤‡ä¸­æ–‡æ•°æ®)\n",
    "6. [æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·](#æ•°æ®è´¨é‡æ£€æŸ¥)\n",
    "7. [æ•°æ®é‡è®¡ç®—å™¨](#æ•°æ®é‡è®¡ç®—å™¨)\n",
    "8. [å®Œæ•´æµç¨‹æ£€æŸ¥æ¸…å•](#æ£€æŸ¥æ¸…å•)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"æ ¸å¿ƒæ¦‚å¿µ\"></a>1. æ ¸å¿ƒæ¦‚å¿µï¼š3 åˆ†é’Ÿå¿«é€Ÿç†è§£\n",
    "\n",
    "### è®­ç»ƒ AI éœ€è¦ä»€ä¹ˆæ•°æ®ï¼Ÿ\n",
    "\n",
    "æƒ³è±¡ä¸€ä¸‹æ•™å°å­©å­¦è¯´è¯çš„è¿‡ç¨‹ï¼š\n",
    "\n",
    "```\n",
    "ğŸ‘¶ ç¬¬ä¸€é˜¶æ®µï¼šå¬å¤§é‡æ—¥å¸¸å¯¹è¯ â†’ å­¦ä¼šåŸºæœ¬è¯­è¨€èƒ½åŠ›\n",
    "ğŸ‘§ ç¬¬äºŒé˜¶æ®µï¼šå­¦ä¹ é—®ç­”æ–¹å¼ â†’ æ‡‚å¾—å¯¹è¯ç»“æ„  \n",
    "ğŸ‘¨ ç¬¬ä¸‰é˜¶æ®µï¼šå­¦ä¹ å›ç­”é—®é¢˜ â†’ èƒ½æŒ‰è¦æ±‚å›ç­”\n",
    "```\n",
    "\n",
    "è®­ç»ƒ AI æ¨¡å‹ä¹Ÿæ˜¯ä¸€æ ·çš„ **ä¸‰ä¸ªé˜¶æ®µ**ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒé˜¶æ®µå¯¹æ¯”è¡¨\n",
    "training_stages = pd.DataFrame({\n",
    "    'é˜¶æ®µ': ['1ï¸âƒ£', '2ï¸âƒ£', '3ï¸âƒ£'],\n",
    "    'åç§°': ['é¢„è®­ç»ƒ (Pretraining)', 'ä¸­æœŸè®­ç»ƒ (Midtraining)', 'å¾®è°ƒ (Fine-tuning)'],\n",
    "    'æ•°æ®ç±»å‹': ['æµ·é‡ç½‘é¡µæ–‡æœ¬', 'å¯¹è¯è®°å½•', 'æŒ‡ä»¤å¯¹è¯å¯¹'],\n",
    "    'å­¦ä»€ä¹ˆ': ['è¯­è¨€çš„åŸºæœ¬è§„å¾‹ã€è¯­æ³•ã€è¯æ±‡ã€å¸¸è¯†', 'å¯¹è¯çš„æ ¼å¼ã€ä¸€é—®ä¸€ç­”çš„ç»“æ„', 'ç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤ã€åšä¸ªå¥½åŠ©æ‰‹'],\n",
    "    'æ•°æ®é‡': ['è¶…çº§å¤§ (å‡ å GB)', 'ä¸­ç­‰ (å‡ ç™¾ MB)', 'è¾ƒå° (å‡ å MB)']\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ¯ AI è®­ç»ƒçš„ä¸‰ä¸ªé˜¶æ®µ\\n\")\n",
    "display(training_stages)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ä¸ºä»€ä¹ˆè¦åˆ†ä¸‰ä¸ªé˜¶æ®µï¼Ÿ\n",
    "\n",
    "**ç±»æ¯”ï¼šå°±åƒå­¦è‹±è¯­**\n",
    "\n",
    "- **é¢„è®­ç»ƒ** = å¤§é‡é˜…è¯»è‹±æ–‡ä¹¦ç±ï¼ˆå­¦è¯­æ³•å’Œè¯æ±‡ï¼‰\n",
    "- **ä¸­æœŸè®­ç»ƒ** = å­¦ä¹ è‹±è¯­å¯¹è¯ï¼ˆå­¦æ€ä¹ˆäº¤æµï¼‰\n",
    "- **å¾®è°ƒ** = å­¦ä¹ å›ç­”é¢è¯•é—®é¢˜ï¼ˆå­¦ç‰¹å®šä»»åŠ¡ï¼‰\n",
    "\n",
    "å¦‚æœç›´æ¥è®© AI å­¦ä¹ å›ç­”é—®é¢˜è€Œä¸å…ˆå­¦è¯­è¨€ï¼Œå°±åƒè®©å®Œå…¨ä¸æ‡‚è‹±è¯­çš„äººç›´æ¥å‚åŠ è‹±è¯­é¢è¯•ï¼Œè‚¯å®šå­¦ä¸å¥½ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"é¢„è®­ç»ƒæ•°æ®\"></a>2. ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒæ•°æ®\n",
    "\n",
    "### ç”¨ä»€ä¹ˆæ•°æ®ï¼Ÿ\n",
    "\n",
    "é¡¹ç›®é»˜è®¤ä½¿ç”¨ **FineWeb-Edu** æ•°æ®é›†ï¼š\n",
    "\n",
    "- ğŸ“– æ¥æºï¼šHuggingFace æ•´ç†çš„é«˜è´¨é‡ç½‘é¡µå†…å®¹\n",
    "- ğŸ“Š è§„æ¨¡ï¼šçº¦ 1000 äº¿ä¸ªå•è¯ï¼ˆæ˜¯çš„ï¼Œ1000 äº¿ï¼ï¼‰\n",
    "- âœ¨ è´¨é‡ï¼šç»è¿‡ç­›é€‰ï¼Œå»æ‰äº†åƒåœ¾å†…å®¹\n",
    "- ğŸ å…è´¹ï¼šå®Œå…¨å¼€æºï¼Œç›´æ¥ä¸‹è½½\n",
    "\n",
    "### ğŸ“Š æˆ‘éœ€è¦ä¸‹è½½å¤šå°‘æ•°æ®ï¼Ÿ\n",
    "\n",
    "å–å†³äºä½ è¦è®­ç»ƒå¤šå¤§çš„æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸åŒæ¨¡å‹è§„æ¨¡çš„æ•°æ®éœ€æ±‚å¯¹æ¯”è¡¨\n",
    "data_requirements = pd.DataFrame({\n",
    "    'æ¨¡å‹è§„æ¨¡': ['d10 (è¿·ä½ )', 'd12 (å°)', 'd20 (é»˜è®¤)', 'd26 (å¤§)', 'd32 (è¶…å¤§)'],\n",
    "    'å‚æ•°é‡': ['42M', '123M', '561M', '1.2B', '2.1B'],\n",
    "    'éœ€è¦ä¸‹è½½': ['16 ä¸ªåˆ†ç‰‡', '48 ä¸ªåˆ†ç‰‡', '215 ä¸ªåˆ†ç‰‡', '460 ä¸ªåˆ†ç‰‡', '806 ä¸ªåˆ†ç‰‡'],\n",
    "    'ç£ç›˜ç©ºé—´': ['~2GB', '~5GB', '~21GB', '~45GB', '~79GB'],\n",
    "    'è®­ç»ƒæ—¶é—´': ['30 åˆ†é’Ÿ', '1-2 å°æ—¶', '4 å°æ—¶', '12 å°æ—¶', '24 å°æ—¶']\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ“Š æ¨¡å‹è§„æ¨¡ä¸æ•°æ®éœ€æ±‚å¯¹ç…§è¡¨\\n\")\n",
    "display(data_requirements)\n",
    "print(\"\\nğŸ’¡ æ–°æ‰‹å»ºè®®ï¼šå…ˆç”¨ d10 æˆ– d12 ç»ƒæ‰‹ï¼Œç†Ÿæ‚‰æµç¨‹åå†è®­ç»ƒå¤§æ¨¡å‹ï¼\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ å¦‚ä½•ä¸‹è½½ï¼Ÿ\n",
    "\n",
    "**ä¸€æ¡å‘½ä»¤æå®šï¼** è¿è¡Œä¸‹é¢çš„ä»£ç å•å…ƒæ ¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½ 8 ä¸ªåˆ†ç‰‡ç”¨äºè®­ç»ƒåˆ†è¯å™¨ï¼ˆçº¦ 800MBï¼‰\n",
    "# è¿™æ˜¯æœ€å°ä¸‹è½½é‡ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•\n",
    "\n",
    "!python -m nanochat.dataset -n 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœè¦è®­ç»ƒ d20 æ¨¡å‹ï¼Œéœ€è¦ä¸‹è½½æ›´å¤šæ•°æ®\n",
    "# âš ï¸ è­¦å‘Šï¼šè¿™ä¼šä¸‹è½½çº¦ 21GB æ•°æ®ï¼Œéœ€è¦è¾ƒé•¿æ—¶é—´ï¼\n",
    "# å¦‚æœä¸éœ€è¦ï¼Œè¯·ä¸è¦è¿è¡Œè¿™ä¸ªå•å…ƒæ ¼\n",
    "\n",
    "# !python -m nanochat.dataset -n 215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ æ•°æ®ä¸‹è½½åˆ°å“ªäº†ï¼Ÿ\n",
    "\n",
    "æ‰€æœ‰æ•°æ®è‡ªåŠ¨ä¿å­˜åˆ° `~/.cache/nanochat/base_data/`\n",
    "\n",
    "è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# è·å–æ•°æ®ç›®å½•\n",
    "data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "\n",
    "print(f\"ğŸ“ æ•°æ®ç›®å½•: {data_dir}\\n\")\n",
    "\n",
    "if data_dir.exists():\n",
    "    # ç»Ÿè®¡å·²ä¸‹è½½çš„æ–‡ä»¶\n",
    "    parquet_files = list(data_dir.glob(\"*.parquet\"))\n",
    "    \n",
    "    if parquet_files:\n",
    "        print(f\"âœ… æ‰¾åˆ° {len(parquet_files)} ä¸ªæ•°æ®æ–‡ä»¶\")\n",
    "        \n",
    "        # è®¡ç®—æ€»å¤§å°\n",
    "        total_size = sum(f.stat().st_size for f in parquet_files)\n",
    "        print(f\"ğŸ’½ æ€»å¤§å°: {total_size / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # æ˜¾ç¤ºå‰ 5 ä¸ªæ–‡ä»¶\n",
    "        print(\"\\nå‰ 5 ä¸ªæ–‡ä»¶:\")\n",
    "        for f in sorted(parquet_files)[:5]:\n",
    "            size_mb = f.stat().st_size / (1024**2)\n",
    "            print(f\"  ğŸ“„ {f.name:25s} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æ•°æ®ç›®å½•å­˜åœ¨ï¼Œä½†æ²¡æœ‰æ‰¾åˆ° .parquet æ–‡ä»¶\")\n",
    "        print(\"   è¯·å…ˆè¿è¡Œä¸Šé¢çš„ä¸‹è½½å‘½ä»¤ï¼\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®ï¼\")\n",
    "    print(f\"   è¿è¡Œ: python -m nanochat.dataset -n 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” æŸ¥çœ‹æ•°æ®å†…å®¹\n",
    "\n",
    "è®©æˆ‘ä»¬æ‰“å¼€ä¸€ä¸ªæ–‡ä»¶çœ‹çœ‹é‡Œé¢æ˜¯ä»€ä¹ˆï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# è¯»å–ç¬¬ä¸€ä¸ªåˆ†ç‰‡\n",
    "data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "parquet_files = list(data_dir.glob(\"*.parquet\")) if data_dir.exists() else []\n",
    "\n",
    "if parquet_files:\n",
    "    first_file = sorted(parquet_files)[0]\n",
    "    print(f\"ğŸ“– æ­£åœ¨è¯»å–: {first_file.name}\\n\")\n",
    "    \n",
    "    # è¯»å– Parquet æ–‡ä»¶\n",
    "    table = pq.read_table(first_file)\n",
    "    \n",
    "    print(f\"ğŸ“Š æ–‡ä»¶ä¿¡æ¯:\")\n",
    "    print(f\"   è¡Œæ•°: {len(table):,}\")\n",
    "    print(f\"   åˆ—å: {table.column_names}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå‰ 3 æ¡æ•°æ®\n",
    "    print(\"\\nğŸ“ å‰ 3 æ¡æ•°æ®ç¤ºä¾‹:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i in range(min(3, len(table))):\n",
    "        text = table['text'][i].as_py()\n",
    "        # åªæ˜¾ç¤ºå‰ 200 ä¸ªå­—ç¬¦\n",
    "        preview = text[:200] + \"...\" if len(text) > 200 else text\n",
    "        print(f\"\\nç¬¬ {i+1} æ¡ (é•¿åº¦: {len(text)} å­—ç¬¦)\")\n",
    "        print(\"-\" * 80)\n",
    "        print(preview)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"âš ï¸ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"ä¸­æœŸè®­ç»ƒæ•°æ®\"></a>3. ç¬¬äºŒé˜¶æ®µï¼šä¸­æœŸè®­ç»ƒæ•°æ®\n",
    "\n",
    "### ç”¨ä»€ä¹ˆæ•°æ®ï¼Ÿ\n",
    "\n",
    "é¡¹ç›®é»˜è®¤ä½¿ç”¨ **SmolTalk** å¯¹è¯æ•°æ®é›†ï¼š\n",
    "\n",
    "- ğŸ—£ï¸ å†…å®¹ï¼šçœŸå®çš„äººç±»å¯¹è¯è®°å½•\n",
    "- ğŸ“ æ ¼å¼ï¼šä¸€é—®ä¸€ç­”çš„å¯¹è¯å½¢å¼\n",
    "- ğŸ¯ ç›®çš„ï¼šè®©æ¨¡å‹å­¦ä¼šå¯¹è¯çš„æ ¼å¼\n",
    "\n",
    "### æ•°æ®æ ¼å¼ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# å¯¹è¯æ•°æ®æ ¼å¼ç¤ºä¾‹\n",
    "dialogue_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"ä½ å¥½ï¼è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”é—®é¢˜ã€æä¾›å»ºè®®...\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"ä½ ä¼šè¯´ä¸­æ–‡å—ï¼Ÿ\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"æ˜¯çš„ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨ä¸­æ–‡äº¤æµã€‚\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ å¯¹è¯æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š\\n\")\n",
    "print(json.dumps(dialogue_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\nğŸ’¡ é‡è¦å­—æ®µè¯´æ˜ï¼š\")\n",
    "print(\"   â€¢ role: è¯´è¯çš„è§’è‰²ï¼Œ'user'(ç”¨æˆ·) æˆ– 'assistant'(åŠ©æ‰‹)\")\n",
    "print(\"   â€¢ content: è¯´è¯çš„å†…å®¹\")\n",
    "\n",
    "print(\"\\nâœ… å¥½æ¶ˆæ¯ï¼šè®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨ä¸‹è½½ SmolTalk æ•°æ®é›†ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"å¾®è°ƒæ•°æ®\"></a>4. ç¬¬ä¸‰é˜¶æ®µï¼šå¾®è°ƒæ•°æ®\n",
    "\n",
    "### ç”¨ä»€ä¹ˆæ•°æ®ï¼Ÿ\n",
    "\n",
    "å¾®è°ƒé˜¶æ®µæ··åˆä½¿ç”¨å¤šä¸ªä»»åŠ¡æ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾®è°ƒæ•°æ®é›†æ¦‚è§ˆ\n",
    "sft_datasets = pd.DataFrame({\n",
    "    'æ•°æ®é›†': ['ARC-Easy', 'ARC-Challenge', 'GSM8K', 'SmolTalk'],\n",
    "    'å†…å®¹': ['ç®€å•é€‰æ‹©é¢˜', 'å›°éš¾é€‰æ‹©é¢˜', 'å°å­¦æ•°å­¦é¢˜', 'æ—¥å¸¸å¯¹è¯'],\n",
    "    'æ•°é‡': ['2,300 æ¡', '1,100 æ¡', '8,000 æ¡', '10,000 æ¡'],\n",
    "    'å­¦ä»€ä¹ˆèƒ½åŠ›': ['å¸¸è¯†æ¨ç†', 'æ·±åº¦æ¨ç†', 'æ•°å­¦è®¡ç®—', 'é—²èŠèƒ½åŠ›']\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ¯ å¾®è°ƒé˜¶æ®µçš„æ•°æ®é›†\\n\")\n",
    "display(sft_datasets)\n",
    "print(\"\\nğŸ“Š æ€»è®¡ï¼šçº¦ 21,400 æ¡è®­ç»ƒæ ·æœ¬\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®æ ¼å¼ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°å­¦é¢˜ç¤ºä¾‹ (GSM8K)\n",
    "math_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"å°æ˜æœ‰8ä¸ªè‹¹æœï¼Œåƒæ‰äº†3ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼Ÿ\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"è®©æˆ‘æ¥ç®—ä¸€ä¸‹ï¼š\\n8 - 3 = 5\\næ‰€ä»¥å°æ˜è¿˜å‰©5ä¸ªè‹¹æœã€‚\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# é€‰æ‹©é¢˜ç¤ºä¾‹ (ARC)\n",
    "arc_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"å“ªä¸ªç‰©ä½“ä¼šæµ®åœ¨æ°´é¢ä¸Šï¼Ÿ\\nA. çŸ³å¤´\\nB. é“é’‰\\nC. æœ¨å¤´\\nD. ç»ç’ƒçƒ\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"ç­”æ¡ˆæ˜¯C. æœ¨å¤´ã€‚å› ä¸ºæœ¨å¤´çš„å¯†åº¦æ¯”æ°´å°ï¼Œæ‰€ä»¥ä¼šæµ®åœ¨æ°´é¢ä¸Šã€‚\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ æ•°å­¦é¢˜ç¤ºä¾‹ (GSM8K)ï¼š\\n\")\n",
    "print(json.dumps(math_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ğŸ“ é€‰æ‹©é¢˜ç¤ºä¾‹ (ARC)ï¼š\\n\")\n",
    "print(json.dumps(arc_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\nâœ… è¿™äº›æ•°æ®é›†ä¼šåœ¨è¿è¡Œå¾®è°ƒè„šæœ¬æ—¶è‡ªåŠ¨ä¸‹è½½ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"å‡†å¤‡ä¸­æ–‡æ•°æ®\"></a>5. å®æˆ˜ï¼šå‡†å¤‡ä¸­æ–‡æ•°æ®\n",
    "\n",
    "> å¦‚æœä½ æƒ³è®­ç»ƒä¸­æ–‡æ¨¡å‹ï¼Œéœ€è¦å‡†å¤‡ä¸­æ–‡æ•°æ®ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼\n",
    "\n",
    "### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ HuggingFace ä¸­æ–‡æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½® HuggingFace é•œåƒï¼ˆå¯é€‰ï¼Œå¦‚æœä¸‹è½½å¤ªæ…¢ï¼‰\n",
    "import os\n",
    "\n",
    "# ä½¿ç”¨å›½å†…é•œåƒåŠ é€Ÿä¸‹è½½\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "print(\"âœ… å·²è®¾ç½® HuggingFace é•œåƒï¼šhttps://hf-mirror.com\")\n",
    "print(\"   è¿™ä¼šåŠ é€Ÿæ•°æ®é›†ä¸‹è½½é€Ÿåº¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰\n",
    "# âš ï¸ è­¦å‘Šï¼šè¿™ä¼šä¸‹è½½è¾ƒå¤§çš„æ•°æ®é›†ï¼Œéœ€è¦æ—¶é—´ï¼\n",
    "# å¦‚æœä¸éœ€è¦ï¼Œè¯·ä¸è¦è¿è¡Œè¿™ä¸ªå•å…ƒæ ¼\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"ğŸ“¥ æ­£åœ¨ä¸‹è½½ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼ˆå‰ 1000 æ¡ç”¨äºæ¼”ç¤ºï¼‰...\\n\")\n",
    "\n",
    "try:\n",
    "    # åªä¸‹è½½å‰ 1000 æ¡ç”¨äºæ¼”ç¤º\n",
    "    wiki = load_dataset(\n",
    "        \"wikipedia\",\n",
    "        \"20220301.zh\",  # ä¸­æ–‡ç‰ˆæœ¬\n",
    "        split=\"train[:1000]\",  # åªå–å‰ 1000 æ¡\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… æˆåŠŸä¸‹è½½ï¼š{len(wiki):,} æ¡æ•°æ®\\n\")\n",
    "    \n",
    "    # æ˜¾ç¤ºç¬¬ä¸€æ¡\n",
    "    print(\"ğŸ“ ç¬¬ä¸€æ¡æ•°æ®ç¤ºä¾‹ï¼š\")\n",
    "    print(\"=\"*80)\n",
    "    first_text = wiki[0]['text'][:300] + \"...\"\n",
    "    print(first_text)\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¸‹è½½å¤±è´¥ï¼š{e}\")\n",
    "    print(\"   å¯èƒ½éœ€è¦æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨é•œåƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–¹æ³•äºŒï¼šè½¬æ¢è‡ªå·±çš„æ–‡æœ¬æ•°æ®\n",
    "\n",
    "å¦‚æœä½ æœ‰è‡ªå·±æ”¶é›†çš„ä¸­æ–‡æ–‡æœ¬ï¼Œå¯ä»¥ä½¿ç”¨é¡¹ç›®æä¾›çš„è½¬æ¢å·¥å…·ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨å†…ç½®å·¥å…·è½¬æ¢è‡ªå®šä¹‰æ•°æ®\n",
    "# è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ data_check/convert_custom_data.py\n",
    "\n",
    "print(\"ğŸ› ï¸ è½¬æ¢è‡ªå®šä¹‰æ–‡æœ¬æ•°æ®çš„æ­¥éª¤ï¼š\\n\")\n",
    "print(\"1. å‡†å¤‡ä½ çš„æ–‡æœ¬æ•°æ®ï¼ˆ.txt æ–‡ä»¶ï¼‰\")\n",
    "print(\"2. è¿è¡Œè½¬æ¢å‘½ä»¤ï¼š\")\n",
    "print(\"   python -m data_check.convert_custom_data\")\n",
    "print(\"\\næ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š\")\n",
    "print(\"   â€¢ å•ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€æ¡æ•°æ®ï¼‰\")\n",
    "print(\"   â€¢ ç›®å½•ï¼ˆåŒ…å«å¤šä¸ª .txt æ–‡ä»¶ï¼‰\")\n",
    "print(\"\\nè¯¦ç»†ä»£ç è¯·æŸ¥çœ‹ï¼šdata_check/convert_custom_data.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"æ•°æ®è´¨é‡æ£€æŸ¥\"></a>6. æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·\n",
    "\n",
    "é¡¹ç›®æä¾›äº†å®Œæ•´çš„æ•°æ®æ£€æŸ¥å·¥å…·é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ£€æŸ¥å·¥å…·æ¦‚è§ˆ\n",
    "tools = pd.DataFrame({\n",
    "    'å·¥å…·': [\n",
    "        'check_data.py',\n",
    "        'check_length_distribution.py',\n",
    "        'check_content_quality.py',\n",
    "        'check_char_distribution.py',\n",
    "        'convert_custom_data.py'\n",
    "    ],\n",
    "    'ç”¨é€”': [\n",
    "        'éªŒè¯æ•°æ®æ–‡ä»¶å®Œæ•´æ€§',\n",
    "        'æ£€æŸ¥æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ',\n",
    "        'æŠ½æ ·æ£€æŸ¥å†…å®¹è´¨é‡',\n",
    "        'æ£€æŸ¥å­—ç¬¦åˆ†å¸ƒç»Ÿè®¡',\n",
    "        'è½¬æ¢è‡ªå®šä¹‰æ–‡æœ¬æ•°æ®'\n",
    "    ],\n",
    "    'å‘½ä»¤': [\n",
    "        'python -m data_check.check_data',\n",
    "        'python -m data_check.check_length_distribution',\n",
    "        'python -m data_check.check_content_quality',\n",
    "        'python -m data_check.check_char_distribution',\n",
    "        'python -m data_check.convert_custom_data'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ› ï¸ æ•°æ®æ£€æŸ¥å·¥å…·æ€»è§ˆ\\n\")\n",
    "display(tools)\n",
    "print(\"\\nğŸ’¡ æ‰€æœ‰å·¥å…·çš„è¯¦ç»†ä»£ç éƒ½åœ¨ data_check/ ç›®å½•ä¸‹\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¿«é€Ÿæ£€æŸ¥æ•°æ®å®Œæ•´æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥\n",
    "!python -m data_check.check_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ£€æŸ¥æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†ææ•°æ®çš„é•¿åº¦åˆ†å¸ƒ\n",
    "# è¿™æœ‰åŠ©äºäº†è§£æ•°æ®è´¨é‡\n",
    "\n",
    "!python -m data_check.check_length_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"æ•°æ®é‡è®¡ç®—å™¨\"></a>7. æ•°æ®é‡è®¡ç®—å™¨\n",
    "\n",
    "### Chinchilla å®šå¾‹\n",
    "\n",
    "**æ•°æ® token æ•° = æ¨¡å‹å‚æ•°é‡ Ã— 20**\n",
    "\n",
    "è®©æˆ‘ä»¬è®¡ç®—ä¸åŒæ¨¡å‹éœ€è¦å¤šå°‘æ•°æ®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_requirement(model_params_million):\n",
    "    \"\"\"\n",
    "    è®¡ç®—è®­ç»ƒæ‰€éœ€çš„æ•°æ®é‡\n",
    "    \n",
    "    å‚æ•°:\n",
    "        model_params_million: æ¨¡å‹å‚æ•°é‡(ç™¾ä¸‡)ï¼Œå¦‚123è¡¨ç¤º123Må‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        å­—å…¸ï¼ŒåŒ…å«å„ç§æ•°æ®é‡ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. éœ€è¦çš„ token æ•°ï¼ˆå‚æ•°é‡ Ã— 20ï¼‰\n",
    "    tokens_billion = model_params_million / 1000 * 20\n",
    "    \n",
    "    # 2. éœ€è¦çš„å­—ç¬¦æ•°ï¼ˆ1 token â‰ˆ 4.8 å­—ç¬¦ï¼‰\n",
    "    chars_billion = tokens_billion * 4.8\n",
    "    \n",
    "    # 3. éœ€è¦çš„åˆ†ç‰‡æ•°ï¼ˆæ¯ä¸ªåˆ†ç‰‡ 250M å­—ç¬¦ï¼‰\n",
    "    num_shards = int(chars_billion * 1000 / 250)\n",
    "    \n",
    "    # 4. ç£ç›˜ç©ºé—´ï¼ˆæ¯ä¸ªåˆ†ç‰‡çº¦ 100MBï¼‰\n",
    "    disk_gb = num_shards * 100 / 1024\n",
    "    \n",
    "    return {\n",
    "        'model_params': f\"{model_params_million}M\",\n",
    "        'tokens': f\"{tokens_billion:.1f}B\",\n",
    "        'chars': f\"{chars_billion:.0f}B\",\n",
    "        'shards': num_shards,\n",
    "        'disk': f\"{disk_gb:.1f}GB\"\n",
    "    }\n",
    "\n",
    "# ä¸åŒè§„æ¨¡æ¨¡å‹\n",
    "models = {\n",
    "    'd10': 42,\n",
    "    'd12': 123,\n",
    "    'd20': 561,\n",
    "    'd26': 1200,\n",
    "    'd32': 2100\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, params in models.items():\n",
    "    req = calculate_data_requirement(params)\n",
    "    results.append({\n",
    "        'æ¨¡å‹': name,\n",
    "        'å‚æ•°é‡': req['model_params'],\n",
    "        'Tokenæ•°': req['tokens'],\n",
    "        'å­—ç¬¦æ•°': req['chars'],\n",
    "        'åˆ†ç‰‡æ•°': req['shards'],\n",
    "        'ç£ç›˜': req['disk']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nğŸ“Š æ¨¡å‹æ•°æ®éœ€æ±‚è®¡ç®—è¡¨\\n\")\n",
    "display(df_results)\n",
    "print(\"\\nğŸ’¡ æç¤ºï¼šæ•°æ®é‡åŸºäº Chinchilla å®šå¾‹è®¡ç®—ï¼ˆå‚æ•°é‡ Ã— 20ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è‡ªå®šä¹‰è®¡ç®—\n",
    "\n",
    "è¾“å…¥ä½ çš„æ¨¡å‹å‚æ•°é‡ï¼Œè®¡ç®—éœ€è¦å¤šå°‘æ•°æ®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå®šä¹‰æ¨¡å‹å‚æ•°é‡ï¼ˆå•ä½ï¼šç™¾ä¸‡ï¼‰\n",
    "my_model_params = 100  # ä¿®æ”¹è¿™é‡Œï¼\n",
    "\n",
    "result = calculate_data_requirement(my_model_params)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ‚¨çš„æ¨¡å‹ï¼ˆ{my_model_params}M å‚æ•°ï¼‰éœ€è¦ï¼š\\n\")\n",
    "print(f\"   Token æ•°é‡ï¼š{result['tokens']}\")\n",
    "print(f\"   å­—ç¬¦æ•°é‡ï¼š{result['chars']}\")\n",
    "print(f\"   æ•°æ®åˆ†ç‰‡ï¼š{result['shards']} ä¸ª\")\n",
    "print(f\"   ç£ç›˜ç©ºé—´ï¼š{result['disk']}\")\n",
    "print(\"\\nä¸‹è½½å‘½ä»¤ï¼š\")\n",
    "print(f\"   python -m nanochat.dataset -n {result['shards']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"æ£€æŸ¥æ¸…å•\"></a>8. å®Œæ•´æµç¨‹æ£€æŸ¥æ¸…å•\n",
    "\n",
    "å‡†å¤‡å¥½æ•°æ®äº†å—ï¼Ÿå¯¹ç…§è¿™ä¸ªæ¸…å•æ£€æŸ¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def check_data_readiness():\n",
    "    \"\"\"æ£€æŸ¥æ•°æ®å‡†å¤‡æƒ…å†µ\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ” æ•°æ®å‡†å¤‡çŠ¶æ€æ£€æŸ¥\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # 1. æ£€æŸ¥é¢„è®­ç»ƒæ•°æ®\n",
    "    base_data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "    if base_data_dir.exists():\n",
    "        parquet_files = list(base_data_dir.glob(\"*.parquet\"))\n",
    "        if len(parquet_files) >= 8:\n",
    "            checks.append((\"âœ…\", f\"é¢„è®­ç»ƒæ•°æ®ï¼šæ‰¾åˆ° {len(parquet_files)} ä¸ªåˆ†ç‰‡\"))\n",
    "        else:\n",
    "            checks.append((\"âš ï¸\", f\"é¢„è®­ç»ƒæ•°æ®ï¼šåªæœ‰ {len(parquet_files)} ä¸ªåˆ†ç‰‡ï¼ˆå»ºè®®è‡³å°‘ 8 ä¸ªï¼‰\"))\n",
    "    else:\n",
    "        checks.append((\"âŒ\", \"é¢„è®­ç»ƒæ•°æ®ï¼šæœªä¸‹è½½\"))\n",
    "    \n",
    "    # 2. æ£€æŸ¥åˆ†è¯å™¨\n",
    "    tokenizer_dir = Path.home() / \".cache\" / \"nanochat\" / \"tokenizer\"\n",
    "    if tokenizer_dir.exists() and list(tokenizer_dir.glob(\"*.model\")):\n",
    "        checks.append((\"âœ…\", \"åˆ†è¯å™¨ï¼šå·²è®­ç»ƒ\"))\n",
    "    else:\n",
    "        checks.append((\"âš ï¸\", \"åˆ†è¯å™¨ï¼šæœªè®­ç»ƒï¼ˆéœ€è¦è¿è¡Œ tok_trainï¼‰\"))\n",
    "    \n",
    "    # 3. æ£€æŸ¥ç£ç›˜ç©ºé—´\n",
    "    cache_dir = Path.home() / \".cache\"\n",
    "    if cache_dir.exists():\n",
    "        try:\n",
    "            stat = shutil.disk_usage(cache_dir)\n",
    "            free_gb = stat.free / (1024**3)\n",
    "            if free_gb > 30:\n",
    "                checks.append((\"âœ…\", f\"ç£ç›˜ç©ºé—´ï¼šå‰©ä½™ {free_gb:.1f} GB\"))\n",
    "            else:\n",
    "                checks.append((\"âš ï¸\", f\"ç£ç›˜ç©ºé—´ï¼šå‰©ä½™ {free_gb:.1f} GBï¼ˆå»ºè®®è‡³å°‘ 30GBï¼‰\"))\n",
    "        except:\n",
    "            checks.append((\"â„¹ï¸\", \"ç£ç›˜ç©ºé—´ï¼šæ— æ³•æ£€æµ‹\"))\n",
    "    \n",
    "    # 4. æ£€æŸ¥ç¯å¢ƒå˜é‡\n",
    "    if 'HF_ENDPOINT' in os.environ:\n",
    "        checks.append((\"âœ…\", f\"HuggingFace é•œåƒï¼š{os.environ['HF_ENDPOINT']}\"))\n",
    "    else:\n",
    "        checks.append((\"â„¹ï¸\", \"HuggingFace é•œåƒï¼šæœªè®¾ç½®ï¼ˆå›½å†…ç”¨æˆ·å»ºè®®è®¾ç½®ï¼‰\"))\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    for status, msg in checks:\n",
    "        print(f\"{status} {msg}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # æ€»ç»“\n",
    "    ready_count = sum(1 for s, _ in checks if s == \"âœ…\")\n",
    "    total_count = len(checks)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š å°±ç»ªçŠ¶æ€ï¼š{ready_count}/{total_count}\")\n",
    "    \n",
    "    if ready_count >= 2:  # è‡³å°‘æœ‰æ•°æ®å’Œç©ºé—´å°±ç®—åŸºæœ¬å°±ç»ª\n",
    "        print(\"\\nğŸ‰ æ•°æ®åŸºæœ¬å‡†å¤‡å®Œæˆï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼\")\n",
    "    else:\n",
    "        print(\"\\nğŸ’¡ è¿˜æœ‰ä¸€äº›å‡†å¤‡å·¥ä½œéœ€è¦å®Œæˆï¼Œè¯·æŸ¥çœ‹ä¸Šé¢çš„æç¤º\")\n",
    "\n",
    "# è¿è¡Œæ£€æŸ¥\n",
    "check_data_readiness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ ä¸‹ä¸€æ­¥\n",
    "\n",
    "æ•°æ®å‡†å¤‡å¥½äº†ï¼æ¥ä¸‹æ¥ï¼š\n",
    "\n",
    "### 1. è®­ç»ƒåˆ†è¯å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒåˆ†è¯å™¨\n",
    "# âš ï¸ è­¦å‘Šï¼šè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼\n",
    "\n",
    "# !python -m scripts.tok_train --max_chars=2000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. å¼€å§‹é¢„è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹é¢„è®­ç»ƒï¼ˆéœ€è¦ GPUï¼‰\n",
    "# âš ï¸ è­¦å‘Šï¼šè¿™éœ€è¦å¤§é‡æ—¶é—´å’Œè®¡ç®—èµ„æºï¼\n",
    "\n",
    "# !torchrun --standalone --nproc_per_node=8 -m scripts.base_train --depth=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š æ‰©å±•é˜…è¯»\n",
    "\n",
    "æƒ³æ·±å…¥äº†è§£ï¼Ÿæ¨èé˜…è¯»ï¼š\n",
    "\n",
    "- ğŸ“„ [README.md](README.md) - é¡¹ç›®æ•´ä½“ä»‹ç»\n",
    "- ğŸ“„ [æ•°æ®.md](æ•°æ®.md) - æ›´è¯¦ç»†çš„æ•°æ®æ–‡æ¡£\n",
    "- ğŸŒ [FineWeb-Edu æ•°æ®é›†](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n",
    "- ğŸ“š [Chinchilla è®ºæ–‡](https://arxiv.org/abs/2203.15556) - ç†è§£æ•°æ®é‡å’Œæ¨¡å‹è§„æ¨¡çš„å…³ç³»\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¬ éœ€è¦å¸®åŠ©ï¼Ÿ\n",
    "\n",
    "é‡åˆ°é—®é¢˜äº†ï¼Ÿ\n",
    "\n",
    "1. å…ˆæŸ¥çœ‹æœ¬æ–‡æ¡£çš„å„ä¸ªç« èŠ‚\n",
    "2. è¿è¡Œæ•°æ®æ£€æŸ¥å·¥å…·è¯Šæ–­é—®é¢˜\n",
    "3. æŸ¥çœ‹ `æ•°æ®.md` æ–‡æ¡£è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯\n",
    "4. æŸ¥çœ‹é¡¹ç›®çš„ GitHub Issues\n",
    "\n",
    "---\n",
    "\n",
    "**ç¥ä½ è®­ç»ƒé¡ºåˆ©ï¼** ğŸ‰\n",
    "\n",
    "> ğŸ’¡ è®°ä½ï¼šæ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦ï¼ä¸è¦ç›²ç›®è¿½æ±‚å¤§æ•°æ®ï¼Œå…ˆç”¨å°æ¨¡å‹éªŒè¯æµç¨‹ï¼Œå†é€æ­¥æ‰©å¤§è§„æ¨¡ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
