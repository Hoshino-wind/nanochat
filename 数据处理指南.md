# nanochat æ•°æ®å¤„ç†å®Œå…¨æŒ‡å—

> æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç» nanochat é¡¹ç›®çš„æ•°æ®å¤„ç†æµç¨‹ï¼Œé€‚åˆåˆå­¦è€…é˜…è¯»ã€‚

## ğŸ“š ç›®å½•

1. [æ•°æ®å¤„ç†æ¦‚è§ˆ](#æ•°æ®å¤„ç†æ¦‚è§ˆ)
2. [æ•°æ®ç±»å‹ä¸æ¥æº](#æ•°æ®ç±»å‹ä¸æ¥æº)
3. [æ•°æ®å¤„ç†æµç¨‹](#æ•°æ®å¤„ç†æµç¨‹)
4. [è¯¦ç»†æ“ä½œæ­¥éª¤](#è¯¦ç»†æ“ä½œæ­¥éª¤)
5. [å¸¸è§é—®é¢˜è§£ç­”](#å¸¸è§é—®é¢˜è§£ç­”)

---

## æ•°æ®å¤„ç†æ¦‚è§ˆ

nanochat é¡¹ç›®è®­ç»ƒä¸€ä¸ªå®Œæ•´çš„ ChatGPT é£æ ¼çš„è¯­è¨€æ¨¡å‹ï¼Œéœ€è¦ç»å†**ä¸‰ä¸ªä¸»è¦é˜¶æ®µ**ï¼Œæ¯ä¸ªé˜¶æ®µä½¿ç”¨ä¸åŒçš„æ•°æ®ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   é¢„è®­ç»ƒé˜¶æ®µ     â”‚  â†’   â”‚   ä¸­æœŸè®­ç»ƒé˜¶æ®µ   â”‚  â†’   â”‚   å¾®è°ƒé˜¶æ®µ       â”‚
â”‚  (Pretraining)  â”‚      â”‚  (Midtraining)  â”‚      â”‚  (Fine-tuning)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®           å¯¹è¯æ ¼å¼æ•°æ®            æŒ‡ä»¤å¯¹è¯æ•°æ®
    å­¦ä¹ åŸºç¡€è¯­è¨€èƒ½åŠ›         å­¦ä¹ å¯¹è¯ç»“æ„            å­¦ä¹ éµå¾ªæŒ‡ä»¤
```

### ä¸ºä»€ä¹ˆéœ€è¦ä¸‰ä¸ªé˜¶æ®µï¼Ÿ

- **é¢„è®­ç»ƒ**ï¼šè®©æ¨¡å‹å­¦ä¹ è¯­è¨€çš„åŸºæœ¬è§„å¾‹ï¼ˆè¯­æ³•ã€å¸¸è¯†ã€æ¨ç†ç­‰ï¼‰
- **ä¸­æœŸè®­ç»ƒ**ï¼šè®©æ¨¡å‹å­¦ä¼šå¯¹è¯çš„æ ¼å¼å’Œç»“æ„
- **å¾®è°ƒ**ï¼šè®©æ¨¡å‹å­¦ä¼šæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œç”¨æˆ·æŒ‡ä»¤

---

## æ•°æ®ç±»å‹ä¸æ¥æº

### 1. é¢„è®­ç»ƒæ•°æ®ï¼ˆBase Training Dataï¼‰

**æ•°æ®æ¥æº**ï¼šFineWeb-Edu æ•°æ®é›†

- **æ•°æ®é›†**ï¼š`HuggingFaceFW/fineweb-edu`ï¼ˆsample-100BTï¼‰
- **æ•°æ®è§„æ¨¡**ï¼šçº¦ 100Bï¼ˆ1000 äº¿ï¼‰ä¸ª GPT-2 tokens
- **æ•°æ®å†…å®¹**ï¼šé«˜è´¨é‡çš„ç½‘é¡µæ–‡æœ¬æ•°æ®
- **æ•°æ®æ ¼å¼**ï¼šParquet æ–‡ä»¶ï¼ˆå‹ç¼©åçš„è¡¨æ ¼æ ¼å¼ï¼‰

**æ•°æ®ç‰¹ç‚¹**ï¼š

- æ¯ä¸ªæ•°æ®åˆ†ç‰‡ï¼ˆshardï¼‰çº¦ 250M å­—ç¬¦
- å‹ç¼©åæ¯ä¸ªæ–‡ä»¶çº¦ 100MB
- ä½¿ç”¨ zstd å‹ç¼©ç®—æ³•
- æ€»å…±çº¦ 1822 ä¸ªåˆ†ç‰‡

**ä¸‹è½½æ–¹å¼**ï¼š

```bash
# ä¸‹è½½ 8 ä¸ªåˆ†ç‰‡ç”¨äºè®­ç»ƒåˆ†è¯å™¨ï¼ˆçº¦ 800MBï¼‰
python -m nanochat.dataset -n 8

# ä¸‹è½½ 240 ä¸ªåˆ†ç‰‡ç”¨äºé¢„è®­ç»ƒï¼ˆçº¦ 24GBï¼‰
python -m nanochat.dataset -n 240
```

### 2. ä¸­æœŸè®­ç»ƒæ•°æ®ï¼ˆMidtraining Dataï¼‰

**æ•°æ®æ¥æº**ï¼šSmolTalk å¯¹è¯æ•°æ®é›†

- **æ•°æ®é›†**ï¼š`HuggingFaceTB/smoltalk`
- **æ•°æ®å†…å®¹**ï¼šçœŸå®çš„å¤šè½®å¯¹è¯æ•°æ®
- **æ•°æ®æ ¼å¼**ï¼šå¯¹è¯æ¶ˆæ¯åˆ—è¡¨

**æ•°æ®ç»“æ„ç¤ºä¾‹**ï¼š

```json
{
  "messages": [
    { "role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±" },
    { "role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹..." }
  ]
}
```

### 3. å¾®è°ƒæ•°æ®ï¼ˆSFT Dataï¼‰

**æ•°æ®æ¥æº**ï¼šå¤šä¸ªä»»åŠ¡æ•°æ®é›†çš„æ··åˆ

- **ARC-Easy**ï¼šç®€å•çš„é€‰æ‹©é¢˜ï¼ˆ2.3K æ¡ï¼‰
- **ARC-Challenge**ï¼šå›°éš¾çš„é€‰æ‹©é¢˜ï¼ˆ1.1K æ¡ï¼‰
- **GSM8K**ï¼šæ•°å­¦æ¨ç†é¢˜ï¼ˆ8K æ¡ï¼‰
- **SmolTalk**ï¼šå¯¹è¯æ•°æ®ï¼ˆ10K æ¡ï¼‰

**æ€»è®¡**ï¼šçº¦ 21.4K æ¡è®­ç»ƒæ ·æœ¬

---

## æ•°æ®å¤„ç†æµç¨‹

### æµç¨‹å›¾

```
åŸå§‹æ•°æ®ä¸‹è½½
    â†“
æ•°æ®é‡æ–°æ‰“åŒ…ï¼ˆrepackageï¼‰
    â†“
æ„å»ºåˆ†è¯å™¨ï¼ˆtokenizerï¼‰
    â†“
æ•°æ®åˆ†è¯åŒ–ï¼ˆtokenizationï¼‰
    â†“
åˆ›å»ºè®­ç»ƒ/éªŒè¯é›†
    â†“
æ‰¹æ¬¡åŠ è½½ï¼ˆbatchingï¼‰
    â†“
è¾“å…¥æ¨¡å‹è®­ç»ƒ
```

### è¯¦ç»†è¯´æ˜

#### æ­¥éª¤ 1: æ•°æ®ä¸‹è½½ä¸é‡æ–°æ‰“åŒ…

**ç›®çš„**ï¼šå°† HuggingFace æ•°æ®é›†è½¬æ¢ä¸ºé€‚åˆæµå¼åŠ è½½çš„æ ¼å¼

**è¿‡ç¨‹**ï¼ˆå‚è€ƒ `dev/repackage_data_reference.py`ï¼‰ï¼š

1. **åŠ è½½åŸå§‹æ•°æ®é›†**

   ```python
   from datasets import load_dataset
   ds = load_dataset("HuggingFaceFW/fineweb-edu", name="sample-100BT", split="train")
   ```

2. **æ•°æ®æ··æ´—**

   ```python
   ds = ds.shuffle(seed=42)  # æ‰“ä¹±æ•°æ®é¡ºåºï¼Œé¿å…æ•°æ®åˆ†å¸ƒåå·®
   ```

3. **é‡æ–°æ‰“åŒ…æˆåˆ†ç‰‡**

   - æ¯ä¸ªåˆ†ç‰‡åŒ…å« 250M å­—ç¬¦
   - ä½¿ç”¨ Parquet æ ¼å¼å­˜å‚¨
   - Row group size = 1024ï¼ˆä¾¿äºåˆ†å¸ƒå¼è¯»å–ï¼‰
   - ä½¿ç”¨ zstd å‹ç¼©ï¼ˆlevel 3ï¼‰

4. **ä¸Šä¼ åˆ° HuggingFace**
   - æ•°æ®é›† IDï¼š`karpathy/fineweb-edu-100b-shuffle`
   - è¿™æ ·å¯ä»¥å¿«é€Ÿä¸‹è½½ï¼Œæ— éœ€è‡ªå·±é‡æ–°æ‰“åŒ…

#### æ­¥éª¤ 2: è®­ç»ƒåˆ†è¯å™¨

**ç›®çš„**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°å­—åºåˆ—

**è¿‡ç¨‹**ï¼ˆå‚è€ƒ `scripts/tok_train.py`ï¼‰ï¼š

1. **å‡†å¤‡è®­ç»ƒæ•°æ®**

   ```python
   # ä½¿ç”¨å‰ 2Bï¼ˆ20äº¿ï¼‰å­—ç¬¦è®­ç»ƒåˆ†è¯å™¨
   python -m scripts.tok_train --max_chars=2000000000
   ```

2. **è®­ç»ƒ BPE åˆ†è¯å™¨**

   - ä½¿ç”¨ Rust å®ç°çš„é«˜æ•ˆ BPEï¼ˆByte Pair Encodingï¼‰
   - è¯æ±‡è¡¨å¤§å°ï¼š65,536ï¼ˆ2^16ï¼‰
   - è®­ç»ƒè¿‡ç¨‹ï¼šå­¦ä¹ æœ€å¸¸è§çš„å­—ç¬¦ç»„åˆ

3. **ä¿å­˜åˆ†è¯å™¨**
   - ä¿å­˜ä½ç½®ï¼š`~/.cache/nanochat/tokenizer/`
   - åŒ…å«è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™

**ä»€ä¹ˆæ˜¯åˆ†è¯å™¨ï¼Ÿ**

åˆ†è¯å™¨å°±åƒä¸€ä¸ª"ç¿»è¯‘å®˜"ï¼ŒæŠŠäººç±»çš„æ–‡å­—è½¬æ¢æˆæ¨¡å‹èƒ½ç†è§£çš„æ•°å­—ï¼š

```
æ–‡æœ¬: "Hello world"
   â†“ (åˆ†è¯)
Token IDs: [15496, 995]
   â†“ (è®­ç»ƒ)
æ¨¡å‹å¤„ç†æ•°å­—
   â†“ (è§£ç )
æ–‡æœ¬: "Hello world"
```

#### æ­¥éª¤ 3: æ•°æ®åˆ†è¯åŒ–ä¸åŠ è½½

**ç›®çš„**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸º token åºåˆ—ï¼Œå¹¶æŒ‰éœ€åŠ è½½

**ç‰¹ç‚¹**ï¼š

- **å³æ—¶åˆ†è¯**ï¼ˆOn-the-fly tokenizationï¼‰
- **æµå¼åŠ è½½**ï¼šä¸éœ€è¦å°†æ‰€æœ‰æ•°æ®é¢„å…ˆåˆ†è¯
- **åˆ†å¸ƒå¼æ”¯æŒ**ï¼šå¤š GPU è‡ªåŠ¨åˆ’åˆ†æ•°æ®

**æ•°æ®åŠ è½½å™¨å·¥ä½œåŸç†**ï¼š

```python
# ä¼ªä»£ç ç¤ºä¾‹
def tokenizing_distributed_data_loader(batch_size, seq_len, split):
    """
    batch_size: æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°
    seq_len: åºåˆ—é•¿åº¦ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰
    split: "train" æˆ– "val"
    """
    while True:
        # 1. ä»ç£ç›˜è¯»å–ä¸€ä¸ª parquet åˆ†ç‰‡
        shard = load_next_shard()

        # 2. éå†åˆ†ç‰‡ä¸­çš„æ–‡æ¡£
        for doc in shard:
            text = doc['text']

            # 3. ä½¿ç”¨åˆ†è¯å™¨è½¬æ¢æ–‡æœ¬
            tokens = tokenizer.encode(text)

            # 4. åˆ‡åˆ†æˆå›ºå®šé•¿åº¦çš„åºåˆ—
            for i in range(0, len(tokens) - seq_len, seq_len):
                inputs = tokens[i:i+seq_len]
                targets = tokens[i+1:i+seq_len+1]  # ç›®æ ‡æ˜¯è¾“å…¥å‘å³åç§»1ä½

                # 5. ç»„è£…æˆæ‰¹æ¬¡
                batch_inputs.append(inputs)
                batch_targets.append(targets)

                if len(batch_inputs) == batch_size:
                    yield (batch_inputs, batch_targets)
                    batch_inputs = []
                    batch_targets = []
```

**ä¸ºä»€ä¹ˆä½¿ç”¨è¿™ç§æ–¹å¼ï¼Ÿ**

- âœ… èŠ‚çœç£ç›˜ç©ºé—´ï¼ˆä¸éœ€è¦é¢„å…ˆåˆ†è¯å¹¶ä¿å­˜ï¼‰
- âœ… èŠ‚çœå†…å­˜ï¼ˆæµå¼åŠ è½½ï¼Œåªä¿ç•™å½“å‰éœ€è¦çš„æ•°æ®ï¼‰
- âœ… çµæ´»æ€§é«˜ï¼ˆå¯ä»¥éšæ—¶æ›´æ¢åˆ†è¯å™¨ï¼‰

---

## è¯¦ç»†æ“ä½œæ­¥éª¤

### åœºæ™¯ 1ï¼šè¿è¡Œé»˜è®¤çš„é¢„è®­ç»ƒï¼ˆè‹±æ–‡ï¼‰

**å‰ææ¡ä»¶**ï¼š

- 8 å¼  H100 æˆ– A100 GPU
- è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼ˆçº¦ 30GBï¼‰

**æ­¥éª¤**ï¼š

```bash
# 1. ä¸‹è½½åˆå§‹æ•°æ®ï¼ˆç”¨äºè®­ç»ƒåˆ†è¯å™¨ï¼‰
python -m nanochat.dataset -n 8

# 2. åå°ä¸‹è½½å®Œæ•´è®­ç»ƒæ•°æ®
python -m nanochat.dataset -n 240 &

# 3. è®­ç»ƒåˆ†è¯å™¨
python -m scripts.tok_train --max_chars=2000000000

# 4. è¯„ä¼°åˆ†è¯å™¨
python -m scripts.tok_eval

# 5. å¼€å§‹é¢„è®­ç»ƒ
torchrun --standalone --nproc_per_node=8 -m scripts.base_train --depth=20

# 6. è¯„ä¼°æ¨¡å‹
torchrun --standalone --nproc_per_node=8 -m scripts.base_eval
```

**æˆ–è€…ç›´æ¥è¿è¡Œä¸€é”®è„šæœ¬**ï¼š

```bash
bash speedrun.sh
```

### åœºæ™¯ 2ï¼šè®­ç»ƒä¸­æ–‡æ¨¡å‹

**æ­¥éª¤ 1ï¼šå‡†å¤‡ä¸­æ–‡æ•°æ®**

åˆ›å»ºæ•°æ®ä¸‹è½½è„šæœ¬ï¼š

```python
# download_chinese_data.py
from datasets import load_dataset, concatenate_datasets
import os

print("ğŸ“¥ ä¸‹è½½ä¸­æ–‡æ•°æ®é›†...")

# 1. ä¸­æ–‡ç»´åŸºç™¾ç§‘
wiki = load_dataset("wikipedia", "20220301.zh", split="train[:100000]")
print(f"âœ… ç»´åŸºç™¾ç§‘: {len(wiki)} æ¡")

# 2. ç™¾åº¦ç™¾ç§‘
baike = load_dataset("xusenlin/baidubaike-563w", split="train[:200000]")
print(f"âœ… ç™¾åº¦ç™¾ç§‘: {len(baike)} æ¡")

# 3. æ–°é—»æ•°æ®
news = load_dataset("THUDM/LongBench", "news", split="train[:50000]")
print(f"âœ… æ–°é—»æ•°æ®: {len(news)} æ¡")

# åˆå¹¶æ•°æ®é›†
combined = concatenate_datasets([wiki, baike, news])

# æ•°æ®æ¸…æ´—
import re

def clean_text(example):
    text = example.get('text', example.get('content', ''))
    # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œæ ‡ç‚¹
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹\s]', '', text)
    if len(text) < 50:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
        return None
    return {'text': text}

combined = combined.map(clean_text)
combined = combined.filter(lambda x: x is not None)

print(f"\nâœ… æ¸…æ´—åæ•°æ®é‡: {len(combined)} æ¡")

# ä¿å­˜ä¸º parquet æ ¼å¼
output_dir = os.path.expanduser("~/.cache/nanochat/chinese_data")
os.makedirs(output_dir, exist_ok=True)

# åˆ†å‰²æˆå¤šä¸ªåˆ†ç‰‡ï¼ˆæ¯ä¸ªçº¦ 10 ä¸‡æ¡ï¼‰
shard_size = 100000
num_shards = (len(combined) + shard_size - 1) // shard_size

for i in range(num_shards):
    start = i * shard_size
    end = min(start + shard_size, len(combined))
    shard = combined.select(range(start, end))

    shard_path = f"{output_dir}/shard_{i:05d}.parquet"
    shard.to_parquet(shard_path, compression='zstd', compression_level=3)

    print(f"ğŸ’¾ å·²ä¿å­˜ {shard_path}: {len(shard)} æ¡")

print("\nâœ… ä¸­æ–‡æ•°æ®å‡†å¤‡å®Œæˆï¼")
```

è¿è¡Œï¼š

```bash
python download_chinese_data.py
```

**æ­¥éª¤ 2ï¼šè®­ç»ƒä¸­æ–‡åˆ†è¯å™¨**

ä¸­æ–‡åˆ†è¯å™¨éœ€è¦æ›´å¤§çš„è¯æ±‡è¡¨ï¼ˆå› ä¸ºä¸­æ–‡å­—ç¬¦æ›´å¤šï¼‰ï¼š

```bash
# ä¿®æ”¹ tok_train.py ä¸­çš„ vocab_size å‚æ•°
python -m scripts.tok_train --max_chars=2000000000 --vocab_size=80000
```

**æ­¥éª¤ 3ï¼šä¿®æ”¹è®­ç»ƒé…ç½®**

ä½¿ç”¨ `configs/chinese_d20.yaml` é…ç½®æ–‡ä»¶ï¼Œæˆ–æ‰‹åŠ¨æŒ‡å®šå‚æ•°ï¼š

```bash
torchrun --standalone --nproc_per_node=8 -m scripts.base_train \
    --depth=20 \
    --vocab_size=80000 \
    --device_batch_size=16  # ä¸­æ–‡æ¨¡å‹å¯èƒ½éœ€è¦è°ƒæ•´æ‰¹æ¬¡å¤§å°
```

**å®Œæ•´æµç¨‹å¯å‚è€ƒ**ï¼š`scripts/train_chinese.sh`

### åœºæ™¯ 3ï¼šå‡†å¤‡è‡ªå®šä¹‰å¯¹è¯æ•°æ®ï¼ˆå¾®è°ƒï¼‰

**æ•°æ®æ ¼å¼è¦æ±‚**ï¼š

æ¯æ¡æ•°æ®æ˜¯ä¸€ä¸ªåŒ…å«å¤šè½®å¯¹è¯çš„ JSON å¯¹è±¡ï¼š

```json
{
  "messages": [
    {
      "role": "user",
      "content": "å¦‚ä½•åšç•ªèŒ„ç‚’è›‹ï¼Ÿ"
    },
    {
      "role": "assistant",
      "content": "åšç•ªèŒ„ç‚’è›‹çš„æ­¥éª¤å¦‚ä¸‹ï¼š\n1. å‡†å¤‡ææ–™ï¼šé¸¡è›‹3ä¸ªï¼Œç•ªèŒ„2ä¸ª...\n2. å°†é¸¡è›‹æ‰“æ•£..."
    },
    {
      "role": "user",
      "content": "éœ€è¦æ”¾ç³–å—ï¼Ÿ"
    },
    {
      "role": "assistant",
      "content": "å¯ä»¥æ”¾ä¸€ç‚¹ç‚¹ç³–ï¼Œèƒ½è®©ç•ªèŒ„çš„é…¸å‘³æ›´æŸ”å’Œ..."
    }
  ]
}
```

**å‡†å¤‡æ•°æ®æ­¥éª¤**ï¼š

1. **ä¸‹è½½æˆ–å‡†å¤‡æ•°æ®é›†**

ä¾‹å¦‚ï¼Œä½¿ç”¨ Belle ä¸­æ–‡å¯¹è¯æ•°æ®é›†ï¼š

```python
from datasets import load_dataset

# ä¸‹è½½ Belle æ•°æ®é›†
belle = load_dataset("BelleGroup/train_1M_CN", split="train[:100000]")

# è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
def convert_format(example):
    instruction = example['instruction']
    input_text = example.get('input', '')
    output = example['output']

    if input_text:
        user_content = f"{instruction}\n\n{input_text}"
    else:
        user_content = instruction

    return {
        "messages": [
            {"role": "user", "content": user_content},
            {"role": "assistant", "content": output}
        ]
    }

converted = belle.map(convert_format)

# ä¿å­˜ä¸º JSONL æ ¼å¼
output_dir = os.path.expanduser("~/.cache/nanochat/chinese_sft")
os.makedirs(output_dir, exist_ok=True)
converted.to_json(f"{output_dir}/belle_100k.jsonl")
```

2. **ä½¿ç”¨è‡ªå®šä¹‰ä»»åŠ¡ç±»åŠ è½½æ•°æ®**

å‚è€ƒ `tasks/smoltalk.py` åˆ›å»ºè‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨ï¼š

```python
# tasks/my_chinese_chat.py
from tasks.common import Task

class MyChineseChat(Task):
    def __init__(self, split, stop=None):
        from datasets import load_dataset
        # ä»æœ¬åœ° JSONL æ–‡ä»¶åŠ è½½
        data_path = "~/.cache/nanochat/chinese_sft/belle_100k.jsonl"
        ds = load_dataset("json", data_files=data_path, split="train")

        # åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†
        ds = ds.train_test_split(test_size=0.1, seed=42)
        self.data = ds[split]

        if stop is not None:
            self.data = self.data.select(range(min(stop, len(self.data))))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

3. **åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨**

ä¿®æ”¹ `scripts/chat_sft.py`ï¼š

```python
from tasks.my_chinese_chat import MyChineseChat

train_ds = TaskMixture([
    MyChineseChat(split="train", stop=10_000),
    # å¯ä»¥æ··åˆå…¶ä»–ä»»åŠ¡...
])
```

---

## æ•°æ®å¤„ç†æŠ€æœ¯ç»†èŠ‚

### 1. æ•°æ®åˆ†ç‰‡ç­–ç•¥

**ä¸ºä»€ä¹ˆè¦åˆ†ç‰‡ï¼Ÿ**

- ä¾¿äºåˆ†å¸ƒå¼ä¸‹è½½å’ŒåŠ è½½
- æ”¯æŒæ–­ç‚¹ç»­ä¼ 
- å¯ä»¥å¹¶è¡Œå¤„ç†

**åˆ†ç‰‡å¤§å°çš„é€‰æ‹©**ï¼š

- æ¯ä¸ªåˆ†ç‰‡ 250M å­—ç¬¦ â‰ˆ 100MB å‹ç¼©æ–‡ä»¶
- ä¸ä¼šå¤ªå¤§ï¼ˆåŠ è½½å¿«ï¼‰
- ä¸ä¼šå¤ªå°ï¼ˆæ–‡ä»¶æ•°é‡åˆç†ï¼‰

### 2. è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†

**é»˜è®¤æ¯”ä¾‹**ï¼š99% è®­ç»ƒï¼Œ1% éªŒè¯

**å®ç°æ–¹å¼**ï¼š

```python
# ä¼ªä»£ç 
total_shards = 240
train_shards = int(total_shards * 0.99)  # 237
val_shards = total_shards - train_shards  # 3

if split == "train":
    shard_indices = range(0, train_shards)
else:  # "val"
    shard_indices = range(train_shards, total_shards)
```

**ä¸ºä»€ä¹ˆéªŒè¯é›†è¿™ä¹ˆå°ï¼Ÿ**

- é¢„è®­ç»ƒæ•°æ®è§„æ¨¡å·¨å¤§ï¼Œ1% å·²ç»è¶³å¤Ÿ
- èŠ‚çœè®¡ç®—èµ„æº
- ä¸»è¦ç”¨äºç›‘æ§è¿‡æ‹Ÿåˆ

### 3. åºåˆ—æ‰“åŒ…ï¼ˆSequence Packingï¼‰

**é—®é¢˜**ï¼šæ–‡æ¡£é•¿åº¦ä¸ä¸€ï¼Œå¦‚ä½•é«˜æ•ˆåˆ©ç”¨ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼šè¿ç»­æ‹¼æ¥

```
æ–‡æ¡£1: "The cat sat on"        (5 tokens)
æ–‡æ¡£2: "Hello world"            (2 tokens)
æ–‡æ¡£3: "Machine learning is"    (3 tokens)

æ‹¼æ¥åï¼ˆå‡è®¾ seq_len=8ï¼‰:
["The", "cat", "sat", "on", "Hello", "world", "Machine", "learning"]
```

**ä¼˜ç‚¹**ï¼š

- âœ… æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡
- âœ… ä¸æµªè´¹è®¡ç®—èµ„æº
- âœ… ç®€å•é«˜æ•ˆ

**æ³¨æ„äº‹é¡¹**ï¼š

- æ–‡æ¡£ä¹‹é—´æ²¡æœ‰ç‰¹æ®Šåˆ†éš”ç¬¦
- æ¨¡å‹ä¼šå­¦ä¹ åˆ°æ–‡æ¡£è¾¹ç•Œï¼ˆéšå¼ï¼‰

### 4. æ•°æ®æ··æ´—ï¼ˆShufflingï¼‰

**ä¸ºä»€ä¹ˆè¦æ··æ´—ï¼Ÿ**

- é¿å…æ¨¡å‹å­¦åˆ°æ•°æ®çš„é¡ºåºåå·®
- æé«˜æ³›åŒ–èƒ½åŠ›

**ä¸¤çº§æ··æ´—**ï¼š

1. **åˆ†ç‰‡çº§åˆ«**ï¼šæ‰“ä¹±åˆ†ç‰‡é¡ºåº
2. **æ–‡æ¡£çº§åˆ«**ï¼šæ‰“ä¹±æ¯ä¸ªåˆ†ç‰‡å†…çš„æ–‡æ¡£é¡ºåº

### 5. åˆ†å¸ƒå¼æ•°æ®åŠ è½½

**å¤š GPU åœºæ™¯**ï¼š

```python
# GPU 0 åŠ è½½åˆ†ç‰‡ 0, 8, 16, 24...
# GPU 1 åŠ è½½åˆ†ç‰‡ 1, 9, 17, 25...
# GPU 2 åŠ è½½åˆ†ç‰‡ 2, 10, 18, 26...
# ...

shard_idx = (global_shard_count + ddp_rank) % total_shards
```

**å¥½å¤„**ï¼š

- æ¯ä¸ª GPU è¯»å–ä¸åŒæ•°æ®
- é¿å…é‡å¤è®¡ç®—
- è‡ªåŠ¨è´Ÿè½½å‡è¡¡

---

## æ•°æ®æ ¼å¼è¯´æ˜

### Parquet æ ¼å¼

**ä»€ä¹ˆæ˜¯ Parquetï¼Ÿ**

- ä¸€ç§åˆ—å¼å­˜å‚¨æ ¼å¼
- é«˜æ•ˆå‹ç¼©ï¼ˆæ¯” JSON å°å¾ˆå¤šï¼‰
- æ”¯æŒå¿«é€ŸæŸ¥è¯¢

**æ–‡ä»¶ç»“æ„**ï¼š

```
parquet æ–‡ä»¶
â”œâ”€â”€ Metadataï¼ˆå…ƒæ•°æ®ï¼‰
â”‚   â”œâ”€â”€ Schemaï¼ˆæ•°æ®ç»“æ„ï¼‰
â”‚   â””â”€â”€ Statisticsï¼ˆç»Ÿè®¡ä¿¡æ¯ï¼‰
â””â”€â”€ Row Groupsï¼ˆæ•°æ®å—ï¼‰
    â”œâ”€â”€ Row Group 0ï¼ˆ1024 è¡Œï¼‰
    â”œâ”€â”€ Row Group 1ï¼ˆ1024 è¡Œï¼‰
    â””â”€â”€ ...
```

**è¯»å–æ–¹å¼**ï¼š

```python
import pyarrow.parquet as pq

# è¯»å–æ•´ä¸ªæ–‡ä»¶
table = pq.read_table("shard_00000.parquet")
texts = table["text"].to_pylist()

# åªè¯»å–éƒ¨åˆ†è¡Œï¼ˆé«˜æ•ˆï¼‰
table = pq.read_table("shard_00000.parquet",
                      columns=["text"],
                      filters=[("row_id", ">=", 0), ("row_id", "<", 1000)])
```

### JSONL æ ¼å¼ï¼ˆå¯¹è¯æ•°æ®ï¼‰

**ä»€ä¹ˆæ˜¯ JSONLï¼Ÿ**

- JSON Linesï¼šæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡
- æ–¹ä¾¿é€è¡Œè¯»å–
- é€‚åˆæµå¼å¤„ç†

**ç¤ºä¾‹æ–‡ä»¶**ï¼š

```jsonl
{"messages": [{"role": "user", "content": "ä½ å¥½"}, {"role": "assistant", "content": "ä½ å¥½ï¼"}]}
{"messages": [{"role": "user", "content": "å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}, {"role": "assistant", "content": "ä»Šå¤©å¤©æ°”ä¸é”™ã€‚"}]}
```

**è¯»å–æ–¹å¼**ï¼š

```python
import json

with open("data.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        data = json.loads(line)
        print(data["messages"])
```

---

## æ•°æ®é‡è®¡ç®—

### å¦‚ä½•ç¡®å®šéœ€è¦å¤šå°‘æ•°æ®ï¼Ÿ

**Chinchilla åŸåˆ™**ï¼šæ•°æ®é‡ = æ¨¡å‹å‚æ•°é‡ Ã— 20

**è®¡ç®—ç¤ºä¾‹**ï¼š

ä»¥é»˜è®¤çš„ d20 æ¨¡å‹ä¸ºä¾‹ï¼š

1. **æ¨¡å‹å‚æ•°é‡**ï¼š561Mï¼ˆ5.61 äº¿ï¼‰
2. **éœ€è¦çš„ token æ•°**ï¼š561M Ã— 20 = 11.2Bï¼ˆ112 äº¿ï¼‰
3. **éœ€è¦çš„å­—ç¬¦æ•°**ï¼š
   - å‡è®¾å‹ç¼©æ¯”ï¼š1 token â‰ˆ 4.8 å­—ç¬¦
   - å­—ç¬¦æ•° = 11.2B Ã— 4.8 â‰ˆ 54Bï¼ˆ540 äº¿ï¼‰
4. **éœ€è¦çš„åˆ†ç‰‡æ•°**ï¼š
   - æ¯ä¸ªåˆ†ç‰‡ 250M å­—ç¬¦
   - åˆ†ç‰‡æ•° = 54B / 250M â‰ˆ 216
   - **å®é™…ä¸‹è½½ 240 ä¸ªåˆ†ç‰‡**ï¼ˆç•™æœ‰ä½™é‡ï¼‰
5. **ç£ç›˜ç©ºé—´**ï¼š
   - æ¯ä¸ªåˆ†ç‰‡çº¦ 100MB
   - æ€»å…± 240 Ã— 100MB = 24GB

### ä¸åŒè§„æ¨¡æ¨¡å‹çš„æ•°æ®éœ€æ±‚

| æ¨¡å‹ | å‚æ•°é‡ | Token æ•° | å­—ç¬¦æ•° | åˆ†ç‰‡æ•° | ç£ç›˜ç©ºé—´ |
| ---- | ------ | -------- | ------ | ------ | -------- |
| d12  | 123M   | 2.5B     | 12B    | 48     | 4.8GB    |
| d20  | 561M   | 11.2B    | 54B    | 240    | 24GB     |
| d26  | 1.2B   | 24B      | 115B   | 450    | 45GB     |
| d32  | 2.1B   | 42B      | 200B   | 800    | 80GB     |

---

## å¸¸è§é—®é¢˜è§£ç­”

### Q1: æ•°æ®ä¸‹è½½å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**A1: ä½¿ç”¨é•œåƒæº**

ä¸­å›½ç”¨æˆ·å¯ä»¥ä½¿ç”¨ HuggingFace é•œåƒï¼š

```bash
# æ–¹æ³•1: è®¾ç½®ç¯å¢ƒå˜é‡
export HF_ENDPOINT=https://hf-mirror.com

# æ–¹æ³•2: ä½¿ç”¨ setup_china.sh è„šæœ¬
bash scripts/setup_china.sh
```

å‚è€ƒé…ç½®ï¼š`configs/chinese_d20.yaml`

```yaml
mirrors:
  huggingface: "https://hf-mirror.com"
  pip: "https://pypi.tuna.tsinghua.edu.cn/simple"
```

### Q2: ç£ç›˜ç©ºé—´ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

**A2: å‡å°‘æ•°æ®é‡**

è®­ç»ƒæ›´å°çš„æ¨¡å‹ï¼š

```bash
# d12 æ¨¡å‹åªéœ€è¦ 48 ä¸ªåˆ†ç‰‡ï¼ˆçº¦ 5GBï¼‰
python -m nanochat.dataset -n 48
python -m scripts.base_train --depth=12
```

**æˆ–è€…**ï¼šä½¿ç”¨æ›´å¤š epochï¼ˆé‡å¤ä½¿ç”¨æ•°æ®ï¼‰

```python
# ä¿®æ”¹ base_train.py
target_param_data_ratio = 10  # ä» 20 é™åˆ° 10
```

**æ³¨æ„**ï¼šæ•°æ®é‡å‡å°‘ä¼šå½±å“æ¨¡å‹æ€§èƒ½ï¼

### Q3: å¦‚ä½•éªŒè¯æ•°æ®ä¸‹è½½æ˜¯å¦å®Œæ•´ï¼Ÿ

**A3: æ£€æŸ¥æ–‡ä»¶**

```bash
# æŸ¥çœ‹å·²ä¸‹è½½çš„åˆ†ç‰‡
ls -lh ~/.cache/nanochat/base_data/

# ç»Ÿè®¡åˆ†ç‰‡æ•°é‡
ls ~/.cache/nanochat/base_data/*.parquet | wc -l

# æ£€æŸ¥æŸä¸ªåˆ†ç‰‡çš„å†…å®¹
python -c "
import pyarrow.parquet as pq
table = pq.read_table('~/.cache/nanochat/base_data/shard_00000.parquet')
print(f'Rows: {len(table)}')
print(f'Columns: {table.column_names}')
print(f'First text: {table[\"text\"][0]}')
"
```

### Q4: å¯ä»¥ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†å—ï¼Ÿ

**A4: å¯ä»¥ï¼**

**æ­¥éª¤**ï¼š

1. **å‡†å¤‡æ•°æ®**ï¼šæ•´ç†æˆæ–‡æœ¬æ–‡ä»¶æˆ– JSONL æ ¼å¼

2. **è½¬æ¢ä¸º Parquet æ ¼å¼**ï¼š

```python
import pyarrow.parquet as pq
import pyarrow as pa

# ä½ çš„æ–‡æœ¬æ•°æ®
texts = ["text1", "text2", "text3", ...]

# è½¬æ¢ä¸º Parquet
table = pa.Table.from_pydict({"text": texts})
pq.write_table(
    table,
    "~/.cache/nanochat/my_data/shard_00000.parquet",
    row_group_size=1024,
    compression="zstd",
    compression_level=3
)
```

3. **ä¿®æ”¹æ•°æ®åŠ è½½è·¯å¾„**ï¼š

```python
# åœ¨ nanochat/dataloader.py ä¸­ä¿®æ”¹ base_dir
base_dir = "~/.cache/nanochat/my_data"
```

### Q5: æ•°æ®æ˜¯å¦éœ€è¦é¢„å¤„ç†ï¼Ÿ

**A5: çœ‹æƒ…å†µ**

**é»˜è®¤æ•°æ®ï¼ˆFineWeb-Eduï¼‰**ï¼š

- âœ… å·²ç»ç»è¿‡é«˜è´¨é‡è¿‡æ»¤
- âœ… å·²ç»å»é‡
- âœ… å¯ä»¥ç›´æ¥ä½¿ç”¨

**è‡ªå®šä¹‰æ•°æ®**ï¼š
å»ºè®®è¿›è¡Œï¼š

1. **å»é‡**ï¼šåˆ é™¤é‡å¤æ–‡æ¡£
2. **è¿‡æ»¤ä½è´¨é‡**ï¼šåˆ é™¤å¤ªçŸ­ã€ä¹±ç ã€é‡å¤å­—ç¬¦è¿‡å¤šçš„æ–‡æœ¬
3. **æ ¼å¼ç»Ÿä¸€**ï¼šç¡®ä¿ç¼–ç ä¸º UTF-8
4. **å»é™¤æ•æ„Ÿä¿¡æ¯**ï¼šéšç§ã€ç‰ˆæƒç­‰

**æ¸…æ´—è„šæœ¬ç¤ºä¾‹**ï¼š

```python
import re

def clean_text(text):
    # å»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)

    # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆå¯é€‰ï¼‰
    text = re.sub(r'[^\w\s\u4e00-\u9fa5ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰]', '', text)

    # å»é™¤å¤ªçŸ­çš„æ–‡æœ¬
    if len(text) < 50:
        return None

    # å»é™¤é‡å¤å­—ç¬¦
    text = re.sub(r'(.)\1{10,}', r'\1\1\1', text)  # è¿ç»­é‡å¤è¶…è¿‡10æ¬¡

    return text

# åº”ç”¨æ¸…æ´—
cleaned_texts = [clean_text(t) for t in texts]
cleaned_texts = [t for t in cleaned_texts if t is not None]
```

### Q6: å¦‚ä½•å¹³è¡¡å¤šä¸ªæ•°æ®é›†ï¼Ÿ

**A6: ä½¿ç”¨æƒé‡æ··åˆ**

åœ¨å¾®è°ƒé˜¶æ®µï¼Œå¯ä»¥æ··åˆå¤šä¸ªæ•°æ®é›†ï¼š

```python
from tasks.common import TaskMixture
from tasks.arc import ARC
from tasks.gsm8k import GSM8K
from tasks.smoltalk import SmolTalk

# æ–¹æ³•1: æŒ‰æ•°é‡æ··åˆï¼ˆé»˜è®¤ï¼‰
train_ds = TaskMixture([
    ARC(split="train"),        # 2.3K
    GSM8K(split="train"),      # 8K
    SmolTalk(split="train"),   # 10K
])  # æ€»å…± 20.3Kï¼Œä¼šå¾ªç¯è¾ƒå°çš„æ•°æ®é›†

# æ–¹æ³•2: æ‰‹åŠ¨æ§åˆ¶æ¯”ä¾‹
train_ds = TaskMixture([
    ARC(split="train", stop=10_000),      # å–10Kï¼ˆä¼šé‡å¤ï¼‰
    GSM8K(split="train", stop=10_000),    # å–10Kï¼ˆä¼šé‡å¤ï¼‰
    SmolTalk(split="train", stop=10_000), # å–10K
])  # æ¯ä¸ªæ•°æ®é›†è´¡çŒ®ç›¸åŒæ•°é‡

# æ–¹æ³•3: åœ¨æ•°æ®ç”Ÿæˆå™¨ä¸­æ§åˆ¶
def weighted_data_generator(datasets, weights):
    """
    datasets: æ•°æ®é›†åˆ—è¡¨
    weights: æƒé‡åˆ—è¡¨ï¼ˆå’Œä¸º1ï¼‰
    """
    import random
    while True:
        # æŒ‰æƒé‡éšæœºé€‰æ‹©æ•°æ®é›†
        dataset = random.choices(datasets, weights=weights)[0]
        # éšæœºé€‰æ‹©è¯¥æ•°æ®é›†ä¸­çš„ä¸€ä¸ªæ ·æœ¬
        idx = random.randint(0, len(dataset) - 1)
        yield dataset[idx]

# ä½¿ç”¨ï¼š70% SmolTalk, 20% GSM8K, 10% ARC
gen = weighted_data_generator(
    [smoltalk_ds, gsm8k_ds, arc_ds],
    [0.7, 0.2, 0.1]
)
```

### Q7: è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥æ›´æ¢æ•°æ®å—ï¼Ÿ

**A7: å¯ä»¥ï¼Œä½†ä¸æ¨è**

**ç†è®ºä¸Šå¯ä»¥**ï¼š

- ä¿®æ”¹æ•°æ®æ–‡ä»¶
- é‡å¯è®­ç»ƒï¼ˆä» checkpoint æ¢å¤ï¼‰
- æ¨¡å‹ä¼šå­¦ä¹ æ–°æ•°æ®

**é£é™©**ï¼š

- æ•°æ®åˆ†å¸ƒå˜åŒ–å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™
- å­¦ä¹ ç‡è°ƒåº¦éœ€è¦é‡æ–°è®¾è®¡
- éš¾ä»¥å¤ç°ç»“æœ

**æœ€ä½³å®è·µ**ï¼š

- åœ¨è®­ç»ƒå¼€å§‹å‰å‡†å¤‡å¥½æ‰€æœ‰æ•°æ®
- ä½¿ç”¨æ•°æ®æ··åˆç­–ç•¥ä»£æ›¿ä¸­é€”æ›´æ¢
- å¦‚æœå¿…é¡»æ›´æ¢ï¼Œå»ºè®®é™ä½å­¦ä¹ ç‡

### Q8: å¦‚ä½•ç›‘æ§æ•°æ®åŠ è½½æ€§èƒ½ï¼Ÿ

**A8: ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·**

```python
import time

# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ è®¡æ—¶
t0 = time.time()
x, y = next(train_loader)
t1 = time.time()
data_loading_time = t1 - t0

# è®¡ç®—æ•°æ®åŠ è½½å æ¯”
if data_loading_time > step_time * 0.1:
    print(f"âš ï¸  æ•°æ®åŠ è½½æ…¢ï¼š{data_loading_time:.2f}s / {step_time:.2f}s")
```

**ä¼˜åŒ–å»ºè®®**ï¼š

1. å¢åŠ  `num_workers`ï¼ˆå¤šè¿›ç¨‹åŠ è½½ï¼‰
2. ä½¿ç”¨æ›´å¿«çš„ç£ç›˜ï¼ˆSSD > HDDï¼‰
3. å¢åŠ ç£ç›˜ç¼“å­˜
4. ä½¿ç”¨é¢„å…ˆåˆ†è¯çš„æ•°æ®ï¼ˆæƒè¡¡å–èˆï¼‰

---

## æ•°æ®ç›®å½•ç»“æ„

è®­ç»ƒåï¼Œæ•°æ®ä¼šä¿å­˜åœ¨ä»¥ä¸‹ä½ç½®ï¼š

```
~/.cache/nanochat/
â”œâ”€â”€ base_data/                    # é¢„è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ shard_00000.parquet
â”‚   â”œâ”€â”€ shard_00001.parquet
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tokenizer/                    # åˆ†è¯å™¨æ–‡ä»¶
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ merges.txt
â”œâ”€â”€ tokenized_data/               # åˆ†è¯åçš„æ•°æ®ï¼ˆå¯é€‰ï¼‰
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ tokens_00000.bin
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ val/
â”‚       â””â”€â”€ tokens_00000.bin
â”œâ”€â”€ base_checkpoints/             # é¢„è®­ç»ƒæ£€æŸ¥ç‚¹
â”‚   â””â”€â”€ d20/
â”‚       â”œâ”€â”€ model.pt
â”‚       â”œâ”€â”€ optimizer.pt
â”‚       â””â”€â”€ meta.json
â”œâ”€â”€ mid_checkpoints/              # ä¸­æœŸè®­ç»ƒæ£€æŸ¥ç‚¹
â”‚   â””â”€â”€ d20/
â”œâ”€â”€ sft_checkpoints/              # å¾®è°ƒæ£€æŸ¥ç‚¹
â”‚   â””â”€â”€ d20/
â””â”€â”€ eval_bundle/                  # è¯„ä¼°æ•°æ®
    â”œâ”€â”€ arc/
    â”œâ”€â”€ gsm8k/
    â””â”€â”€ ...
```

**ç£ç›˜ç©ºé—´è§„åˆ’**ï¼š

- é¢„è®­ç»ƒæ•°æ®ï¼š24GBï¼ˆd20ï¼‰
- åˆ†è¯å™¨ï¼š< 100MB
- æ£€æŸ¥ç‚¹ï¼šæ¯ä¸ªçº¦ 2-3GBï¼ˆå–å†³äºæ¨¡å‹å¤§å°ï¼‰
- è¯„ä¼°æ•°æ®ï¼šçº¦ 200MB
- **æ€»è®¡**ï¼šçº¦ 30-35GBï¼ˆd20 æ¨¡å‹ï¼‰

---

## æœ€ä½³å®è·µæ€»ç»“

### âœ… æ¨èåšæ³•

1. **ä½¿ç”¨å®˜æ–¹æ•°æ®é›†å¼€å§‹**

   - å…ˆè·‘é€š `speedrun.sh`
   - ç†è§£å®Œæ•´æµç¨‹
   - å†å°è¯•è‡ªå®šä¹‰æ•°æ®

2. **é€æ­¥æ‰©å¤§è§„æ¨¡**

   - ä»å°æ¨¡å‹å¼€å§‹ï¼ˆd12ï¼‰
   - éªŒè¯æµç¨‹æ­£ç¡®
   - å†è®­ç»ƒå¤§æ¨¡å‹

3. **ç›‘æ§æ•°æ®è´¨é‡**

   - å®šæœŸæ£€æŸ¥é‡‡æ ·è¾“å‡º
   - è§‚å¯Ÿè®­ç»ƒ/éªŒè¯ loss æ›²çº¿
   - è¯„ä¼° perplexity

4. **å……åˆ†åˆ©ç”¨ç¼“å­˜**

   - æ•°æ®ä¸‹è½½ä¸€æ¬¡ï¼Œå¤šæ¬¡ä½¿ç”¨
   - åˆ†è¯å™¨è®­ç»ƒä¸€æ¬¡ï¼Œå¤ç”¨
   - æ£€æŸ¥ç‚¹å®šæœŸä¿å­˜

5. **æ–‡æ¡£è®°å½•**
   - è®°å½•æ•°æ®æ¥æºå’Œå¤„ç†æ­¥éª¤
   - ä¿å­˜è¶…å‚æ•°é…ç½®
   - ä¾¿äºå¤ç°å’Œè°ƒè¯•

### âŒ é¿å…åšæ³•

1. **ä¸è¦è·³è¿‡æ•°æ®éªŒè¯**

   - ä¸‹è½½åæ£€æŸ¥å®Œæ•´æ€§
   - ç¡®è®¤æ ¼å¼æ­£ç¡®
   - æµ‹è¯•æ•°æ®åŠ è½½

2. **ä¸è¦è¿‡åº¦æ¸…æ´—**

   - ä¿ç•™è‡ªç„¶è¯­è¨€çš„å¤šæ ·æ€§
   - ä¸è¦è¿‡åº¦æ ‡å‡†åŒ–
   - é¿å…å¼•å…¥åè§

3. **ä¸è¦å¿½è§†ç‰ˆæƒ**

   - ä½¿ç”¨å¼€æºæ•°æ®é›†
   - æ³¨æ„è®¸å¯åè®®
   - é¿å…ä¾µæƒé£é™©

4. **ä¸è¦ç›²ç›®å¢åŠ æ•°æ®**
   - è´¨é‡ > æ•°é‡
   - éµå¾ª Chinchilla åŸåˆ™
   - å¹³è¡¡æ•°æ®å’Œè®¡ç®—é¢„ç®—

---

## è¿›é˜¶è¯é¢˜

### æ•°æ®å¢å¼º

å¯¹äºå°æ•°æ®é›†ï¼Œå¯ä»¥è€ƒè™‘ï¼š

- **å›è¯‘**ï¼ˆBack Translationï¼‰ï¼šä¸­æ–‡ â†’ è‹±æ–‡ â†’ ä¸­æ–‡
- **åŒä¹‰æ›¿æ¢**ï¼šä½¿ç”¨åŒä¹‰è¯æ›¿æ¢éƒ¨åˆ†è¯æ±‡
- **éšæœºåˆ é™¤**ï¼šéšæœºåˆ é™¤å°‘é‡ token

### è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰

é€æ­¥å¢åŠ æ•°æ®éš¾åº¦ï¼š

1. ä»ç®€å•ã€çŸ­æ–‡æœ¬å¼€å§‹
2. é€æ­¥å¢åŠ æ–‡æœ¬é•¿åº¦
3. æœ€ååŠ å…¥å¤æ‚ä»»åŠ¡

### å¤šæ¨¡æ€æ•°æ®

æœªæ¥å¯ä»¥åŠ å…¥ï¼š

- å›¾åƒ-æ–‡æœ¬å¯¹
- ä»£ç -æ³¨é‡Šå¯¹
- è¡¨æ ¼æ•°æ®

---

## æ€»ç»“

nanochat çš„æ•°æ®å¤„ç†æµç¨‹è®¾è®¡ç²¾å·§ï¼š

1. **æµå¼åŠ è½½** - èŠ‚çœå†…å­˜å’Œç£ç›˜
2. **å³æ—¶åˆ†è¯** - çµæ´»æ€§é«˜
3. **åˆ†å¸ƒå¼æ”¯æŒ** - é«˜æ•ˆåˆ©ç”¨å¤š GPU
4. **æ ‡å‡†æ ¼å¼** - æ˜“äºæ‰©å±•å’Œå®šåˆ¶

ç†è§£æ•°æ®å¤„ç†æ˜¯è®­ç»ƒå¥½æ¨¡å‹çš„å…³é”®ç¬¬ä¸€æ­¥ï¼

---

## å‚è€ƒèµ„æ–™

- [FineWeb-Edu æ•°æ®é›†](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)
- [SmolTalk æ•°æ®é›†](https://huggingface.co/datasets/HuggingFaceTB/smoltalk)
- [Chinchilla è®ºæ–‡](https://arxiv.org/abs/2203.15556)
- [Parquet æ–‡æ¡£](https://parquet.apache.org/docs/)
- [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)

---

**æœ‰é—®é¢˜ï¼Ÿ**

- æŸ¥çœ‹ä»£ç ï¼š`dev/repackage_data_reference.py`
- è¿è¡Œç¤ºä¾‹ï¼š`bash speedrun.sh`
- æäº¤ Issueï¼š[GitHub Issues](https://github.com/karpathy/nanochat/issues)

**å¿«ä¹è®­ç»ƒï¼** ğŸš€
