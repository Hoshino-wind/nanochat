# nanochat 数据处理完全指南

> 本文档详细介绍 nanochat 项目的数据处理流程，适合初学者阅读。

## 📚 目录

1. [数据处理概览](#数据处理概览)
2. [数据类型与来源](#数据类型与来源)
3. [数据处理流程](#数据处理流程)
4. [详细操作步骤](#详细操作步骤)
5. [常见问题解答](#常见问题解答)

---

## 数据处理概览

nanochat 项目训练一个完整的 ChatGPT 风格的语言模型，需要经历**三个主要阶段**，每个阶段使用不同的数据：

```
┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐
│   预训练阶段     │  →   │   中期训练阶段   │  →   │   微调阶段       │
│  (Pretraining)  │      │  (Midtraining)  │      │  (Fine-tuning)  │
└─────────────────┘      └─────────────────┘      └─────────────────┘
    大规模文本数据           对话格式数据            指令对话数据
    学习基础语言能力         学习对话结构            学习遵循指令
```

### 为什么需要三个阶段？

- **预训练**：让模型学习语言的基本规律（语法、常识、推理等）
- **中期训练**：让模型学会对话的格式和结构
- **微调**：让模型学会更好地理解和执行用户指令

---

## 数据类型与来源

### 1. 预训练数据（Base Training Data）

**数据来源**：FineWeb-Edu 数据集

- **数据集**：`HuggingFaceFW/fineweb-edu`（sample-100BT）
- **数据规模**：约 100B（1000 亿）个 GPT-2 tokens
- **数据内容**：高质量的网页文本数据
- **数据格式**：Parquet 文件（压缩后的表格格式）

**数据特点**：

- 每个数据分片（shard）约 250M 字符
- 压缩后每个文件约 100MB
- 使用 zstd 压缩算法
- 总共约 1822 个分片

**下载方式**：

```bash
# 下载 8 个分片用于训练分词器（约 800MB）
python -m nanochat.dataset -n 8

# 下载 240 个分片用于预训练（约 24GB）
python -m nanochat.dataset -n 240
```

### 2. 中期训练数据（Midtraining Data）

**数据来源**：SmolTalk 对话数据集

- **数据集**：`HuggingFaceTB/smoltalk`
- **数据内容**：真实的多轮对话数据
- **数据格式**：对话消息列表

**数据结构示例**：

```json
{
  "messages": [
    { "role": "user", "content": "你好，请介绍一下自己" },
    { "role": "assistant", "content": "你好！我是一个AI助手..." }
  ]
}
```

### 3. 微调数据（SFT Data）

**数据来源**：多个任务数据集的混合

- **ARC-Easy**：简单的选择题（2.3K 条）
- **ARC-Challenge**：困难的选择题（1.1K 条）
- **GSM8K**：数学推理题（8K 条）
- **SmolTalk**：对话数据（10K 条）

**总计**：约 21.4K 条训练样本

---

## 数据处理流程

### 流程图

```
原始数据下载
    ↓
数据重新打包（repackage）
    ↓
构建分词器（tokenizer）
    ↓
数据分词化（tokenization）
    ↓
创建训练/验证集
    ↓
批次加载（batching）
    ↓
输入模型训练
```

### 详细说明

#### 步骤 1: 数据下载与重新打包

**目的**：将 HuggingFace 数据集转换为适合流式加载的格式

**过程**（参考 `dev/repackage_data_reference.py`）：

1. **加载原始数据集**

   ```python
   from datasets import load_dataset
   ds = load_dataset("HuggingFaceFW/fineweb-edu", name="sample-100BT", split="train")
   ```

2. **数据混洗**

   ```python
   ds = ds.shuffle(seed=42)  # 打乱数据顺序，避免数据分布偏差
   ```

3. **重新打包成分片**

   - 每个分片包含 250M 字符
   - 使用 Parquet 格式存储
   - Row group size = 1024（便于分布式读取）
   - 使用 zstd 压缩（level 3）

4. **上传到 HuggingFace**
   - 数据集 ID：`karpathy/fineweb-edu-100b-shuffle`
   - 这样可以快速下载，无需自己重新打包

#### 步骤 2: 训练分词器

**目的**：将文本转换为模型可以处理的数字序列

**过程**（参考 `scripts/tok_train.py`）：

1. **准备训练数据**

   ```python
   # 使用前 2B（20亿）字符训练分词器
   python -m scripts.tok_train --max_chars=2000000000
   ```

2. **训练 BPE 分词器**

   - 使用 Rust 实现的高效 BPE（Byte Pair Encoding）
   - 词汇表大小：65,536（2^16）
   - 训练过程：学习最常见的字符组合

3. **保存分词器**
   - 保存位置：`~/.cache/nanochat/tokenizer/`
   - 包含词汇表和合并规则

**什么是分词器？**

分词器就像一个"翻译官"，把人类的文字转换成模型能理解的数字：

```
文本: "Hello world"
   ↓ (分词)
Token IDs: [15496, 995]
   ↓ (训练)
模型处理数字
   ↓ (解码)
文本: "Hello world"
```

#### 步骤 3: 数据分词化与加载

**目的**：将文本转换为 token 序列，并按需加载

**特点**：

- **即时分词**（On-the-fly tokenization）
- **流式加载**：不需要将所有数据预先分词
- **分布式支持**：多 GPU 自动划分数据

**数据加载器工作原理**：

```python
# 伪代码示例
def tokenizing_distributed_data_loader(batch_size, seq_len, split):
    """
    batch_size: 每个GPU的批次大小
    seq_len: 序列长度（上下文窗口）
    split: "train" 或 "val"
    """
    while True:
        # 1. 从磁盘读取一个 parquet 分片
        shard = load_next_shard()

        # 2. 遍历分片中的文档
        for doc in shard:
            text = doc['text']

            # 3. 使用分词器转换文本
            tokens = tokenizer.encode(text)

            # 4. 切分成固定长度的序列
            for i in range(0, len(tokens) - seq_len, seq_len):
                inputs = tokens[i:i+seq_len]
                targets = tokens[i+1:i+seq_len+1]  # 目标是输入向右偏移1位

                # 5. 组装成批次
                batch_inputs.append(inputs)
                batch_targets.append(targets)

                if len(batch_inputs) == batch_size:
                    yield (batch_inputs, batch_targets)
                    batch_inputs = []
                    batch_targets = []
```

**为什么使用这种方式？**

- ✅ 节省磁盘空间（不需要预先分词并保存）
- ✅ 节省内存（流式加载，只保留当前需要的数据）
- ✅ 灵活性高（可以随时更换分词器）

---

## 详细操作步骤

### 场景 1：运行默认的预训练（英文）

**前提条件**：

- 8 张 H100 或 A100 GPU
- 足够的磁盘空间（约 30GB）

**步骤**：

```bash
# 1. 下载初始数据（用于训练分词器）
python -m nanochat.dataset -n 8

# 2. 后台下载完整训练数据
python -m nanochat.dataset -n 240 &

# 3. 训练分词器
python -m scripts.tok_train --max_chars=2000000000

# 4. 评估分词器
python -m scripts.tok_eval

# 5. 开始预训练
torchrun --standalone --nproc_per_node=8 -m scripts.base_train --depth=20

# 6. 评估模型
torchrun --standalone --nproc_per_node=8 -m scripts.base_eval
```

**或者直接运行一键脚本**：

```bash
bash speedrun.sh
```

### 场景 2：训练中文模型

**步骤 1：准备中文数据**

创建数据下载脚本：

```python
# download_chinese_data.py
from datasets import load_dataset, concatenate_datasets
import os

print("📥 下载中文数据集...")

# 1. 中文维基百科
wiki = load_dataset("wikipedia", "20220301.zh", split="train[:100000]")
print(f"✅ 维基百科: {len(wiki)} 条")

# 2. 百度百科
baike = load_dataset("xusenlin/baidubaike-563w", split="train[:200000]")
print(f"✅ 百度百科: {len(baike)} 条")

# 3. 新闻数据
news = load_dataset("THUDM/LongBench", "news", split="train[:50000]")
print(f"✅ 新闻数据: {len(news)} 条")

# 合并数据集
combined = concatenate_datasets([wiki, baike, news])

# 数据清洗
import re

def clean_text(example):
    text = example.get('text', example.get('content', ''))
    # 去除特殊字符，只保留中文、英文、数字和标点
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9，。！？、；：""''（）【】《》\s]', '', text)
    if len(text) < 50:  # 过滤太短的文本
        return None
    return {'text': text}

combined = combined.map(clean_text)
combined = combined.filter(lambda x: x is not None)

print(f"\n✅ 清洗后数据量: {len(combined)} 条")

# 保存为 parquet 格式
output_dir = os.path.expanduser("~/.cache/nanochat/chinese_data")
os.makedirs(output_dir, exist_ok=True)

# 分割成多个分片（每个约 10 万条）
shard_size = 100000
num_shards = (len(combined) + shard_size - 1) // shard_size

for i in range(num_shards):
    start = i * shard_size
    end = min(start + shard_size, len(combined))
    shard = combined.select(range(start, end))

    shard_path = f"{output_dir}/shard_{i:05d}.parquet"
    shard.to_parquet(shard_path, compression='zstd', compression_level=3)

    print(f"💾 已保存 {shard_path}: {len(shard)} 条")

print("\n✅ 中文数据准备完成！")
```

运行：

```bash
python download_chinese_data.py
```

**步骤 2：训练中文分词器**

中文分词器需要更大的词汇表（因为中文字符更多）：

```bash
# 修改 tok_train.py 中的 vocab_size 参数
python -m scripts.tok_train --max_chars=2000000000 --vocab_size=80000
```

**步骤 3：修改训练配置**

使用 `configs/chinese_d20.yaml` 配置文件，或手动指定参数：

```bash
torchrun --standalone --nproc_per_node=8 -m scripts.base_train \
    --depth=20 \
    --vocab_size=80000 \
    --device_batch_size=16  # 中文模型可能需要调整批次大小
```

**完整流程可参考**：`scripts/train_chinese.sh`

### 场景 3：准备自定义对话数据（微调）

**数据格式要求**：

每条数据是一个包含多轮对话的 JSON 对象：

```json
{
  "messages": [
    {
      "role": "user",
      "content": "如何做番茄炒蛋？"
    },
    {
      "role": "assistant",
      "content": "做番茄炒蛋的步骤如下：\n1. 准备材料：鸡蛋3个，番茄2个...\n2. 将鸡蛋打散..."
    },
    {
      "role": "user",
      "content": "需要放糖吗？"
    },
    {
      "role": "assistant",
      "content": "可以放一点点糖，能让番茄的酸味更柔和..."
    }
  ]
}
```

**准备数据步骤**：

1. **下载或准备数据集**

例如，使用 Belle 中文对话数据集：

```python
from datasets import load_dataset

# 下载 Belle 数据集
belle = load_dataset("BelleGroup/train_1M_CN", split="train[:100000]")

# 转换为对话格式
def convert_format(example):
    instruction = example['instruction']
    input_text = example.get('input', '')
    output = example['output']

    if input_text:
        user_content = f"{instruction}\n\n{input_text}"
    else:
        user_content = instruction

    return {
        "messages": [
            {"role": "user", "content": user_content},
            {"role": "assistant", "content": output}
        ]
    }

converted = belle.map(convert_format)

# 保存为 JSONL 格式
output_dir = os.path.expanduser("~/.cache/nanochat/chinese_sft")
os.makedirs(output_dir, exist_ok=True)
converted.to_json(f"{output_dir}/belle_100k.jsonl")
```

2. **使用自定义任务类加载数据**

参考 `tasks/smoltalk.py` 创建自定义数据加载器：

```python
# tasks/my_chinese_chat.py
from tasks.common import Task

class MyChineseChat(Task):
    def __init__(self, split, stop=None):
        from datasets import load_dataset
        # 从本地 JSONL 文件加载
        data_path = "~/.cache/nanochat/chinese_sft/belle_100k.jsonl"
        ds = load_dataset("json", data_files=data_path, split="train")

        # 分割训练/验证集
        ds = ds.train_test_split(test_size=0.1, seed=42)
        self.data = ds[split]

        if stop is not None:
            self.data = self.data.select(range(min(stop, len(self.data))))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

3. **在训练脚本中使用**

修改 `scripts/chat_sft.py`：

```python
from tasks.my_chinese_chat import MyChineseChat

train_ds = TaskMixture([
    MyChineseChat(split="train", stop=10_000),
    # 可以混合其他任务...
])
```

---

## 数据处理技术细节

### 1. 数据分片策略

**为什么要分片？**

- 便于分布式下载和加载
- 支持断点续传
- 可以并行处理

**分片大小的选择**：

- 每个分片 250M 字符 ≈ 100MB 压缩文件
- 不会太大（加载快）
- 不会太小（文件数量合理）

### 2. 训练/验证集划分

**默认比例**：99% 训练，1% 验证

**实现方式**：

```python
# 伪代码
total_shards = 240
train_shards = int(total_shards * 0.99)  # 237
val_shards = total_shards - train_shards  # 3

if split == "train":
    shard_indices = range(0, train_shards)
else:  # "val"
    shard_indices = range(train_shards, total_shards)
```

**为什么验证集这么小？**

- 预训练数据规模巨大，1% 已经足够
- 节省计算资源
- 主要用于监控过拟合

### 3. 序列打包（Sequence Packing）

**问题**：文档长度不一，如何高效利用？

**解决方案**：连续拼接

```
文档1: "The cat sat on"        (5 tokens)
文档2: "Hello world"            (2 tokens)
文档3: "Machine learning is"    (3 tokens)

拼接后（假设 seq_len=8）:
["The", "cat", "sat", "on", "Hello", "world", "Machine", "learning"]
```

**优点**：

- ✅ 最大化 GPU 利用率
- ✅ 不浪费计算资源
- ✅ 简单高效

**注意事项**：

- 文档之间没有特殊分隔符
- 模型会学习到文档边界（隐式）

### 4. 数据混洗（Shuffling）

**为什么要混洗？**

- 避免模型学到数据的顺序偏差
- 提高泛化能力

**两级混洗**：

1. **分片级别**：打乱分片顺序
2. **文档级别**：打乱每个分片内的文档顺序

### 5. 分布式数据加载

**多 GPU 场景**：

```python
# GPU 0 加载分片 0, 8, 16, 24...
# GPU 1 加载分片 1, 9, 17, 25...
# GPU 2 加载分片 2, 10, 18, 26...
# ...

shard_idx = (global_shard_count + ddp_rank) % total_shards
```

**好处**：

- 每个 GPU 读取不同数据
- 避免重复计算
- 自动负载均衡

---

## 数据格式说明

### Parquet 格式

**什么是 Parquet？**

- 一种列式存储格式
- 高效压缩（比 JSON 小很多）
- 支持快速查询

**文件结构**：

```
parquet 文件
├── Metadata（元数据）
│   ├── Schema（数据结构）
│   └── Statistics（统计信息）
└── Row Groups（数据块）
    ├── Row Group 0（1024 行）
    ├── Row Group 1（1024 行）
    └── ...
```

**读取方式**：

```python
import pyarrow.parquet as pq

# 读取整个文件
table = pq.read_table("shard_00000.parquet")
texts = table["text"].to_pylist()

# 只读取部分行（高效）
table = pq.read_table("shard_00000.parquet",
                      columns=["text"],
                      filters=[("row_id", ">=", 0), ("row_id", "<", 1000)])
```

### JSONL 格式（对话数据）

**什么是 JSONL？**

- JSON Lines：每行一个 JSON 对象
- 方便逐行读取
- 适合流式处理

**示例文件**：

```jsonl
{"messages": [{"role": "user", "content": "你好"}, {"role": "assistant", "content": "你好！"}]}
{"messages": [{"role": "user", "content": "天气怎么样？"}, {"role": "assistant", "content": "今天天气不错。"}]}
```

**读取方式**：

```python
import json

with open("data.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        data = json.loads(line)
        print(data["messages"])
```

---

## 数据量计算

### 如何确定需要多少数据？

**Chinchilla 原则**：数据量 = 模型参数量 × 20

**计算示例**：

以默认的 d20 模型为例：

1. **模型参数量**：561M（5.61 亿）
2. **需要的 token 数**：561M × 20 = 11.2B（112 亿）
3. **需要的字符数**：
   - 假设压缩比：1 token ≈ 4.8 字符
   - 字符数 = 11.2B × 4.8 ≈ 54B（540 亿）
4. **需要的分片数**：
   - 每个分片 250M 字符
   - 分片数 = 54B / 250M ≈ 216
   - **实际下载 240 个分片**（留有余量）
5. **磁盘空间**：
   - 每个分片约 100MB
   - 总共 240 × 100MB = 24GB

### 不同规模模型的数据需求

| 模型 | 参数量 | Token 数 | 字符数 | 分片数 | 磁盘空间 |
| ---- | ------ | -------- | ------ | ------ | -------- |
| d12  | 123M   | 2.5B     | 12B    | 48     | 4.8GB    |
| d20  | 561M   | 11.2B    | 54B    | 240    | 24GB     |
| d26  | 1.2B   | 24B      | 115B   | 450    | 45GB     |
| d32  | 2.1B   | 42B      | 200B   | 800    | 80GB     |

---

## 常见问题解答

### Q1: 数据下载很慢怎么办？

**A1: 使用镜像源**

中国用户可以使用 HuggingFace 镜像：

```bash
# 方法1: 设置环境变量
export HF_ENDPOINT=https://hf-mirror.com

# 方法2: 使用 setup_china.sh 脚本
bash scripts/setup_china.sh
```

参考配置：`configs/chinese_d20.yaml`

```yaml
mirrors:
  huggingface: "https://hf-mirror.com"
  pip: "https://pypi.tuna.tsinghua.edu.cn/simple"
```

### Q2: 磁盘空间不够怎么办？

**A2: 减少数据量**

训练更小的模型：

```bash
# d12 模型只需要 48 个分片（约 5GB）
python -m nanochat.dataset -n 48
python -m scripts.base_train --depth=12
```

**或者**：使用更多 epoch（重复使用数据）

```python
# 修改 base_train.py
target_param_data_ratio = 10  # 从 20 降到 10
```

**注意**：数据量减少会影响模型性能！

### Q3: 如何验证数据下载是否完整？

**A3: 检查文件**

```bash
# 查看已下载的分片
ls -lh ~/.cache/nanochat/base_data/

# 统计分片数量
ls ~/.cache/nanochat/base_data/*.parquet | wc -l

# 检查某个分片的内容
python -c "
import pyarrow.parquet as pq
table = pq.read_table('~/.cache/nanochat/base_data/shard_00000.parquet')
print(f'Rows: {len(table)}')
print(f'Columns: {table.column_names}')
print(f'First text: {table[\"text\"][0]}')
"
```

### Q4: 可以使用自己的数据集吗？

**A4: 可以！**

**步骤**：

1. **准备数据**：整理成文本文件或 JSONL 格式

2. **转换为 Parquet 格式**：

```python
import pyarrow.parquet as pq
import pyarrow as pa

# 你的文本数据
texts = ["text1", "text2", "text3", ...]

# 转换为 Parquet
table = pa.Table.from_pydict({"text": texts})
pq.write_table(
    table,
    "~/.cache/nanochat/my_data/shard_00000.parquet",
    row_group_size=1024,
    compression="zstd",
    compression_level=3
)
```

3. **修改数据加载路径**：

```python
# 在 nanochat/dataloader.py 中修改 base_dir
base_dir = "~/.cache/nanochat/my_data"
```

### Q5: 数据是否需要预处理？

**A5: 看情况**

**默认数据（FineWeb-Edu）**：

- ✅ 已经经过高质量过滤
- ✅ 已经去重
- ✅ 可以直接使用

**自定义数据**：
建议进行：

1. **去重**：删除重复文档
2. **过滤低质量**：删除太短、乱码、重复字符过多的文本
3. **格式统一**：确保编码为 UTF-8
4. **去除敏感信息**：隐私、版权等

**清洗脚本示例**：

```python
import re

def clean_text(text):
    # 去除多余空白
    text = re.sub(r'\s+', ' ', text)

    # 去除特殊字符（可选）
    text = re.sub(r'[^\w\s\u4e00-\u9fa5，。！？、；：""''（）]', '', text)

    # 去除太短的文本
    if len(text) < 50:
        return None

    # 去除重复字符
    text = re.sub(r'(.)\1{10,}', r'\1\1\1', text)  # 连续重复超过10次

    return text

# 应用清洗
cleaned_texts = [clean_text(t) for t in texts]
cleaned_texts = [t for t in cleaned_texts if t is not None]
```

### Q6: 如何平衡多个数据集？

**A6: 使用权重混合**

在微调阶段，可以混合多个数据集：

```python
from tasks.common import TaskMixture
from tasks.arc import ARC
from tasks.gsm8k import GSM8K
from tasks.smoltalk import SmolTalk

# 方法1: 按数量混合（默认）
train_ds = TaskMixture([
    ARC(split="train"),        # 2.3K
    GSM8K(split="train"),      # 8K
    SmolTalk(split="train"),   # 10K
])  # 总共 20.3K，会循环较小的数据集

# 方法2: 手动控制比例
train_ds = TaskMixture([
    ARC(split="train", stop=10_000),      # 取10K（会重复）
    GSM8K(split="train", stop=10_000),    # 取10K（会重复）
    SmolTalk(split="train", stop=10_000), # 取10K
])  # 每个数据集贡献相同数量

# 方法3: 在数据生成器中控制
def weighted_data_generator(datasets, weights):
    """
    datasets: 数据集列表
    weights: 权重列表（和为1）
    """
    import random
    while True:
        # 按权重随机选择数据集
        dataset = random.choices(datasets, weights=weights)[0]
        # 随机选择该数据集中的一个样本
        idx = random.randint(0, len(dataset) - 1)
        yield dataset[idx]

# 使用：70% SmolTalk, 20% GSM8K, 10% ARC
gen = weighted_data_generator(
    [smoltalk_ds, gsm8k_ds, arc_ds],
    [0.7, 0.2, 0.1]
)
```

### Q7: 训练过程中可以更换数据吗？

**A7: 可以，但不推荐**

**理论上可以**：

- 修改数据文件
- 重启训练（从 checkpoint 恢复）
- 模型会学习新数据

**风险**：

- 数据分布变化可能导致性能下降
- 学习率调度需要重新设计
- 难以复现结果

**最佳实践**：

- 在训练开始前准备好所有数据
- 使用数据混合策略代替中途更换
- 如果必须更换，建议降低学习率

### Q8: 如何监控数据加载性能？

**A8: 使用性能分析工具**

```python
import time

# 在训练脚本中添加计时
t0 = time.time()
x, y = next(train_loader)
t1 = time.time()
data_loading_time = t1 - t0

# 计算数据加载占比
if data_loading_time > step_time * 0.1:
    print(f"⚠️  数据加载慢：{data_loading_time:.2f}s / {step_time:.2f}s")
```

**优化建议**：

1. 增加 `num_workers`（多进程加载）
2. 使用更快的磁盘（SSD > HDD）
3. 增加磁盘缓存
4. 使用预先分词的数据（权衡取舍）

---

## 数据目录结构

训练后，数据会保存在以下位置：

```
~/.cache/nanochat/
├── base_data/                    # 预训练数据
│   ├── shard_00000.parquet
│   ├── shard_00001.parquet
│   └── ...
├── tokenizer/                    # 分词器文件
│   ├── tokenizer.json
│   └── merges.txt
├── tokenized_data/               # 分词后的数据（可选）
│   ├── train/
│   │   ├── tokens_00000.bin
│   │   └── ...
│   └── val/
│       └── tokens_00000.bin
├── base_checkpoints/             # 预训练检查点
│   └── d20/
│       ├── model.pt
│       ├── optimizer.pt
│       └── meta.json
├── mid_checkpoints/              # 中期训练检查点
│   └── d20/
├── sft_checkpoints/              # 微调检查点
│   └── d20/
└── eval_bundle/                  # 评估数据
    ├── arc/
    ├── gsm8k/
    └── ...
```

**磁盘空间规划**：

- 预训练数据：24GB（d20）
- 分词器：< 100MB
- 检查点：每个约 2-3GB（取决于模型大小）
- 评估数据：约 200MB
- **总计**：约 30-35GB（d20 模型）

---

## 最佳实践总结

### ✅ 推荐做法

1. **使用官方数据集开始**

   - 先跑通 `speedrun.sh`
   - 理解完整流程
   - 再尝试自定义数据

2. **逐步扩大规模**

   - 从小模型开始（d12）
   - 验证流程正确
   - 再训练大模型

3. **监控数据质量**

   - 定期检查采样输出
   - 观察训练/验证 loss 曲线
   - 评估 perplexity

4. **充分利用缓存**

   - 数据下载一次，多次使用
   - 分词器训练一次，复用
   - 检查点定期保存

5. **文档记录**
   - 记录数据来源和处理步骤
   - 保存超参数配置
   - 便于复现和调试

### ❌ 避免做法

1. **不要跳过数据验证**

   - 下载后检查完整性
   - 确认格式正确
   - 测试数据加载

2. **不要过度清洗**

   - 保留自然语言的多样性
   - 不要过度标准化
   - 避免引入偏见

3. **不要忽视版权**

   - 使用开源数据集
   - 注意许可协议
   - 避免侵权风险

4. **不要盲目增加数据**
   - 质量 > 数量
   - 遵循 Chinchilla 原则
   - 平衡数据和计算预算

---

## 进阶话题

### 数据增强

对于小数据集，可以考虑：

- **回译**（Back Translation）：中文 → 英文 → 中文
- **同义替换**：使用同义词替换部分词汇
- **随机删除**：随机删除少量 token

### 课程学习（Curriculum Learning）

逐步增加数据难度：

1. 从简单、短文本开始
2. 逐步增加文本长度
3. 最后加入复杂任务

### 多模态数据

未来可以加入：

- 图像-文本对
- 代码-注释对
- 表格数据

---

## 总结

nanochat 的数据处理流程设计精巧：

1. **流式加载** - 节省内存和磁盘
2. **即时分词** - 灵活性高
3. **分布式支持** - 高效利用多 GPU
4. **标准格式** - 易于扩展和定制

理解数据处理是训练好模型的关键第一步！

---

## 参考资料

- [FineWeb-Edu 数据集](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)
- [SmolTalk 数据集](https://huggingface.co/datasets/HuggingFaceTB/smoltalk)
- [Chinchilla 论文](https://arxiv.org/abs/2203.15556)
- [Parquet 文档](https://parquet.apache.org/docs/)
- [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)

---

**有问题？**

- 查看代码：`dev/repackage_data_reference.py`
- 运行示例：`bash speedrun.sh`
- 提交 Issue：[GitHub Issues](https://github.com/karpathy/nanochat/issues)

**快乐训练！** 🚀
