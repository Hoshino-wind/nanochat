# 📦 数据处理指南

> **写给小白的话**：这篇文档会手把手教你如何准备训练数据,不需要任何专业背景,跟着做就行!

## 🎯 核心概念:3 分钟快速理解

### 训练 AI 需要什么数据?

想象一下教小孩学说话的过程:

```
👶 第一阶段:听大量日常对话 → 学会基本语言能力
👧 第二阶段:学习问答方式 → 懂得对话结构
👨 第三阶段:学习回答问题 → 能按要求回答
```

训练 AI 模型也是一样的**三个阶段**:

| 阶段 | 名称                          | 数据类型     | 学什么                             | 数据量              |
| ---- | ----------------------------- | ------------ | ---------------------------------- | ------------------- |
| 1️⃣   | **预训练**<br>(Pretraining)   | 海量网页文本 | 语言的基本规律<br>语法、词汇、常识 | 超级大<br>(几十 GB) |
| 2️⃣   | **中期训练**<br>(Midtraining) | 对话记录     | 对话的格式<br>一问一答的结构       | 中等<br>(几百 MB)   |
| 3️⃣   | **微调**<br>(Fine-tuning)     | 指令对话对   | 理解和执行指令<br>做个好助手       | 较小<br>(几十 MB)   |

### 为什么要分三个阶段?

**类比**:就像学英语

- 预训练 = 大量阅读英文书籍(学语法和词汇)
- 中期训练 = 学习英语对话(学怎么交流)
- 微调 = 学习回答面试问题(学特定任务)

如果直接让 AI 学习回答问题而不先学语言,就像让完全不懂英语的人直接参加英语面试,肯定学不好!

---

## 📚 第一阶段:预训练数据

### 用什么数据?

项目默认使用 **FineWeb-Edu** 数据集:

- 📖 来源:HuggingFace 整理的高质量网页内容
- 📊 规模:约 1000 亿个单词(是的,1000 亿!)
- ✨ 质量:经过筛选,去掉了垃圾内容
- 🎁 免费:完全开源,直接下载

### 如何下载?

**一条命令搞定:**

```bash
# 下载8个分片(约800MB),用于训练分词器
python -m nanochat.dataset -n 8

# 在后台下载240个分片(约24GB),用于正式训练
python -m nanochat.dataset -n 240 &
```

**解释**:

- `-n 8` 表示下载 8 个数据包
- 每个数据包约 100MB
- `&` 表示在后台运行,你可以继续做其他事

### 数据下载到哪了?

所有数据自动保存到:

```
~/.cache/nanochat/base_data/
├── shard_00000.parquet  # 第1个数据包
├── shard_00001.parquet  # 第2个数据包
├── shard_00002.parquet
└── ...
```

> **小贴士**: `~` 在 Linux/Mac 是你的用户目录,在 Windows 是 `C:\Users\你的用户名\`

### 我需要下载多少数据?

**取决于你要训练多大的模型:**

| 模型规模      | 参数量 | 需要下载   | 磁盘空间 | 训练时间 |
| ------------- | ------ | ---------- | -------- | -------- |
| **d10**(迷你) | 42M    | 16 个分片  | ~2GB     | 30 分钟  |
| **d12**(小)   | 123M   | 48 个分片  | ~5GB     | 1-2 小时 |
| **d20**(默认) | 561M   | 215 个分片 | ~21GB    | 4 小时   |
| **d26**(大)   | 1.2B   | 460 个分片 | ~45GB    | 12 小时  |
| **d32**(超大) | 2.1B   | 806 个分片 | ~79GB    | 24 小时  |

> 💡 **新手建议**: 先用 **d10** 或 **d12** 练手,熟悉流程后再训练大模型! d10 模型体积小、训练快，最适合快速验证流程。

### 验证下载是否成功

```bash
# 查看已下载的文件
ls ~/.cache/nanochat/base_data/

# 统计下载了多少个
ls ~/.cache/nanochat/base_data/*.parquet | wc -l

# 查看文件大小
du -sh ~/.cache/nanochat/base_data/
```

---

## 💬 第二阶段:中期训练数据

### 用什么数据?

项目默认使用 **SmolTalk** 对话数据集:

- 🗣️ 内容:真实的人类对话记录
- 📝 格式:一问一答的对话形式
- 🎯 目的:让模型学会对话的格式

### 数据长什么样?

每条数据是一个对话:

```json
{
  "messages": [
    {
      "role": "user",
      "content": "你好!请介绍一下自己"
    },
    {
      "role": "assistant",
      "content": "你好!我是一个AI助手,可以回答问题、提供建议..."
    },
    {
      "role": "user",
      "content": "你会说中文吗?"
    },
    {
      "role": "assistant",
      "content": "是的,我可以使用中文交流。"
    }
  ]
}
```

**重要字段说明**:

- `role`: 说话的角色,`user`(用户) 或 `assistant`(助手)
- `content`: 说话的内容

### 如何下载?

**好消息:不需要手动下载!**

训练脚本会自动从 HuggingFace 下载 SmolTalk 数据集。你只需运行训练命令就行。

---

## 🎯 第三阶段:微调数据

### 用什么数据?

微调阶段混合使用多个任务数据集:

| 数据集            | 内容       | 数量      | 学什么能力 |
| ----------------- | ---------- | --------- | ---------- |
| **ARC-Easy**      | 简单选择题 | 2,300 条  | 常识推理   |
| **ARC-Challenge** | 困难选择题 | 1,100 条  | 深度推理   |
| **GSM8K**         | 小学数学题 | 8,000 条  | 数学计算   |
| **SmolTalk**      | 日常对话   | 10,000 条 | 闲聊能力   |

**总计: 约 21,400 条训练样本**

### 数据格式是什么样的?

**数学题示例** (GSM8K):

```json
{
  "messages": [
    {
      "role": "user",
      "content": "小明有8个苹果,吃掉了3个,还剩几个?"
    },
    {
      "role": "assistant",
      "content": "让我来算一下:\n8 - 3 = 5\n所以小明还剩5个苹果。"
    }
  ]
}
```

**选择题示例** (ARC):

```json
{
  "messages": [
    {
      "role": "user",
      "content": "哪个物体会浮在水面上?\nA. 石头\nB. 铁钉\nC. 木头\nD. 玻璃球"
    },
    {
      "role": "assistant",
      "content": "答案是C. 木头。因为木头的密度比水小,所以会浮在水面上。"
    }
  ]
}
```

### 如何获取这些数据?

**会自动下载!**

运行微调脚本时,这些数据集(ARC、GSM8K、SmolTalk)会自动从 HuggingFace 下载,无需手动操作。

---

## 🛠️ 实战:准备中文数据

> 如果你想训练中文模型,需要准备中文数据。下面手把手教你!

### 方案一:使用现成的中文数据集(推荐)

#### 1. 创建下载脚本

新建文件 `download_chinese_data.py`:

```python
"""
下载和处理中文数据集
适合新手使用,带详细注释
"""

from datasets import load_dataset, concatenate_datasets
import os
import re

print("=" * 50)
print("开始准备中文训练数据")
print("=" * 50)

# ============================================
# 第1步:下载数据集
# ============================================

print("\n📥 正在下载中文维基百科...")
try:
    wiki = load_dataset(
        "wikipedia",
        "20220301.zh",  # 中文版本
        split="train[:100000]",  # 只取前10万条
        trust_remote_code=True
    )
    print(f"✅ 成功下载维基百科: {len(wiki):,} 条")
except Exception as e:
    print(f"❌ 下载失败: {e}")
    wiki = None

print("\n📥 正在下载百度百科...")
try:
    baike = load_dataset(
        "xusenlin/baidubaike-563w",
        split="train[:200000]"  # 取前20万条
    )
    print(f"✅ 成功下载百度百科: {len(baike):,} 条")
except Exception as e:
    print(f"❌ 下载失败: {e}")
    baike = None

print("\n📥 正在下载中文新闻...")
try:
    news = load_dataset(
        "shibing624/nli_zh",
        "STS-B",
        split="train[:50000]"  # 取前5万条
    )
    print(f"✅ 成功下载新闻数据: {len(news):,} 条")
except Exception as e:
    print(f"❌ 下载失败: {e}")
    news = None

# ============================================
# 第2步:合并数据集
# ============================================

print("\n🔗 正在合并数据集...")
datasets_to_merge = [ds for ds in [wiki, baike, news] if ds is not None]

if not datasets_to_merge:
    print("❌ 没有成功下载任何数据集,程序退出")
    exit(1)

combined = concatenate_datasets(datasets_to_merge)
print(f"✅ 合并完成,总共: {len(combined):,} 条")

# ============================================
# 第3步:清洗数据
# ============================================

print("\n🧹 正在清洗数据...")

def clean_text(example):
    """
    清洗文本数据
    - 提取文本字段
    - 去除特殊字符
    - 过滤太短的文本
    """
    # 提取文本(不同数据集字段名可能不同)
    text = example.get('text') or example.get('content') or example.get('sentence1') or ''

    if not text:
        return None

    # 去除多余空白
    text = re.sub(r'\s+', ' ', text)

    # 去除网址
    text = re.sub(r'http[s]?://\S+', '', text)

    # 去除Email
    text = re.sub(r'\S+@\S+', '', text)

    # 保留中文、英文、数字、常用标点
    text = re.sub(
        r'[^\u4e00-\u9fa5a-zA-Z0-9，。！？、；：""''（）【】《》\s]',
        '',
        text
    )

    # 过滤太短的文本(少于50个字符)
    if len(text.strip()) < 50:
        return None

    return {'text': text.strip()}

# 应用清洗函数
combined = combined.map(
    clean_text,
    num_proc=4,  # 使用4个进程加速
    desc="清洗中"
)

# 过滤掉空值
combined = combined.filter(
    lambda x: x is not None and x.get('text'),
    desc="过滤中"
)

print(f"✅ 清洗完成,剩余: {len(combined):,} 条")

# ============================================
# 第4步:保存数据
# ============================================

print("\n💾 正在保存数据...")

# 创建保存目录
output_dir = os.path.expanduser("~/.cache/nanochat/chinese_data")
os.makedirs(output_dir, exist_ok=True)

# 打乱数据顺序(重要!)
combined = combined.shuffle(seed=42)

# 分割成多个分片(每个10万条)
shard_size = 100000
num_shards = (len(combined) + shard_size - 1) // shard_size

print(f"将分成 {num_shards} 个文件保存...")

for i in range(num_shards):
    start_idx = i * shard_size
    end_idx = min(start_idx + shard_size, len(combined))

    # 选择这个分片的数据
    shard = combined.select(range(start_idx, end_idx))

    # 保存为parquet格式(压缩后的表格文件)
    shard_path = f"{output_dir}/shard_{i:05d}.parquet"
    shard.to_parquet(
        shard_path,
        compression='zstd',  # 使用zstd压缩
        compression_level=3   # 压缩级别
    )

    # 计算文件大小
    file_size = os.path.getsize(shard_path) / (1024 * 1024)  # MB
    print(f"  ✅ 保存 {shard_path}: {len(shard):,} 条 ({file_size:.1f} MB)")

# ============================================
# 完成!
# ============================================

print("\n" + "=" * 50)
print("✅ 中文数据准备完成!")
print("=" * 50)
print(f"📁 保存位置: {output_dir}")
print(f"📊 文件数量: {num_shards} 个")
print(f"📈 数据总量: {len(combined):,} 条")
print(f"💽 磁盘占用: 约 {sum(os.path.getsize(f'{output_dir}/shard_{i:05d}.parquet') for i in range(num_shards)) / (1024**3):.2f} GB")
print("\n下一步:")
print("  python -m scripts.tok_train --max_chars=2000000000")
```

#### 2. 运行下载脚本

```bash
# 安装依赖(如果没有)
pip install datasets

# 运行脚本
python download_chinese_data.py
```

**等待时间**: 取决于网速,通常 10-30 分钟

#### 3. 使用国内镜像加速(可选)

如果下载太慢,可以配置国内镜像:

**方法 1: 临时设置(推荐)**

```bash
# Linux/Mac
export HF_ENDPOINT=https://hf-mirror.com

# Windows PowerShell
$env:HF_ENDPOINT="https://hf-mirror.com"

# Windows CMD
set HF_ENDPOINT=https://hf-mirror.com
```

设置后,在同一个终端窗口运行下载脚本:

```bash
python download_chinese_data.py
```

**方法 2: 永久设置**

在 Python 脚本开头添加:

```python
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# 然后再导入 datasets
from datasets import load_dataset
```

**其他常用镜像**:

- HuggingFace 镜像: `https://hf-mirror.com`
- ModelScope(阿里云): `https://www.modelscope.cn`
- pip 清华镜像: `pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple`

---

## 🔍 数据格式详解

### 什么是 Parquet 格式?

**简单理解**: Parquet 是一种压缩的表格文件格式

**为什么用它?**

- ✅ 压缩效果好:比普通文本文件小 5-10 倍
- ✅ 读取速度快:专门优化过
- ✅ 支持大文件:可以只读取一部分

**文件结构**:

```
shard_00000.parquet
├── 文件头(元数据)
│   └── 告诉你这个文件有什么列、多少行
└── 数据块(Row Groups)
    ├── 第1块: 行1-1024
    ├── 第2块: 行1025-2048
    └── ...
```

### 如何查看 Parquet 文件?

**方法 1: 用 Python 查看**

```python
import pyarrow.parquet as pq

# 读取文件
table = pq.read_table("~/.cache/nanochat/base_data/shard_00000.parquet")

# 查看基本信息
print(f"行数: {len(table)}")
print(f"列名: {table.column_names}")

# 查看前3条数据
for i in range(3):
    print(f"\n--- 第{i+1}条 ---")
    print(table['text'][i])
```

**方法 2: 用在线工具**

1. 安装 `parquet-tools`:

```bash
pip install parquet-tools
```

2. 查看文件:

```bash
parquet-tools show ~/.cache/nanochat/base_data/shard_00000.parquet
```

---

## 🛠️ 数据工具总览

项目提供了一系列开箱即用的数据处理工具,帮你快速检查和转换数据:

| 工具                             | 用途               | 命令                                             |
| -------------------------------- | ------------------ | ------------------------------------------------ |
| **check_data.py**                | 验证数据文件完整性 | `python -m data_check.check_data`                |
| **convert_custom_data.py**       | 转换自定义文本数据 | `python -m data_check.convert_custom_data`       |
| **check_length_distribution.py** | 检查文本长度分布   | `python -m data_check.check_length_distribution` |
| **check_content_quality.py**     | 抽样检查内容质量   | `python -m data_check.check_content_quality`     |
| **check_char_distribution.py**   | 检查字符分布统计   | `python -m data_check.check_char_distribution`   |

**快速开始**:

```bash
# 1. 检查已下载的数据
python -m data_check.check_data

# 2. 查看数据质量
python -m data_check.check_length_distribution
python -m data_check.check_content_quality
python -m data_check.check_char_distribution

# 3. 转换自定义数据
python -m data_check.convert_custom_data
```

> 💡 所有工具的详细代码都在 `data_check/` 目录下,可以根据需要修改!

---

## 🎓 常见问题解答

### Q1: 数据下载太慢怎么办?

**解决方案**:

**方法 1**: 使用国内镜像

```bash
export HF_ENDPOINT=https://hf-mirror.com
```

**方法 2**: 后台下载

```bash
# 让下载在后台进行,关闭终端也不会停止
nohup python -m nanochat.dataset -n 240 > download.log 2>&1 &

# 查看下载进度
tail -f download.log
```

**方法 3**: 减少下载量

```bash
# 只下载48个分片(够训练d12小模型)
python -m nanochat.dataset -n 48
```

### Q2: 我的磁盘空间不够怎么办?

**解决方案**:

1. **训练更小的模型**

```bash
# d12模型只需要5GB数据
python -m nanochat.dataset -n 48
```

2. **清理其他文件**

```bash
# 查看哪些文件占空间
du -sh ~/.cache/nanochat/*

# 删除不需要的检查点
rm -rf ~/.cache/nanochat/base_checkpoints/old_*
```

3. **使用外置硬盘**

```bash
# 把数据放到外置硬盘
mkdir /mnt/external_drive/nanochat_data
ln -s /mnt/external_drive/nanochat_data ~/.cache/nanochat/base_data
```

### Q3: 如何验证数据下载完整?

**检查列表**:

**方法 1: 快速检查文件数量**

```bash
# Linux/Mac
ls ~/.cache/nanochat/base_data/*.parquet 2>/dev/null | wc -l

# Windows PowerShell
(Get-ChildItem "$env:USERPROFILE\.cache\nanochat\base_data\*.parquet").Count
```

**方法 2: 使用内置检查工具**

项目已提供了完整的数据检查脚本:

```bash
# 检查所有已下载的数据文件
python -m data_check.check_data
```

工具会自动:

- 📂 扫描数据目录中的所有 Parquet 文件
- ✅ 验证每个文件是否完整可读
- 📊 统计总数据量
- ⚠️ 标记损坏或空文件

详细代码请查看: [`data_check/check_data.py`](./data_check/check_data.py)

### Q4: 可以用自己收集的数据吗?

**当然可以!** 只需要转换成 Parquet 格式:

```bash
# 使用内置工具转换自定义数据
python -m data_check.convert_custom_data
```

**使用步骤**:

1. 准备好你的文本数据 (`.txt` 文件)
2. 编辑 `data_check/convert_custom_data.py`,设置输入路径
3. 运行转换命令

**支持两种输入方式**:

- 方法 1: 单个文本文件,每行一条数据
- 方法 2: 目录,包含多个 `.txt` 文件

详细代码请查看: [`data_check/convert_custom_data.py`](./data_check/convert_custom_data.py)

### Q5: 怎么知道数据质量好不好?

**检查清单**:

#### 1. 文本长度分布

```bash
# 检查文本长度分布
python -m data_check.check_length_distribution

# 指定其他数据文件
python -m data_check.check_length_distribution ~/.cache/nanochat/my_data/shard_00000.parquet
```

工具会自动:

- 📊 统计平均/最短/最长长度
- 📈 显示长度分布 (按区间)
- 🖼️ 生成可视化图表 (需安装 matplotlib)
- ✅ 给出数据质量评估

详细代码请查看: [`data_check/check_length_distribution.py`](./data_check/check_length_distribution.py)

**好的数据长度分布**:

- 平均长度: 500-2000 字符
- 不要有太多超短文本(<50 字符)
- 也不要都是超长文本(>10000 字符)

#### 2. 内容质量检查

```bash
# 随机抽样检查10条数据
python -m data_check.check_content_quality

# 抽样20条数据
python -m data_check.check_content_quality ~/.cache/nanochat/base_data/shard_00000.parquet 20
```

工具会自动:

- 📝 随机抽取样本展示
- 🔍 自动检测重复、极短文本、可能的乱码
- ✅ 给出内容质量检查清单

详细代码请查看: [`data_check/check_content_quality.py`](./data_check/check_content_quality.py)

**好的数据特征**:

- ✅ 句子完整,语法正确
- ✅ 没有大量乱码
- ✅ 没有大量重复内容
- ✅ 内容有意义,信息量足够

#### 3. 字符分布检查

```bash
# 检查字符分布
python -m data_check.check_char_distribution

# 指定其他数据文件
python -m data_check.check_char_distribution ~/.cache/nanochat/my_data/shard_00000.parquet
```

工具会自动:

- 🔤 统计各类字符占比 (中文/英文/数字/标点等)
- 🔝 显示最常见的字符
- ✅ 给出数据质量评估 (针对中文数据)

详细代码请查看: [`data_check/check_char_distribution.py`](./data_check/check_char_distribution.py)

**中文数据的合理比例**:

- 中文字符: 70-90%
- 英文字符: 5-20%
- 数字: 1-5%

### Q6: 训练时数据是怎么加载的?

**流程详解** (从文件到模型):

```
磁盘上的数据文件 (shard_00000.parquet)
        ↓
    读取一个分片
        ↓
    提取文本 ("今天天气真好...")
        ↓
    分词器转换 ([1234, 5678, 9012])
        ↓
    切分成固定长度 ([1234, 5678], [5678, 9012], ...)
        ↓
    组装成批次 (batch_size=32)
        ↓
    输入到GPU
        ↓
    模型计算
```

**关键特点**:

- **流式加载**: 不需要一次性把所有数据读入内存
- **即时分词**: 读取时才分词,不需要提前分好
- **自动打乱**: 训练时会随机打乱数据顺序

### Q7: 多个 GPU 怎么分配数据?

**自动分配机制**:

```python
# 假设你有8张GPU,编号0-7

# GPU 0 读取: shard_0, shard_8, shard_16, ...
# GPU 1 读取: shard_1, shard_9, shard_17, ...
# GPU 2 读取: shard_2, shard_10, shard_18, ...
# ...
# GPU 7 读取: shard_7, shard_15, shard_23, ...
```

**好处**:

- ✅ 每个 GPU 读取不同数据,不重复
- ✅ 自动负载均衡
- ✅ 你不需要手动分配!

---

## 📊 数据量计算器

### 我需要准备多少数据?

**Chinchilla 定律**: 数据 token 数 = 模型参数量 × 20

**计算步骤**:

```python
"""
数据量计算器
"""

def calculate_data_requirement(model_params_million):
    """
    计算训练所需的数据量

    参数:
        model_params_million: 模型参数量(百万),如123表示123M参数

    返回:
        字典,包含各种数据量信息
    """

    # 1. 需要的token数 (参数量 × 20)
    tokens_billion = model_params_million / 1000 * 20

    # 2. 需要的字符数 (1 token ≈ 4.8 字符)
    chars_billion = tokens_billion * 4.8

    # 3. 需要的分片数 (每个分片250M字符)
    num_shards = int(chars_billion * 1000 / 250)

    # 4. 磁盘空间 (每个分片约100MB)
    disk_gb = num_shards * 100 / 1024

    return {
        'model_params': f"{model_params_million}M",
        'tokens': f"{tokens_billion:.1f}B",
        'chars': f"{chars_billion:.0f}B",
        'shards': num_shards,
        'disk': f"{disk_gb:.1f}GB"
    }

# 示例:不同规模模型
models = {
    'd10': 42,
    'd12': 123,
    'd20': 561,
    'd26': 1200,
    'd32': 2100
}

print("模型数据需求表")
print("=" * 70)
print(f"{'模型':<6} {'参数量':<10} {'Token数':<12} {'字符数':<12} {'分片数':<8} {'磁盘':<10}")
print("-" * 70)

for name, params in models.items():
    req = calculate_data_requirement(params)
    print(f"{name:<6} {req['model_params']:<10} {req['tokens']:<12} {req['chars']:<12} {req['shards']:<8} {req['disk']:<10}")

print("=" * 70)
```

**输出示例**:

```
模型数据需求表
======================================================================
模型     参数量      Token数       字符数        分片数    磁盘
----------------------------------------------------------------------
d10    42M        0.8B         4B           16       1.6GB
d12    123M       2.5B         12B          47       4.6GB
d20    561M       11.2B        54B          215      21.0GB
d26    1200M      24.0B        115B         460      44.9GB
d32    2100M      42.0B        202B         806      78.7GB
======================================================================
```

---

## ✅ 完整流程检查清单

准备好数据了吗? 对照这个清单检查:

### 预训练数据

- [ ] 下载了足够的数据分片(根据模型大小)
- [ ] 验证了文件完整性(能正常打开)
- [ ] 数据保存在 `~/.cache/nanochat/base_data/`
- [ ] 磁盘空间充足(至少预留 30GB)

### 分词器

- [ ] 已运行 `python -m scripts.tok_train`
- [ ] 分词器保存在 `~/.cache/nanochat/tokenizer/`
- [ ] 可以正常编码和解码文本

### 中期训练数据

- [ ] SmolTalk 数据会自动下载(或准备了自己的对话数据)
- [ ] 数据格式正确(有 role 和 content 字段)

### 微调数据

- [ ] 评估数据集会自动下载
- [ ] 或准备了自己的指令数据

### 环境配置

- [ ] 如果在中国,已配置镜像 `export HF_ENDPOINT=https://hf-mirror.com`
- [ ] 网络连接正常,能访问 HuggingFace

---

## 🚀 下一步

数据准备好了! 接下来:

1. **训练分词器**

   ```bash
   python -m scripts.tok_train --max_chars=2000000000
   ```

2. **开始预训练**

   ```bash
   torchrun --standalone --nproc_per_node=8 -m scripts.base_train --depth=20
   ```

---

## 📖 扩展阅读

想深入了解? 推荐阅读:

- 📄 [README.md](README.md) - 项目整体介绍
- 🌐 [FineWeb-Edu 数据集](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)
- 📚 [Chinchilla 论文](https://arxiv.org/abs/2203.15556) - 理解数据量和模型规模的关系

---

## 💬 需要帮助?

遇到问题了?

1. 先查看本文档的"常见问题解答"部分
2. 查看项目的 [GitHub Issues](https://github.com/karpathy/nanochat/issues)
3. 在项目讨论区提问

---

**祝你训练顺利!** 🎉

> 记住:数据质量比数量更重要! 不要盲目追求大数据,先用小模型验证流程,再逐步扩大规模。
