#!/usr/bin/env python3
"""
åˆ›å»ºæ•°æ®å¤„ç†æŒ‡å— Jupyter Notebook
"""
import json

notebook = {
    "cells": [],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {"name": "ipython", "version": 3},
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

def add_markdown(text):
    """æ·»åŠ  markdown å•å…ƒæ ¼"""
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": text.split("\n")
    })

def add_code(code):
    """æ·»åŠ ä»£ç å•å…ƒæ ¼"""
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": code.split("\n")
    })

# ============= æ ‡é¢˜å’Œç›®å½• =============
add_markdown("""# ğŸ“¦ NanoChat æ•°æ®å¤„ç†æŒ‡å—

> **å†™ç»™å°ç™½çš„è¯**ï¼šè¿™ä¸ª Notebook ä¼šæ‰‹æŠŠæ‰‹æ•™ä½ å¦‚ä½•å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä¸éœ€è¦ä»»ä½•ä¸“ä¸šèƒŒæ™¯ï¼Œè·Ÿç€è¿è¡Œæ¯ä¸ªå•å…ƒæ ¼å°±è¡Œï¼

---

## ğŸ“š ç›®å½•

1. [æ ¸å¿ƒæ¦‚å¿µï¼š3 åˆ†é’Ÿå¿«é€Ÿç†è§£](#æ ¸å¿ƒæ¦‚å¿µ)
2. [ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒæ•°æ®](#é¢„è®­ç»ƒæ•°æ®)
3. [ç¬¬äºŒé˜¶æ®µï¼šä¸­æœŸè®­ç»ƒæ•°æ®](#ä¸­æœŸè®­ç»ƒæ•°æ®)
4. [ç¬¬ä¸‰é˜¶æ®µï¼šå¾®è°ƒæ•°æ®](#å¾®è°ƒæ•°æ®)
5. [å®æˆ˜ï¼šå‡†å¤‡ä¸­æ–‡æ•°æ®](#å‡†å¤‡ä¸­æ–‡æ•°æ®)
6. [æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·](#æ•°æ®è´¨é‡æ£€æŸ¥)
7. [æ•°æ®é‡è®¡ç®—å™¨](#æ•°æ®é‡è®¡ç®—å™¨)
8. [å®Œæ•´æµç¨‹æ£€æŸ¥æ¸…å•](#æ£€æŸ¥æ¸…å•)""")

# ============= ç¬¬1ç« ï¼šæ ¸å¿ƒæ¦‚å¿µ =============
add_markdown("""---

## <a id="æ ¸å¿ƒæ¦‚å¿µ"></a>1. æ ¸å¿ƒæ¦‚å¿µï¼š3 åˆ†é’Ÿå¿«é€Ÿç†è§£

### è®­ç»ƒ AI éœ€è¦ä»€ä¹ˆæ•°æ®ï¼Ÿ

æƒ³è±¡ä¸€ä¸‹æ•™å°å­©å­¦è¯´è¯çš„è¿‡ç¨‹ï¼š

```
ğŸ‘¶ ç¬¬ä¸€é˜¶æ®µï¼šå¬å¤§é‡æ—¥å¸¸å¯¹è¯ â†’ å­¦ä¼šåŸºæœ¬è¯­è¨€èƒ½åŠ›
ğŸ‘§ ç¬¬äºŒé˜¶æ®µï¼šå­¦ä¹ é—®ç­”æ–¹å¼ â†’ æ‡‚å¾—å¯¹è¯ç»“æ„  
ğŸ‘¨ ç¬¬ä¸‰é˜¶æ®µï¼šå­¦ä¹ å›ç­”é—®é¢˜ â†’ èƒ½æŒ‰è¦æ±‚å›ç­”
```

è®­ç»ƒ AI æ¨¡å‹ä¹Ÿæ˜¯ä¸€æ ·çš„ **ä¸‰ä¸ªé˜¶æ®µ**ï¼š""")

add_code("""import pandas as pd

# åˆ›å»ºè®­ç»ƒé˜¶æ®µå¯¹æ¯”è¡¨
training_stages = pd.DataFrame({
    'é˜¶æ®µ': ['1ï¸âƒ£', '2ï¸âƒ£', '3ï¸âƒ£'],
    'åç§°': ['é¢„è®­ç»ƒ (Pretraining)', 'ä¸­æœŸè®­ç»ƒ (Midtraining)', 'å¾®è°ƒ (Fine-tuning)'],
    'æ•°æ®ç±»å‹': ['æµ·é‡ç½‘é¡µæ–‡æœ¬', 'å¯¹è¯è®°å½•', 'æŒ‡ä»¤å¯¹è¯å¯¹'],
    'å­¦ä»€ä¹ˆ': ['è¯­è¨€çš„åŸºæœ¬è§„å¾‹ã€è¯­æ³•ã€è¯æ±‡ã€å¸¸è¯†', 'å¯¹è¯çš„æ ¼å¼ã€ä¸€é—®ä¸€ç­”çš„ç»“æ„', 'ç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤ã€åšä¸ªå¥½åŠ©æ‰‹'],
    'æ•°æ®é‡': ['è¶…çº§å¤§ (å‡ å GB)', 'ä¸­ç­‰ (å‡ ç™¾ MB)', 'è¾ƒå° (å‡ å MB)']
})

print("\\nğŸ¯ AI è®­ç»ƒçš„ä¸‰ä¸ªé˜¶æ®µ\\n")
display(training_stages)
print("\\n" + "="*80)""")

add_markdown("""### ğŸ’¡ ä¸ºä»€ä¹ˆè¦åˆ†ä¸‰ä¸ªé˜¶æ®µï¼Ÿ

**ç±»æ¯”ï¼šå°±åƒå­¦è‹±è¯­**

- **é¢„è®­ç»ƒ** = å¤§é‡é˜…è¯»è‹±æ–‡ä¹¦ç±ï¼ˆå­¦è¯­æ³•å’Œè¯æ±‡ï¼‰
- **ä¸­æœŸè®­ç»ƒ** = å­¦ä¹ è‹±è¯­å¯¹è¯ï¼ˆå­¦æ€ä¹ˆäº¤æµï¼‰
- **å¾®è°ƒ** = å­¦ä¹ å›ç­”é¢è¯•é—®é¢˜ï¼ˆå­¦ç‰¹å®šä»»åŠ¡ï¼‰

å¦‚æœç›´æ¥è®© AI å­¦ä¹ å›ç­”é—®é¢˜è€Œä¸å…ˆå­¦è¯­è¨€ï¼Œå°±åƒè®©å®Œå…¨ä¸æ‡‚è‹±è¯­çš„äººç›´æ¥å‚åŠ è‹±è¯­é¢è¯•ï¼Œè‚¯å®šå­¦ä¸å¥½ï¼""")

# ============= ç¬¬2ç« ï¼šé¢„è®­ç»ƒæ•°æ® =============
add_markdown("""---

## <a id="é¢„è®­ç»ƒæ•°æ®"></a>2. ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒæ•°æ®

### ç”¨ä»€ä¹ˆæ•°æ®ï¼Ÿ

é¡¹ç›®é»˜è®¤ä½¿ç”¨ **FineWeb-Edu** æ•°æ®é›†ï¼š

- ğŸ“– æ¥æºï¼šHuggingFace æ•´ç†çš„é«˜è´¨é‡ç½‘é¡µå†…å®¹
- ğŸ“Š è§„æ¨¡ï¼šçº¦ 1000 äº¿ä¸ªå•è¯ï¼ˆæ˜¯çš„ï¼Œ1000 äº¿ï¼ï¼‰
- âœ¨ è´¨é‡ï¼šç»è¿‡ç­›é€‰ï¼Œå»æ‰äº†åƒåœ¾å†…å®¹
- ğŸ å…è´¹ï¼šå®Œå…¨å¼€æºï¼Œç›´æ¥ä¸‹è½½

### ğŸ“Š æˆ‘éœ€è¦ä¸‹è½½å¤šå°‘æ•°æ®ï¼Ÿ

å–å†³äºä½ è¦è®­ç»ƒå¤šå¤§çš„æ¨¡å‹ï¼š""")

add_code("""# ä¸åŒæ¨¡å‹è§„æ¨¡çš„æ•°æ®éœ€æ±‚å¯¹æ¯”è¡¨
data_requirements = pd.DataFrame({
    'æ¨¡å‹è§„æ¨¡': ['d10 (è¿·ä½ )', 'd12 (å°)', 'd20 (é»˜è®¤)', 'd26 (å¤§)', 'd32 (è¶…å¤§)'],
    'å‚æ•°é‡': ['42M', '123M', '561M', '1.2B', '2.1B'],
    'éœ€è¦ä¸‹è½½': ['16 ä¸ªåˆ†ç‰‡', '48 ä¸ªåˆ†ç‰‡', '215 ä¸ªåˆ†ç‰‡', '460 ä¸ªåˆ†ç‰‡', '806 ä¸ªåˆ†ç‰‡'],
    'ç£ç›˜ç©ºé—´': ['~2GB', '~5GB', '~21GB', '~45GB', '~79GB'],
    'è®­ç»ƒæ—¶é—´': ['30 åˆ†é’Ÿ', '1-2 å°æ—¶', '4 å°æ—¶', '12 å°æ—¶', '24 å°æ—¶']
})

print("\\nğŸ“Š æ¨¡å‹è§„æ¨¡ä¸æ•°æ®éœ€æ±‚å¯¹ç…§è¡¨\\n")
display(data_requirements)
print("\\nğŸ’¡ æ–°æ‰‹å»ºè®®ï¼šå…ˆç”¨ d10 æˆ– d12 ç»ƒæ‰‹ï¼Œç†Ÿæ‚‰æµç¨‹åå†è®­ç»ƒå¤§æ¨¡å‹ï¼")
print("="*80)""")

add_markdown("""### ğŸš€ å¦‚ä½•ä¸‹è½½ï¼Ÿ

**ä¸€æ¡å‘½ä»¤æå®šï¼** è¿è¡Œä¸‹é¢çš„ä»£ç å•å…ƒæ ¼ï¼š""")

add_code("""# ä¸‹è½½ 8 ä¸ªåˆ†ç‰‡ç”¨äºè®­ç»ƒåˆ†è¯å™¨ï¼ˆçº¦ 800MBï¼‰
# è¿™æ˜¯æœ€å°ä¸‹è½½é‡ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•

!python -m nanochat.dataset -n 8""")

add_code("""# å¦‚æœè¦è®­ç»ƒ d20 æ¨¡å‹ï¼Œéœ€è¦ä¸‹è½½æ›´å¤šæ•°æ®
# âš ï¸ è­¦å‘Šï¼šè¿™ä¼šä¸‹è½½çº¦ 21GB æ•°æ®ï¼Œéœ€è¦è¾ƒé•¿æ—¶é—´ï¼
# å¦‚æœä¸éœ€è¦ï¼Œè¯·ä¸è¦è¿è¡Œè¿™ä¸ªå•å…ƒæ ¼

# !python -m nanochat.dataset -n 215""")

add_markdown("""### ğŸ“ æ•°æ®ä¸‹è½½åˆ°å“ªäº†ï¼Ÿ

æ‰€æœ‰æ•°æ®è‡ªåŠ¨ä¿å­˜åˆ° `~/.cache/nanochat/base_data/`

è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ï¼š""")

add_code("""import os
from pathlib import Path

# è·å–æ•°æ®ç›®å½•
data_dir = Path.home() / ".cache" / "nanochat" / "base_data"

print(f"ğŸ“ æ•°æ®ç›®å½•: {data_dir}\\n")

if data_dir.exists():
    # ç»Ÿè®¡å·²ä¸‹è½½çš„æ–‡ä»¶
    parquet_files = list(data_dir.glob("*.parquet"))
    
    if parquet_files:
        print(f"âœ… æ‰¾åˆ° {len(parquet_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        # è®¡ç®—æ€»å¤§å°
        total_size = sum(f.stat().st_size for f in parquet_files)
        print(f"ğŸ’½ æ€»å¤§å°: {total_size / (1024**3):.2f} GB")
        
        # æ˜¾ç¤ºå‰ 5 ä¸ªæ–‡ä»¶
        print("\\nå‰ 5 ä¸ªæ–‡ä»¶:")
        for f in sorted(parquet_files)[:5]:
            size_mb = f.stat().st_size / (1024**2)
            print(f"  ğŸ“„ {f.name:25s} ({size_mb:.1f} MB)")
    else:
        print("âš ï¸ æ•°æ®ç›®å½•å­˜åœ¨ï¼Œä½†æ²¡æœ‰æ‰¾åˆ° .parquet æ–‡ä»¶")
        print("   è¯·å…ˆè¿è¡Œä¸Šé¢çš„ä¸‹è½½å‘½ä»¤ï¼")
else:
    print("âš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®ï¼")
    print(f"   è¿è¡Œ: python -m nanochat.dataset -n 8")""")

add_markdown("""### ğŸ” æŸ¥çœ‹æ•°æ®å†…å®¹

è®©æˆ‘ä»¬æ‰“å¼€ä¸€ä¸ªæ–‡ä»¶çœ‹çœ‹é‡Œé¢æ˜¯ä»€ä¹ˆï¼š""")

add_code("""import pyarrow.parquet as pq

# è¯»å–ç¬¬ä¸€ä¸ªåˆ†ç‰‡
data_dir = Path.home() / ".cache" / "nanochat" / "base_data"
parquet_files = list(data_dir.glob("*.parquet")) if data_dir.exists() else []

if parquet_files:
    first_file = sorted(parquet_files)[0]
    print(f"ğŸ“– æ­£åœ¨è¯»å–: {first_file.name}\\n")
    
    # è¯»å– Parquet æ–‡ä»¶
    table = pq.read_table(first_file)
    
    print(f"ğŸ“Š æ–‡ä»¶ä¿¡æ¯:")
    print(f"   è¡Œæ•°: {len(table):,}")
    print(f"   åˆ—å: {table.column_names}")
    
    # æ˜¾ç¤ºå‰ 3 æ¡æ•°æ®
    print("\\nğŸ“ å‰ 3 æ¡æ•°æ®ç¤ºä¾‹:\\n")
    print("=" * 80)
    
    for i in range(min(3, len(table))):
        text = table['text'][i].as_py()
        # åªæ˜¾ç¤ºå‰ 200 ä¸ªå­—ç¬¦
        preview = text[:200] + "..." if len(text) > 200 else text
        print(f"\\nç¬¬ {i+1} æ¡ (é•¿åº¦: {len(text)} å­—ç¬¦)")
        print("-" * 80)
        print(preview)
    
    print("\\n" + "=" * 80)
else:
    print("âš ï¸ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®ï¼")""")

# ============= ç¬¬3ç« ï¼šä¸­æœŸè®­ç»ƒæ•°æ® =============
add_markdown("""---

## <a id="ä¸­æœŸè®­ç»ƒæ•°æ®"></a>3. ç¬¬äºŒé˜¶æ®µï¼šä¸­æœŸè®­ç»ƒæ•°æ®

### ç”¨ä»€ä¹ˆæ•°æ®ï¼Ÿ

é¡¹ç›®é»˜è®¤ä½¿ç”¨ **SmolTalk** å¯¹è¯æ•°æ®é›†ï¼š

- ğŸ—£ï¸ å†…å®¹ï¼šçœŸå®çš„äººç±»å¯¹è¯è®°å½•
- ğŸ“ æ ¼å¼ï¼šä¸€é—®ä¸€ç­”çš„å¯¹è¯å½¢å¼
- ğŸ¯ ç›®çš„ï¼šè®©æ¨¡å‹å­¦ä¼šå¯¹è¯çš„æ ¼å¼

### æ•°æ®æ ¼å¼ç¤ºä¾‹""")

add_code("""import json

# å¯¹è¯æ•°æ®æ ¼å¼ç¤ºä¾‹
dialogue_example = {
    "messages": [
        {
            "role": "user",
            "content": "ä½ å¥½ï¼è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"
        },
        {
            "role": "assistant",
            "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”é—®é¢˜ã€æä¾›å»ºè®®..."
        },
        {
            "role": "user",
            "content": "ä½ ä¼šè¯´ä¸­æ–‡å—ï¼Ÿ"
        },
        {
            "role": "assistant",
            "content": "æ˜¯çš„ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨ä¸­æ–‡äº¤æµã€‚"
        }
    ]
}

print("ğŸ“ å¯¹è¯æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š\\n")
print(json.dumps(dialogue_example, ensure_ascii=False, indent=2))

print("\\nğŸ’¡ é‡è¦å­—æ®µè¯´æ˜ï¼š")
print("   â€¢ role: è¯´è¯çš„è§’è‰²ï¼Œ'user'(ç”¨æˆ·) æˆ– 'assistant'(åŠ©æ‰‹)")
print("   â€¢ content: è¯´è¯çš„å†…å®¹")

print("\\nâœ… å¥½æ¶ˆæ¯ï¼šè®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨ä¸‹è½½ SmolTalk æ•°æ®é›†ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œï¼")""")

# ============= ç¬¬4ç« ï¼šå¾®è°ƒæ•°æ® =============
add_markdown("""---

## <a id="å¾®è°ƒæ•°æ®"></a>4. ç¬¬ä¸‰é˜¶æ®µï¼šå¾®è°ƒæ•°æ®

### ç”¨ä»€ä¹ˆæ•°æ®ï¼Ÿ

å¾®è°ƒé˜¶æ®µæ··åˆä½¿ç”¨å¤šä¸ªä»»åŠ¡æ•°æ®é›†ï¼š""")

add_code("""# å¾®è°ƒæ•°æ®é›†æ¦‚è§ˆ
sft_datasets = pd.DataFrame({
    'æ•°æ®é›†': ['ARC-Easy', 'ARC-Challenge', 'GSM8K', 'SmolTalk'],
    'å†…å®¹': ['ç®€å•é€‰æ‹©é¢˜', 'å›°éš¾é€‰æ‹©é¢˜', 'å°å­¦æ•°å­¦é¢˜', 'æ—¥å¸¸å¯¹è¯'],
    'æ•°é‡': ['2,300 æ¡', '1,100 æ¡', '8,000 æ¡', '10,000 æ¡'],
    'å­¦ä»€ä¹ˆèƒ½åŠ›': ['å¸¸è¯†æ¨ç†', 'æ·±åº¦æ¨ç†', 'æ•°å­¦è®¡ç®—', 'é—²èŠèƒ½åŠ›']
})

print("\\nğŸ¯ å¾®è°ƒé˜¶æ®µçš„æ•°æ®é›†\\n")
display(sft_datasets)
print("\\nğŸ“Š æ€»è®¡ï¼šçº¦ 21,400 æ¡è®­ç»ƒæ ·æœ¬")
print("="*80)""")

add_markdown("""### æ•°æ®æ ¼å¼ç¤ºä¾‹""")

add_code("""# æ•°å­¦é¢˜ç¤ºä¾‹ (GSM8K)
math_example = {
    "messages": [
        {
            "role": "user",
            "content": "å°æ˜æœ‰8ä¸ªè‹¹æœï¼Œåƒæ‰äº†3ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼Ÿ"
        },
        {
            "role": "assistant",
            "content": "è®©æˆ‘æ¥ç®—ä¸€ä¸‹ï¼š\\n8 - 3 = 5\\næ‰€ä»¥å°æ˜è¿˜å‰©5ä¸ªè‹¹æœã€‚"
        }
    ]
}

# é€‰æ‹©é¢˜ç¤ºä¾‹ (ARC)
arc_example = {
    "messages": [
        {
            "role": "user",
            "content": "å“ªä¸ªç‰©ä½“ä¼šæµ®åœ¨æ°´é¢ä¸Šï¼Ÿ\\nA. çŸ³å¤´\\nB. é“é’‰\\nC. æœ¨å¤´\\nD. ç»ç’ƒçƒ"
        },
        {
            "role": "assistant",
            "content": "ç­”æ¡ˆæ˜¯C. æœ¨å¤´ã€‚å› ä¸ºæœ¨å¤´çš„å¯†åº¦æ¯”æ°´å°ï¼Œæ‰€ä»¥ä¼šæµ®åœ¨æ°´é¢ä¸Šã€‚"
        }
    ]
}

print("ğŸ“ æ•°å­¦é¢˜ç¤ºä¾‹ (GSM8K)ï¼š\\n")
print(json.dumps(math_example, ensure_ascii=False, indent=2))

print("\\n" + "="*80 + "\\n")

print("ğŸ“ é€‰æ‹©é¢˜ç¤ºä¾‹ (ARC)ï¼š\\n")
print(json.dumps(arc_example, ensure_ascii=False, indent=2))

print("\\nâœ… è¿™äº›æ•°æ®é›†ä¼šåœ¨è¿è¡Œå¾®è°ƒè„šæœ¬æ—¶è‡ªåŠ¨ä¸‹è½½ï¼")""")

# ============= ç¬¬5ç« ï¼šå‡†å¤‡ä¸­æ–‡æ•°æ® =============
add_markdown("""---

## <a id="å‡†å¤‡ä¸­æ–‡æ•°æ®"></a>5. å®æˆ˜ï¼šå‡†å¤‡ä¸­æ–‡æ•°æ®

> å¦‚æœä½ æƒ³è®­ç»ƒä¸­æ–‡æ¨¡å‹ï¼Œéœ€è¦å‡†å¤‡ä¸­æ–‡æ•°æ®ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ HuggingFace ä¸­æ–‡æ•°æ®é›†""")

add_code("""# è®¾ç½® HuggingFace é•œåƒï¼ˆå¯é€‰ï¼Œå¦‚æœä¸‹è½½å¤ªæ…¢ï¼‰
import os

# ä½¿ç”¨å›½å†…é•œåƒåŠ é€Ÿä¸‹è½½
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

print("âœ… å·²è®¾ç½® HuggingFace é•œåƒï¼šhttps://hf-mirror.com")
print("   è¿™ä¼šåŠ é€Ÿæ•°æ®é›†ä¸‹è½½é€Ÿåº¦")""")

add_code("""# ä¸‹è½½ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰
# âš ï¸ è­¦å‘Šï¼šè¿™ä¼šä¸‹è½½è¾ƒå¤§çš„æ•°æ®é›†ï¼Œéœ€è¦æ—¶é—´ï¼
# å¦‚æœä¸éœ€è¦ï¼Œè¯·ä¸è¦è¿è¡Œè¿™ä¸ªå•å…ƒæ ¼

from datasets import load_dataset

print("ğŸ“¥ æ­£åœ¨ä¸‹è½½ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼ˆå‰ 1000 æ¡ç”¨äºæ¼”ç¤ºï¼‰...\\n")

try:
    # åªä¸‹è½½å‰ 1000 æ¡ç”¨äºæ¼”ç¤º
    wiki = load_dataset(
        "wikipedia",
        "20220301.zh",  # ä¸­æ–‡ç‰ˆæœ¬
        split="train[:1000]",  # åªå–å‰ 1000 æ¡
        trust_remote_code=True
    )
    
    print(f"âœ… æˆåŠŸä¸‹è½½ï¼š{len(wiki):,} æ¡æ•°æ®\\n")
    
    # æ˜¾ç¤ºç¬¬ä¸€æ¡
    print("ğŸ“ ç¬¬ä¸€æ¡æ•°æ®ç¤ºä¾‹ï¼š")
    print("="*80)
    first_text = wiki[0]['text'][:300] + "..."
    print(first_text)
    print("="*80)
    
except Exception as e:
    print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{e}")
    print("   å¯èƒ½éœ€è¦æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨é•œåƒ")""")

add_markdown("""### æ–¹æ³•äºŒï¼šè½¬æ¢è‡ªå·±çš„æ–‡æœ¬æ•°æ®

å¦‚æœä½ æœ‰è‡ªå·±æ”¶é›†çš„ä¸­æ–‡æ–‡æœ¬ï¼Œå¯ä»¥ä½¿ç”¨é¡¹ç›®æä¾›çš„è½¬æ¢å·¥å…·ï¼š""")

add_code("""# ä½¿ç”¨å†…ç½®å·¥å…·è½¬æ¢è‡ªå®šä¹‰æ•°æ®
# è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ data_check/convert_custom_data.py

print("ğŸ› ï¸ è½¬æ¢è‡ªå®šä¹‰æ–‡æœ¬æ•°æ®çš„æ­¥éª¤ï¼š\\n")
print("1. å‡†å¤‡ä½ çš„æ–‡æœ¬æ•°æ®ï¼ˆ.txt æ–‡ä»¶ï¼‰")
print("2. è¿è¡Œè½¬æ¢å‘½ä»¤ï¼š")
print("   python -m data_check.convert_custom_data")
print("\\næ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š")
print("   â€¢ å•ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€æ¡æ•°æ®ï¼‰")
print("   â€¢ ç›®å½•ï¼ˆåŒ…å«å¤šä¸ª .txt æ–‡ä»¶ï¼‰")
print("\\nè¯¦ç»†ä»£ç è¯·æŸ¥çœ‹ï¼šdata_check/convert_custom_data.py")""")

# ============= ç¬¬6ç« ï¼šæ•°æ®è´¨é‡æ£€æŸ¥ =============
add_markdown("""---

## <a id="æ•°æ®è´¨é‡æ£€æŸ¥"></a>6. æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·

é¡¹ç›®æä¾›äº†å®Œæ•´çš„æ•°æ®æ£€æŸ¥å·¥å…·é›†ï¼š""")

add_code("""# æ•°æ®æ£€æŸ¥å·¥å…·æ¦‚è§ˆ
tools = pd.DataFrame({
    'å·¥å…·': [
        'check_data.py',
        'check_length_distribution.py',
        'check_content_quality.py',
        'check_char_distribution.py',
        'convert_custom_data.py'
    ],
    'ç”¨é€”': [
        'éªŒè¯æ•°æ®æ–‡ä»¶å®Œæ•´æ€§',
        'æ£€æŸ¥æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ',
        'æŠ½æ ·æ£€æŸ¥å†…å®¹è´¨é‡',
        'æ£€æŸ¥å­—ç¬¦åˆ†å¸ƒç»Ÿè®¡',
        'è½¬æ¢è‡ªå®šä¹‰æ–‡æœ¬æ•°æ®'
    ],
    'å‘½ä»¤': [
        'python -m data_check.check_data',
        'python -m data_check.check_length_distribution',
        'python -m data_check.check_content_quality',
        'python -m data_check.check_char_distribution',
        'python -m data_check.convert_custom_data'
    ]
})

print("\\nğŸ› ï¸ æ•°æ®æ£€æŸ¥å·¥å…·æ€»è§ˆ\\n")
display(tools)
print("\\nğŸ’¡ æ‰€æœ‰å·¥å…·çš„è¯¦ç»†ä»£ç éƒ½åœ¨ data_check/ ç›®å½•ä¸‹")
print("="*80)""")

add_markdown("""### å¿«é€Ÿæ£€æŸ¥æ•°æ®å®Œæ•´æ€§""")

add_code("""# è¿è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
!python -m data_check.check_data""")

add_markdown("""### æ£€æŸ¥æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ""")

add_code("""# åˆ†ææ•°æ®çš„é•¿åº¦åˆ†å¸ƒ
# è¿™æœ‰åŠ©äºäº†è§£æ•°æ®è´¨é‡

!python -m data_check.check_length_distribution""")

# ============= ç¬¬7ç« ï¼šæ•°æ®é‡è®¡ç®—å™¨ =============
add_markdown("""---

## <a id="æ•°æ®é‡è®¡ç®—å™¨"></a>7. æ•°æ®é‡è®¡ç®—å™¨

### Chinchilla å®šå¾‹

**æ•°æ® token æ•° = æ¨¡å‹å‚æ•°é‡ Ã— 20**

è®©æˆ‘ä»¬è®¡ç®—ä¸åŒæ¨¡å‹éœ€è¦å¤šå°‘æ•°æ®ï¼š""")

add_code("""def calculate_data_requirement(model_params_million):
    \"\"\"
    è®¡ç®—è®­ç»ƒæ‰€éœ€çš„æ•°æ®é‡
    
    å‚æ•°:
        model_params_million: æ¨¡å‹å‚æ•°é‡(ç™¾ä¸‡)ï¼Œå¦‚123è¡¨ç¤º123Må‚æ•°
    
    è¿”å›:
        å­—å…¸ï¼ŒåŒ…å«å„ç§æ•°æ®é‡ä¿¡æ¯
    \"\"\"
    
    # 1. éœ€è¦çš„ token æ•°ï¼ˆå‚æ•°é‡ Ã— 20ï¼‰
    tokens_billion = model_params_million / 1000 * 20
    
    # 2. éœ€è¦çš„å­—ç¬¦æ•°ï¼ˆ1 token â‰ˆ 4.8 å­—ç¬¦ï¼‰
    chars_billion = tokens_billion * 4.8
    
    # 3. éœ€è¦çš„åˆ†ç‰‡æ•°ï¼ˆæ¯ä¸ªåˆ†ç‰‡ 250M å­—ç¬¦ï¼‰
    num_shards = int(chars_billion * 1000 / 250)
    
    # 4. ç£ç›˜ç©ºé—´ï¼ˆæ¯ä¸ªåˆ†ç‰‡çº¦ 100MBï¼‰
    disk_gb = num_shards * 100 / 1024
    
    return {
        'model_params': f"{model_params_million}M",
        'tokens': f"{tokens_billion:.1f}B",
        'chars': f"{chars_billion:.0f}B",
        'shards': num_shards,
        'disk': f"{disk_gb:.1f}GB"
    }

# ä¸åŒè§„æ¨¡æ¨¡å‹
models = {
    'd10': 42,
    'd12': 123,
    'd20': 561,
    'd26': 1200,
    'd32': 2100
}

results = []
for name, params in models.items():
    req = calculate_data_requirement(params)
    results.append({
        'æ¨¡å‹': name,
        'å‚æ•°é‡': req['model_params'],
        'Tokenæ•°': req['tokens'],
        'å­—ç¬¦æ•°': req['chars'],
        'åˆ†ç‰‡æ•°': req['shards'],
        'ç£ç›˜': req['disk']
    })

df_results = pd.DataFrame(results)

print("\\nğŸ“Š æ¨¡å‹æ•°æ®éœ€æ±‚è®¡ç®—è¡¨\\n")
display(df_results)
print("\\nğŸ’¡ æç¤ºï¼šæ•°æ®é‡åŸºäº Chinchilla å®šå¾‹è®¡ç®—ï¼ˆå‚æ•°é‡ Ã— 20ï¼‰")""")

add_markdown("""### è‡ªå®šä¹‰è®¡ç®—

è¾“å…¥ä½ çš„æ¨¡å‹å‚æ•°é‡ï¼Œè®¡ç®—éœ€è¦å¤šå°‘æ•°æ®ï¼š""")

add_code("""# è‡ªå®šä¹‰æ¨¡å‹å‚æ•°é‡ï¼ˆå•ä½ï¼šç™¾ä¸‡ï¼‰
my_model_params = 100  # ä¿®æ”¹è¿™é‡Œï¼

result = calculate_data_requirement(my_model_params)

print(f"\\nğŸ¯ æ‚¨çš„æ¨¡å‹ï¼ˆ{my_model_params}M å‚æ•°ï¼‰éœ€è¦ï¼š\\n")
print(f"   Token æ•°é‡ï¼š{result['tokens']}")
print(f"   å­—ç¬¦æ•°é‡ï¼š{result['chars']}")
print(f"   æ•°æ®åˆ†ç‰‡ï¼š{result['shards']} ä¸ª")
print(f"   ç£ç›˜ç©ºé—´ï¼š{result['disk']}")
print("\\nä¸‹è½½å‘½ä»¤ï¼š")
print(f"   python -m nanochat.dataset -n {result['shards']}")""")

# ============= ç¬¬8ç« ï¼šæ£€æŸ¥æ¸…å• =============
add_markdown("""---

## <a id="æ£€æŸ¥æ¸…å•"></a>8. å®Œæ•´æµç¨‹æ£€æŸ¥æ¸…å•

å‡†å¤‡å¥½æ•°æ®äº†å—ï¼Ÿå¯¹ç…§è¿™ä¸ªæ¸…å•æ£€æŸ¥ï¼š""")

add_code("""import shutil

def check_data_readiness():
    \"\"\"æ£€æŸ¥æ•°æ®å‡†å¤‡æƒ…å†µ\"\"\"
    
    print("\\nğŸ” æ•°æ®å‡†å¤‡çŠ¶æ€æ£€æŸ¥\\n")
    print("="*80)
    
    checks = []
    
    # 1. æ£€æŸ¥é¢„è®­ç»ƒæ•°æ®
    base_data_dir = Path.home() / ".cache" / "nanochat" / "base_data"
    if base_data_dir.exists():
        parquet_files = list(base_data_dir.glob("*.parquet"))
        if len(parquet_files) >= 8:
            checks.append(("âœ…", f"é¢„è®­ç»ƒæ•°æ®ï¼šæ‰¾åˆ° {len(parquet_files)} ä¸ªåˆ†ç‰‡"))
        else:
            checks.append(("âš ï¸", f"é¢„è®­ç»ƒæ•°æ®ï¼šåªæœ‰ {len(parquet_files)} ä¸ªåˆ†ç‰‡ï¼ˆå»ºè®®è‡³å°‘ 8 ä¸ªï¼‰"))
    else:
        checks.append(("âŒ", "é¢„è®­ç»ƒæ•°æ®ï¼šæœªä¸‹è½½"))
    
    # 2. æ£€æŸ¥åˆ†è¯å™¨
    tokenizer_dir = Path.home() / ".cache" / "nanochat" / "tokenizer"
    if tokenizer_dir.exists() and list(tokenizer_dir.glob("*.model")):
        checks.append(("âœ…", "åˆ†è¯å™¨ï¼šå·²è®­ç»ƒ"))
    else:
        checks.append(("âš ï¸", "åˆ†è¯å™¨ï¼šæœªè®­ç»ƒï¼ˆéœ€è¦è¿è¡Œ tok_trainï¼‰"))
    
    # 3. æ£€æŸ¥ç£ç›˜ç©ºé—´
    cache_dir = Path.home() / ".cache"
    if cache_dir.exists():
        try:
            stat = shutil.disk_usage(cache_dir)
            free_gb = stat.free / (1024**3)
            if free_gb > 30:
                checks.append(("âœ…", f"ç£ç›˜ç©ºé—´ï¼šå‰©ä½™ {free_gb:.1f} GB"))
            else:
                checks.append(("âš ï¸", f"ç£ç›˜ç©ºé—´ï¼šå‰©ä½™ {free_gb:.1f} GBï¼ˆå»ºè®®è‡³å°‘ 30GBï¼‰"))
        except:
            checks.append(("â„¹ï¸", "ç£ç›˜ç©ºé—´ï¼šæ— æ³•æ£€æµ‹"))
    
    # 4. æ£€æŸ¥ç¯å¢ƒå˜é‡
    if 'HF_ENDPOINT' in os.environ:
        checks.append(("âœ…", f"HuggingFace é•œåƒï¼š{os.environ['HF_ENDPOINT']}"))
    else:
        checks.append(("â„¹ï¸", "HuggingFace é•œåƒï¼šæœªè®¾ç½®ï¼ˆå›½å†…ç”¨æˆ·å»ºè®®è®¾ç½®ï¼‰"))
    
    # æ˜¾ç¤ºç»“æœ
    for status, msg in checks:
        print(f"{status} {msg}")
    
    print("="*80)
    
    # æ€»ç»“
    ready_count = sum(1 for s, _ in checks if s == "âœ…")
    total_count = len(checks)
    
    print(f"\\nğŸ“Š å°±ç»ªçŠ¶æ€ï¼š{ready_count}/{total_count}")
    
    if ready_count >= 2:  # è‡³å°‘æœ‰æ•°æ®å’Œç©ºé—´å°±ç®—åŸºæœ¬å°±ç»ª
        print("\\nğŸ‰ æ•°æ®åŸºæœ¬å‡†å¤‡å®Œæˆï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
    else:
        print("\\nğŸ’¡ è¿˜æœ‰ä¸€äº›å‡†å¤‡å·¥ä½œéœ€è¦å®Œæˆï¼Œè¯·æŸ¥çœ‹ä¸Šé¢çš„æç¤º")

# è¿è¡Œæ£€æŸ¥
check_data_readiness()""")

# ============= ä¸‹ä¸€æ­¥å’Œæ€»ç»“ =============
add_markdown("""---

## ğŸš€ ä¸‹ä¸€æ­¥

æ•°æ®å‡†å¤‡å¥½äº†ï¼æ¥ä¸‹æ¥ï¼š

### 1. è®­ç»ƒåˆ†è¯å™¨""")

add_code("""# è®­ç»ƒåˆ†è¯å™¨
# âš ï¸ è­¦å‘Šï¼šè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼

# !python -m scripts.tok_train --max_chars=2000000000""")

add_markdown("""### 2. å¼€å§‹é¢„è®­ç»ƒ""")

add_code("""# å¼€å§‹é¢„è®­ç»ƒï¼ˆéœ€è¦ GPUï¼‰
# âš ï¸ è­¦å‘Šï¼šè¿™éœ€è¦å¤§é‡æ—¶é—´å’Œè®¡ç®—èµ„æºï¼

# !torchrun --standalone --nproc_per_node=8 -m scripts.base_train --depth=20""")

add_markdown("""---

## ğŸ“š æ‰©å±•é˜…è¯»

æƒ³æ·±å…¥äº†è§£ï¼Ÿæ¨èé˜…è¯»ï¼š

- ğŸ“„ [README.md](README.md) - é¡¹ç›®æ•´ä½“ä»‹ç»
- ğŸ“„ [æ•°æ®.md](æ•°æ®.md) - æ›´è¯¦ç»†çš„æ•°æ®æ–‡æ¡£
- ğŸŒ [FineWeb-Edu æ•°æ®é›†](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)
- ğŸ“š [Chinchilla è®ºæ–‡](https://arxiv.org/abs/2203.15556) - ç†è§£æ•°æ®é‡å’Œæ¨¡å‹è§„æ¨¡çš„å…³ç³»

---

## ğŸ’¬ éœ€è¦å¸®åŠ©ï¼Ÿ

é‡åˆ°é—®é¢˜äº†ï¼Ÿ

1. å…ˆæŸ¥çœ‹æœ¬æ–‡æ¡£çš„å„ä¸ªç« èŠ‚
2. è¿è¡Œæ•°æ®æ£€æŸ¥å·¥å…·è¯Šæ–­é—®é¢˜
3. æŸ¥çœ‹ `æ•°æ®.md` æ–‡æ¡£è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯
4. æŸ¥çœ‹é¡¹ç›®çš„ GitHub Issues

---

**ç¥ä½ è®­ç»ƒé¡ºåˆ©ï¼** ğŸ‰

> ğŸ’¡ è®°ä½ï¼šæ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦ï¼ä¸è¦ç›²ç›®è¿½æ±‚å¤§æ•°æ®ï¼Œå…ˆç”¨å°æ¨¡å‹éªŒè¯æµç¨‹ï¼Œå†é€æ­¥æ‰©å¤§è§„æ¨¡ã€‚""")

# ä¿å­˜ notebook
output_file = "æ•°æ®å¤„ç†æŒ‡å—.ipynb"
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f"âœ… æˆåŠŸåˆ›å»º Jupyter Notebook: {output_file}")
print(f"ğŸ“Š å…± {len(notebook['cells'])} ä¸ªå•å…ƒæ ¼")
print(f"\\nä½¿ç”¨æ–¹æ³•ï¼š")
print(f"  jupyter notebook {output_file}")
print(f"  æˆ–ä½¿ç”¨ VS Code / JupyterLab æ‰“å¼€")


